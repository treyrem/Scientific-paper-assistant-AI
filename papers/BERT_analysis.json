{
  "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
  "authors": [
    "Google AI Language"
  ],
  "publication_year": 2018,
  "doi": null,
  "sections": {
    "introduction": {
      "title": "1 Introduction",
      "content": "strictions are sub-optimal for sentence-level tasks,\n Language model pre-training has been shown to and could be very harmful when applying fine-\n be effective for improving many natural language tuning based approaches to token-level tasks sucharXiv:1810.04805v2\n processing tasks (Dai and Le, 2015; Peters et al., as question answering, where it is crucial to incor-\n 2018a; Radford et al., 2018; Howard and Ruder, porate context from both directions.\n 2018). These include sentence-level tasks such as In this paper, we improve the fine-tuning based\n natural language inference (Bowman et al., 2015; approaches by proposing BERT: Bidirectional\n Williams et al., 2018) and paraphrasing (Dolan Encoder Representations from Transformers.\n and Brockett, 2005), which aim to predict the re- BERT alleviates the previously mentioned unidi-\n lationships between sentences by analyzing them rectionality constraint by using a \u201cmasked lan-\n holistically, as well as token-level tasks such as guage model\u201d (MLM) pre-training objective, in-\n named entity recognition and question answering, spired by the Cloze task (Taylor, 1953). The\n where models are required to produce fine-grained masked language model randomly masks some of\n output at the token level (Tjong Kim Sang and the tokens from the input, and the objective is to\n De Meulder, 2003; Rajpurkar et al., 2016). predict the original vocabulary id of the masked\n\nword based only on its context. Unlike left-to- These approaches have been generalized to\nright language model pre-training, the MLM ob- coarser granularities, such as sentence embed-\njective enables the representation to fuse the left dings (Kiros et al., 2015; Logeswaran and Lee,\nand the right context, which allows us to pre- 2018) or paragraph embeddings (Le and Mikolov,\ntrain a deep bidirectional Transformer. In addi- 2014). To train sentence representations, prior\ntion to the masked language model, we also use work has used objectives to rank candidate next\na \u201cnext sentence prediction\u201d task that jointly pre- sentences (Jernite et al., 2017; Logeswaran and\ntrains text-pair representations. The contributions Lee, 2018), left-to-right generation of next sen-\nof our paper are as follows: tence words given a representation of the previous\n sentence (Kiros et al., 2015), or denoising auto-\n \u2022 We demonstrate the importance of bidirectional\n encoder derived objectives (Hill et al., 2016).\n pre-training for language representations. Un-\n ELMo and its predecessor (Peters et al., 2017, like Radford et al. (2018), which uses unidirec-\n 2018a) generalize traditional word embedding re- tional language models for pre-training, BERT\n search along a different dimension. They extract uses masked language models to enable pre-\n context-sensitive features from a left-to-right and a trained deep bidirectional representations. This\n right-to-left language model. The contextual rep- is also in contrast to Peters et al. (2018a), which\n resentation of each token is the concatenation of uses a shallow concatenation of independently\n the left-to-right and right-to-left representations. trained left-to-right and right-to-left LMs.\n When integrating contextual word embeddings\n \u2022 We show that pre-trained representations reduce with existing task-specific architectures, ELMo\n the need for many heavily-engineered task- advances the state of the art for several major NLP\n specific architectures. BERT is the first fine- benchmarks (Peters et al., 2018a) including ques-\n tuning based representation model that achieves tion answering (Rajpurkar et al., 2016), sentiment\n state-of-the-art performance on a large suite analysis (Socher et al., 2013), and named entity\n of sentence-level and token-level tasks, outper- recognition (Tjong Kim Sang and De Meulder,\n forming many task-specific architectures. 2003). Melamud et al. (2016) proposed learning\n contextual representations through a task to pre-\n \u2022 BERT advances the state of the art for eleven\n dict a single word from both left and right context\n NLP tasks. The code and pre-trained mod-\n using LSTMs. Similar to ELMo, their model is\n els are available at https://github.com/\n feature-based and not deeply bidirectional. Fedus\n google-research/bert.\n et al. (2018) shows that the cloze task can be used\n to improve the robustness of text generation mod-2 Related Work\n els.\nThere is a long history of pre-training general lan-\nguage representations, and we briefly review the 2.2 Unsupervised Fine-tuning Approaches\nmost widely-used approaches in this section.\n As with the feature-based approaches, the first\n2.1 Unsupervised Feature-based Approaches works in this direction only pre-trained word em-\nLearning widely applicable representations of bedding parameters from unlabeled text (Col-\nwords has been an active area of research for lobert and Weston, 2008).\ndecades, including non-neural (Brown et al., 1992; More recently, sentence or document encoders\nAndo and Zhang, 2005; Blitzer et al., 2006) and which produce contextual token representations\nneural (Mikolov et al., 2013; Pennington et al., have been pre-trained from unlabeled text and\n2014) methods. Pre-trained word embeddings fine-tuned for a supervised downstream task (Dai\nare an integral part of modern NLP systems, of- and Le, 2015; Howard and Ruder, 2018; Radford\nfering significant improvements over embeddings et al., 2018). The advantage of these approaches\nlearned from scratch (Turian et al., 2010). To pre- is that few parameters need to be learned from\ntrain word embedding vectors, left-to-right lan- scratch. At least partly due to this advantage,\nguage modeling objectives have been used (Mnih OpenAI GPT (Radford et al., 2018) achieved pre-\nand Hinton, 2009), as well as objectives to dis- viously state-of-the-art results on many sentence-\ncriminate correct from incorrect words in left and level tasks from the GLUE benchmark (Wang\nright context (Mikolov et al., 2013). et al., 2018a). Left-to-right language model-\n\nNSP Mask LM Mask LM MNLI NER SQuAD Start/End Span\n\n C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 ... TN T[SEP] T1\u2019 ... TM\u2019\n\n BERT BERT BERT\n\n E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019\n\n [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM\n\n Masked Sentence A Masked Sentence B Question Paragraph\n\n Unlabeled Sentence A and B Pair Question Answer Pair\n\n Pre-training Fine-Tuning\n\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n\ning and auto-encoder objectives have been used mal difference between the pre-trained architec-\nfor pre-training such models (Howard and Ruder, ture and the final downstream architecture.\n2018; Radford et al., 2018; Dai and Le, 2015).\n Model Architecture BERT\u2019s model architec-\n2.3 Transfer Learning from Supervised Data ture is a multi-layer bidirectional Transformer en-\n coder based on the original implementation de-There has also been work showing effective trans-\n scribed in Vaswani et al. (2017) and released infer from supervised tasks with large datasets, such\n the tensor2tensor library.1 Because the useas natural language inference (Conneau et al.,\n of Transformers has become common and our im-2017) and machine translation (McCann et al.,\n plementation is almost identical to the original,2017). Computer vision research has also demon-\n we will omit an exhaustive background descrip-strated the importance of transfer learning from\n tion of the model architecture and refer readers tolarge pre-trained models, where an effective recipe\n Vaswani et al. (2017) as well as excellent guidesis to fine-tune models pre-trained with Ima-\n such as \u201cThe Annotated Transformer.\u201d2geNet (Deng et al., 2009; Yosinski et al., 2014).\n In this work, we denote the number of layers\n3 BERT (i.e., Transformer blocks) as L, the hidden size as\n H, and the number of self-attention heads as A.3\nWe introduce BERT and its detailed implementa- We primarily report results on two model sizes:\ntion in this section. There are two steps in our BERTBASE (L=12, H=768, A=12, Total Param-\nframework: pre-training and fine-tuning. Dur- eters=110M) and BERTLARGE (L=24, H=1024,\ning pre-training, the model is trained on unlabeled A=16, Total Parameters=340M).\ndata over different pre-training tasks. For fine- BERTBASE was chosen to have the same model\ntuning, the BERT model is first initialized with size as OpenAI GPT for comparison purposes.\nthe pre-trained parameters, and all of the param- Critically, however, the BERT Transformer uses\neters are fine-tuned using labeled data from the bidirectional self-attention, while the GPT Trans-\ndownstream tasks. Each downstream task has sep- former uses constrained self-attention where every\narate fine-tuned models, even though they are ini- token can only attend to context to its left.4\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve 1https://github.com/tensorflow/tensor2tensor\n 2http://nlp.seas.harvard.edu/2018/04/03/attention.htmlas a running example for this section.\n 3In all cases we set the feed-forward/filter size to be 4H,\n A distinctive feature of BERT is its unified ar- i.e., 3072 for the H = 768 and 4096 for the H = 1024.\nchitecture across different tasks. There is mini- 4We note that in the literature the bidirectional Trans-\n\nInput/Output Representations To make BERT In order to train a deep bidirectional representa-\nhandle a variety of down-stream tasks, our input tion, we simply mask some percentage of the input\nrepresentation is able to unambiguously represent tokens at random, and then predict those masked\nboth a single sentence and a pair of sentences tokens. We refer to this procedure as a \u201cmasked\n(e.g., \u27e8Question, Answer \u27e9) in one token sequence. LM\u201d (MLM), although it is often referred to as a\nThroughout this work, a \u201csentence\u201d can be an arbi- Cloze task in the literature (Taylor, 1953). In this\ntrary span of contiguous text, rather than an actual case, the final hidden vectors corresponding to the\nlinguistic sentence. A \u201csequence\u201d refers to the in- mask tokens are fed into an output softmax over\nput token sequence to BERT, which may be a sin- the vocabulary, as in a standard LM. In all of our\ngle sentence or two sentences packed together. experiments, we mask 15% of all WordPiece to-\n We use WordPiece embeddings (Wu et al., kens in each sequence at random. In contrast to\n2016) with a 30,000 token vocabulary. The first denoising auto-encoders (Vincent et al., 2008), we\ntoken of every sequence is always a special clas- only predict the masked words rather than recon-\nsification token ([CLS]). The final hidden state structing the entire input.\ncorresponding to this token is used as the ag- Although this allows us to obtain a bidirec-\ngregate sequence representation for classification tional pre-trained model, a downside is that we\ntasks. Sentence pairs are packed together into a are creating a mismatch between pre-training and\nsingle sequence. We differentiate the sentences in fine-tuning, since the [MASK] token does not ap-\ntwo ways. First, we separate them with a special pear during fine-tuning. To mitigate this, we do\ntoken ([SEP]). Second, we add a learned embed- not always replace \u201cmasked\u201d words with the ac-\nding to every token indicating whether it belongs tual [MASK] token. The training data generator\nto sentence A or sentence B. As shown in Figure 1, chooses 15% of the token positions at random for\nwe denote input embedding as E, the final hidden prediction. If the i-th token is chosen, we replace\nvector of the special [CLS] token as C \u2208RH, the i-th token with (1) the [MASK] token 80% of\nand the final hidden vector for the ith input token the time (2) a random token 10% of the time (3)\nas Ti \u2208RH. the unchanged i-th token 10% of the time. Then,\n For a given token, its input representation is Ti will be used to predict the original token with\nconstructed by summing the corresponding token, cross entropy loss. We compare variations of this\nsegment, and position embeddings. A visualiza- procedure in Appendix C.2.\ntion of this construction can be seen in Figure 2.\n Task #2: Next Sentence Prediction (NSP)\n3.1 Pre-training BERT Many important downstream tasks such as Ques-\n tion Answering (QA) and Natural Language Infer-Unlike Peters et al. (2018a) and Radford et al.\n ence (NLI) are based on understanding the rela-(2018), we do not use traditional left-to-right or\n tionship between two sentences, which is not di-right-to-left language models to pre-train BERT.\n rectly captured by language modeling. In orderInstead, we pre-train BERT using two unsuper-\n to train a model that understands sentence rela-vised tasks, described in this section. This step\n tionships, we pre-train for a binarized next sen-is presented in the left part of Figure 1.\n tence prediction task that can be trivially gener-\nTask #1: Masked LM Intuitively, it is reason- ated from any monolingual corpus. Specifically,\nable to believe that a deep bidirectional model is when choosing the sentences A and B for each pre-\nstrictly more powerful than either a left-to-right training example, 50% of the time B is the actual\nmodel or the shallow concatenation of a left-to- next sentence that follows A (labeled as IsNext),\nright and a right-to-left model. Unfortunately, and 50% of the time it is a random sentence from\nstandard conditional language models can only be the corpus (labeled as NotNext). As we show\ntrained left-to-right or right-to-left, since bidirec- in Figure 1, C is used for next sentence predic-\ntional conditioning would allow each word to in- tion (NSP).5 Despite its simplicity, we demon-\ndirectly \u201csee itself\u201d, and the model could trivially strate in Section 5.1 that pre-training towards this\npredict the target word in a multi-layered context. task is very beneficial to both QA and NLI. 6\n\nformer is often referred to as a \u201cTransformer encoder\u201d while 5The final model achieves 97%-98% accuracy on NSP.\nthe left-context-only version is referred to as a \u201cTransformer 6The vector C is not a meaningful sentence representation\ndecoder\u201d since it can be used for text generation. without fine-tuning, since it was trained with NSP.\n\nInput [CLS] my dog is cute [SEP] he likes play ##ing [SEP]\n\n Token\n Embeddings E[CLS] Emy Edog Eis Ecute E[SEP] Ehe Elikes Eplay E##ing E[SEP]\n\n Segment\n Embeddings EA EA EA EA EA EA EB EB EB EB EB\n\n Position\n Embeddings E0 E1 E2 E3 E4 E5 E6 E7 E8 E9 E10\n\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings.\n\nThe NSP task is closely related to representation- (4) a degenerate text-\u2205pair in text classification\nlearning objectives used in Jernite et al. (2017) and or sequence tagging. At the output, the token rep-\nLogeswaran and Lee (2018). However, in prior resentations are fed into an output layer for token-\nwork, only sentence embeddings are transferred to level tasks, such as sequence tagging or question\ndown-stream tasks, where BERT transfers all pa- answering, and the [CLS] representation is fed\nrameters to initialize end-task model parameters. into an output layer for classification, such as en-\n tailment or sentiment analysis.\nPre-training data The pre-training procedure Compared to pre-training, fine-tuning is rela-\nlargely follows the existing literature on language tively inexpensive. All of the results in the pa-\nmodel pre-training. For the pre-training corpus we per can be replicated in at most 1 hour on a sin-\nuse the BooksCorpus (800M words) (Zhu et al., gle Cloud TPU, or a few hours on a GPU, starting\n2015) and English Wikipedia (2,500M words). from the exact same pre-trained model.7 We de-\nFor Wikipedia we extract only the text passages scribe the task-specific details in the correspond-\nand ignore lists, tables, and headers. It is criti- ing subsections of Section 4. More details can be\ncal to use a document-level corpus rather than a found in Appendix A.5.\nshuffled sentence-level corpus such as the Billion\nWord Benchmark (Chelba et al., 2013) in order to 4 Experiments\nextract long contiguous sequences.\n In this section, we present BERT fine-tuning re-\n3.2 Fine-tuning BERT sults on 11 NLP tasks.\n\nFine-tuning is straightforward since the self- 4.1 GLUE\nattention mechanism in the Transformer al-\n The General Language Understanding Evaluation\nlows BERT to model many downstream tasks\u2014\n (GLUE) benchmark (Wang et al., 2018a) is a col-\nwhether they involve single text or text pairs\u2014by\n lection of diverse natural language understanding\nswapping out the appropriate inputs and outputs.\n tasks. Detailed descriptions of GLUE datasets are\nFor applications involving text pairs, a common\n included in Appendix B.1.\npattern is to independently encode text pairs be-\n To fine-tune on GLUE, we represent the input\nfore applying bidirectional cross attention, such\n sequence (for single sentence or sentence pairs)\nas Parikh et al. (2016); Seo et al. (2017). BERT\n as described in Section 3, and use the final hid-\ninstead uses the self-attention mechanism to unify den vector C \u2208RH corresponding to the first\nthese two stages, as encoding a concatenated text input token ([CLS]) as the aggregate representa-\npair with self-attention effectively includes bidi-\n tion. The only new parameters introduced during\nrectional cross attention between two sentences. fine-tuning are classification layer weights W \u2208\n For each task, we simply plug in the task- RK\u00d7H, where K is the number of labels. We com-\nspecific inputs and outputs into BERT and fine- pute a standard classification loss with C and W,\ntune all the parameters end-to-end. At the in- T i.e., log(softmax(CW )).\nput, sentence A and sentence B from pre-training\n 7For example, the BERT SQuAD model can be trained inare analogous to (1) sentence pairs in paraphras-\n around 30 minutes on a single Cloud TPU to achieve a Dev\ning, (2) hypothesis-premise pairs in entailment, (3) F1 score of 91.0%.\nquestion-passage pairs in question answering, and 8See (10) in https://gluebenchmark.com/faq.\n\nSystem MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\n Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\n BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\n OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\n BERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\n BERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\n\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\nThe number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different\nthan the official GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\n We use a batch size of 32 and fine-tune for 3 Wikipedia containing the answer, the task is to\nepochs over the data for all GLUE tasks. For each predict the answer text span in the passage.\ntask, we selected the best fine-tuning learning rate As shown in Figure 1, in the question answer-\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. ing task, we represent the input question and pas-\nAdditionally, for BERTLARGE we found that fine- sage as a single packed sequence, with the ques-\ntuning was sometimes unstable on small datasets, tion using the A embedding and the passage using\nso we ran several random restarts and selected the the B embedding. We only introduce a start vec-\nbest model on the Dev set. With random restarts, tor S \u2208RH and an end vector E \u2208RH during\nwe use the same pre-trained checkpoint but per- fine-tuning. The probability of word i being the\nform different fine-tuning data shuffling and clas- start of the answer span is computed as a dot prod-\nsifier layer initialization.9 uct between Ti and S followed by a softmax over\n Results are presented in Table 1. Both all of the words in the paragraph: Pi = eS\u00b7Ti .\n Pj eS\u00b7TjBERTBASE and BERTLARGE outperform all sys- The analogous formula is used for the end of the\ntems on all tasks by a substantial margin, obtaining\n answer span. The score of a candidate span from\n4.5% and 7.0% respective average accuracy im-\n position i to position j is defined as S\u00b7Ti + E\u00b7Tj,\nprovement over the prior state of the art. Note that\n and the maximum scoring span where j \u2265i is\nBERTBASE and OpenAI GPT are nearly identical used as a prediction. The training objective is the\nin terms of model architecture apart from the at-\n sum of the log-likelihoods of the correct start and\ntention masking. For the largest and most widely\n end positions. We fine-tune for 3 epochs with a\nreported GLUE task, MNLI, BERT obtains a 4.6%\n learning rate of 5e-5 and a batch size of 32.\nabsolute accuracy improvement. On the official\n Table 2 shows top leaderboard entries as well\nGLUE leaderboard10, BERTLARGE obtains a score\n as results from top published systems (Seo et al.,\nof 80.5, compared to OpenAI GPT, which obtains\n 2017; Clark and Gardner, 2018; Peters et al.,\n72.8 as of the date of writing.\n 2018a; Hu et al., 2018). The top results from the\n We find that BERTLARGE significantly outper-\n SQuAD leaderboard do not have up-to-date public\nforms BERTBASE across all tasks, especially those system descriptions available,11 and are allowed to\nwith very little training data. The effect of model\n use any public data when training their systems.\nsize is explored more thoroughly in Section 5.2.\n We therefore use modest data augmentation in\n our system by first fine-tuning on TriviaQA (Joshi4.2 SQuAD v1.1\n et al., 2017) befor fine-tuning on SQuAD.\nThe Stanford Question Answering Dataset Our best performing system outperforms the top\n(SQuAD v1.1) is a collection of 100k crowd- leaderboard system by +1.5 F1 in ensembling and\nsourced question/answer pairs (Rajpurkar et al., +1.3 F1 as a single system. In fact, our single\n2016). Given a question and a passage from BERT model outperforms the top ensemble sys-\n 9The GLUE data set distribution does not include the Test tem in terms of F1 score. Without TriviaQA fine-\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERTBASE and BERTLARGE. 11QANet is described in Yu et al. (2018), but the system\n 10https://gluebenchmark.com/leaderboard has improved substantially after publication.\n\nSystem Dev Test System Dev Test\n EM F1 EM F1\n ESIM+GloVe 51.9 52.7\n Top Leaderboard Systems (Dec 10th, 2018) ESIM+ELMo 59.1 59.2\n Human - - 82.3 91.2 OpenAI GPT - 78.0\n #1 Ensemble - nlnet - - 86.0 91.7\n BERTBASE 81.6 - #2 Ensemble - QANet - - 84.5 90.5\n BERTLARGE 86.6 86.3\n Published\n Human (expert)\u2020 - 85.0 BiDAF+ELMo (Single) - 85.6 - 85.8\n R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Human (5 annotations)\u2020 - 88.0\n\n Ours\n Table 4: SWAG Dev and Test accuracies. \u2020Human per- BERTBASE (Single) 80.8 88.5 - -\n BERTLARGE (Single) 84.1 90.9 - - formance is measured with 100 samples, as reported in\n BERTLARGE (Ensemble) 85.8 91.8 - - the SWAG paper.\n BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\n BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\n\n si,j\u02c6 = maxj\u2265iS\u00b7Ti + E\u00b7Tj. We predict a non-null\nTable 2: SQuAD 1.1 results. The BERT ensemble\n answer when si,j\u02c6 > snull + \u03c4, where the thresh-\nis 7x systems which use different pre-training check-\n old \u03c4 is selected on the dev set to maximize F1.points and fine-tuning seeds.\n We did not use TriviaQA data for this model. We\n fine-tuned for 2 epochs with a learning rate of 5e-5\n System Dev Test\n EM F1 EM F1 and a batch size of 48.\n Top Leaderboard Systems (Dec 10th, 2018) The results compared to prior leaderboard en-\n Human 86.3 89.0 86.9 89.5 tries and top published work (Sun et al., 2018;\n #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 Wang et al., 2018b) are shown in Table 3, exclud-\n #2 Single - nlnet - - 74.2 77.1\n ing systems that use BERT as one of their com-\n Published\n ponents. We observe a +5.1 F1 improvement over unet (Ensemble) - - 71.4 74.9\n SLQA+ (Single) - 71.4 74.4 the previous best system.\n\n Ours\n BERTLARGE (Single) 78.7 81.9 80.0 83.1 4.4 SWAG\n\n The Situations With Adversarial Generations\nTable 3: SQuAD 2.0 results. We exclude entries that (SWAG) dataset contains 113k sentence-pair com-\nuse BERT as one of their components. pletion examples that evaluate grounded common-\n sense inference (Zellers et al., 2018). Given a sen-\n tence, the task is to choose the most plausible con-tuning data, we only lose 0.1-0.4 F1, still outper-\n tinuation among four choices.forming all existing systems by a wide margin.12\n When fine-tuning on the SWAG dataset, we\n4.3 SQuAD v2.0 construct four input sequences, each containing\n the concatenation of the given sentence (sentence\nThe SQuAD 2.0 task extends the SQuAD 1.1\n A) and a possible continuation (sentence B). The\nproblem definition by allowing for the possibility\n only task-specific parameters introduced is a vec-\nthat no short answer exists in the provided para-\n tor whose dot product with the [CLS] token rep-\ngraph, making the problem more realistic.\n resentation C denotes a score for each choice\n We use a simple approach to extend the SQuAD\n which is normalized with a softmax layer.\nv1.1 BERT model for this task. We treat ques-\n We fine-tune the model for 3 epochs with a\ntions that do not have an answer as having an an-\n learning rate of 2e-5 and a batch size of 16. Re-\nswer span with start and end at the [CLS] to-\n sults are presented in Table 4. BERTLARGE out-\nken. The probability space for the start and end\n performs the authors\u2019 baseline ESIM+ELMo sys-\nanswer span positions is extended to include the\n tem by +27.1% and OpenAI GPT by 8.3%.\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull = 5 Ablation Studies\nS\u00b7C + E\u00b7C to the score of the best non-null span\n In this section, we perform ablation experiments\n 12The TriviaQA data we used consists of paragraphs from\n over a number of facets of BERT in order to betterTriviaQA-Wiki formed of the first 400 tokens in documents,\nthat contain at least one of the provided possible answers. understand their relative importance. Additional\n\nDev Set results are still far worse than those of the pre-\nTasks MNLI-m QNLI MRPC SST-2 SQuAD trained bidirectional models. The BiLSTM hurts\n (Acc) (Acc) (Acc) (Acc) (F1)\n performance on the GLUE tasks.\nBERTBASE 84.4 88.4 86.7 92.7 88.5\nNo NSP 83.9 84.9 86.5 92.6 87.9 We recognize that it would also be possible to\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8 train separate LTR and RTL models and represent\n + BiLSTM 82.1 84.1 75.7 91.6 84.9 each token as the concatenation of the two mod-\n els, as ELMo does. However: (a) this is twice as\nTable 5: Ablation over the pre-training tasks using the\n expensive as a single bidirectional model; (b) this\nBERTBASE architecture. \u201cNo NSP\u201d is trained without\n is non-intuitive for tasks like QA, since the RTLthe next sentence prediction task. \u201cLTR & No NSP\u201d is\ntrained as a left-to-right LM without the next sentence model would not be able to condition the answer\nprediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran- on the question; (c) this it is strictly less powerful\ndomly initialized BiLSTM on top of the \u201cLTR + No than a deep bidirectional model, since it can use\nNSP\u201d model during fine-tuning. both left and right context at every layer.\n\n 5.2 Effect of Model Size\nablation studies can be found in Appendix C.\n In this section, we explore the effect of model size\n5.1 Effect of Pre-training Tasks on fine-tuning task accuracy. We trained a number\n of BERT models with a differing number of layers,\nWe demonstrate the importance of the deep bidi-\n hidden units, and attention heads, while otherwise\nrectionality of BERT by evaluating two pre-\n using the same hyperparameters and training pro-\ntraining objectives using exactly the same pre-\n cedure as described previously.\ntraining data, fine-tuning scheme, and hyperpa-\n Results on selected GLUE tasks are shown inrameters as BERTBASE:\n Table 6. In this table, we report the average Dev\nNo NSP: A bidirectional model which is trained Set accuracy from 5 random restarts of fine-tuning.\nusing the \u201cmasked LM\u201d (MLM) but without the We can see that larger models lead to a strict ac-\n\u201cnext sentence prediction\u201d (NSP) task. curacy improvement across all four datasets, even\nLTR & No NSP: A left-context-only model which for MRPC which only has 3,600 labeled train-\nis trained using a standard Left-to-Right (LTR) ing examples, and is substantially different from\nLM, rather than an MLM. The left-only constraint the pre-training tasks. It is also perhaps surpris-\nwas also applied at fine-tuning, because removing ing that we are able to achieve such significant\nit introduced a pre-train/fine-tune mismatch that improvements on top of models which are al-\ndegraded downstream performance. Additionally, ready quite large relative to the existing literature.\nthis model was pre-trained without the NSP task. For example, the largest Transformer explored in\nThis is directly comparable to OpenAI GPT, but Vaswani et al. (2017) is (L=6, H=1024, A=16)\nusing our larger training dataset, our input repre- with 100M parameters for the encoder, and the\nsentation, and our fine-tuning scheme. largest Transformer we have found in the literature\n We first examine the impact brought by the NSP is (L=64, H=512, A=2) with 235M parameters\ntask. In Table 5, we show that removing NSP (Al-Rfou et al., 2018). By contrast, BERTBASE\nhurts performance significantly on QNLI, MNLI, contains 110M parameters and BERTLARGE con-\nand SQuAD 1.1. Next, we evaluate the impact tains 340M parameters.\nof training bidirectional representations by com- It has long been known that increasing the\nparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model size will lead to continual improvements\nmodel performs worse than the MLM model on all on large-scale tasks such as machine translation\ntasks, with large drops on MRPC and SQuAD. and language modeling, which is demonstrated\n For SQuAD it is intuitively clear that a LTR by the LM perplexity of held-out training data\nmodel will perform poorly at token predictions, shown in Table 6. However, we believe that\nsince the token-level hidden states have no right- this is the first work to demonstrate convinc-\nside context. In order to make a good faith at- ingly that scaling to extreme model sizes also\ntempt at strengthening the LTR system, we added leads to large improvements on very small scale\na randomly initialized BiLSTM on top. This does tasks, provided that the model has been suffi-\nsignificantly improve results on SQuAD, but the ciently pre-trained. Peters et al. (2018b) presented\n\nmixed results on the downstream task impact of System Dev F1 Test F1\nincreasing the pre-trained bi-LM size from two ELMo (Peters et al., 2018a) 95.7 92.2\nto four layers and Melamud et al. (2016) men- CVT (Clark et al., 2018) - 92.6\n CSE (Akbik et al., 2018) - 93.1\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing Fine-tuning approach\n BERTLARGE 96.6 92.8\nfurther to 1,000 did not bring further improve- BERTBASE 96.4 92.4\nments. Both of these prior works used a feature-\n Feature-based approach (BERTBASE)\nbased approach \u2014 we hypothesize that when the Embeddings 91.0 -\nmodel is fine-tuned directly on the downstream Second-to-Last Hidden 95.6 -\n Last Hidden 94.9 -\ntasks and uses only a very small number of ran- Weighted Sum Last Four Hidden 95.9 -\ndomly initialized additional parameters, the task- Concat Last Four Hidden 96.1 -\nspecific models can benefit from the larger, more Weighted Sum All 12 Layers 95.5 -\nexpressive pre-trained representations even when\n Table 7: CoNLL-2003 Named Entity Recognition re-\ndownstream task data is very small.\n sults. Hyperparameters were selected using the Dev\n set. The reported Dev and Test scores are averaged over\n5.3 Feature-based Approach with BERT 5 random restarts using those hyperparameters.\nAll of the BERT results presented so far have used\nthe fine-tuning approach, where a simple classifi-\ncation layer is added to the pre-trained model, and layer in the output. We use the representation of\nall parameters are jointly fine-tuned on a down- the first sub-token as the input to the token-level\nstream task. However, the feature-based approach, classifier over the NER label set.\nwhere fixed features are extracted from the pre- To ablate the fine-tuning approach, we apply the\ntrained model, has certain advantages. First, not feature-based approach by extracting the activa-\nall tasks can be easily represented by a Trans- tions from one or more layers without fine-tuning\nformer encoder architecture, and therefore require any parameters of BERT. These contextual em-\na task-specific model architecture to be added. beddings are used as input to a randomly initial-\nSecond, there are major computational benefits ized two-layer 768-dimensional BiLSTM before\nto pre-compute an expensive representation of the the classification layer.\ntraining data once and then run many experiments\n Results are presented in Table 7. BERTLARGEwith cheaper models on top of this representation.\n performs competitively with state-of-the-art meth-\n In this section, we compare the two approaches\n ods. The best performing method concatenates the\nby applying BERT to the CoNLL-2003 Named\n token representations from the top four hidden lay-\nEntity Recognition (NER) task (Tjong Kim Sang\n ers of the pre-trained Transformer, which is only\nand De Meulder, 2003). In the input to BERT, we\n 0.3 F1 behind fine-tuning the entire model. This\nuse a case-preserving WordPiece model, and we\n demonstrates that BERT is effective for both fine-\ninclude the maximal document context provided\n tuning and feature-based approaches.\nby the data. Following standard practice, we for-\nmulate this as a tagging task but do not use a CRF",
      "section_type": "introduction",
      "page_numbers": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
      ],
      "confidence": 0.6
    },
    "conclusion": {
      "title": "6 Conclusion",
      "content": "Hyperparams Dev Set Accuracy\n #L #H #A LM (ppl) MNLI-m MRPC SST-2 Recent empirical improvements due to transfer\n learning with language models have demonstrated 3 768 12 5.84 77.9 79.8 88.4\n 6 768 3 5.24 80.6 82.2 90.7 that rich, unsupervised pre-training is an integral\n 6 768 12 4.68 81.9 84.8 91.3 part of many language understanding systems. In\n 12 768 12 3.99 84.4 86.7 92.9\n 12 1024 16 3.54 85.7 86.9 93.3 particular, these results enable even low-resource\n 24 1024 16 3.23 86.6 87.8 93.7 tasks to benefit from deep unidirectional architec-\n tures. Our major contribution is further general-\nTable 6: Ablation over BERT model size. #L = the izing these findings to deep bidirectional architec-\nnumber of layers; #H = hidden size; #A = number of at- tures, allowing the same pre-trained model to suc-\ntention heads. \u201cLM (ppl)\u201d is the masked LM perplexity cessfully tackle a broad set of NLP tasks.\nof held-out training data.\n\nReferences Kevin Clark, Minh-Thang Luong, Christopher D Man-\n ning, and Quoc Le. 2018. Semi-supervised se-\nAlan Akbik, Duncan Blythe, and Roland Vollgraf. quence modeling with cross-view training. In Pro-\n 2018. Contextual string embeddings for sequence ceedings of the 2018 Conference on Empirical Meth-\n labeling. In Proceedings of the 27th International ods in Natural Language Processing, pages 1914\u2013\n Conference on Computational Linguistics, pages 1925.\n 1638\u20131649.\n Ronan Collobert and Jason Weston. 2008. A unified\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy architecture for natural language processing: Deep\n Guo, and Llion Jones. 2018. Character-level lan- neural networks with multitask learning. In Pro-\n guage modeling with deeper self-attention. arXiv ceedings of the 25th international conference on\n preprint arXiv:1808.04444. Machine learning, pages 160\u2013167. ACM.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\n for learning predictive structures from multiple tasks Barrault, and Antoine Bordes. 2017. Supervised\n and unlabeled data. Journal of Machine Learning learning of universal sentence representations from\n Research, 6(Nov):1817\u20131853. natural language inference data. In Proceedings of\n the 2017 Conference on Empirical Methods in Nat-\n ural Language Processing, pages 670\u2013680, Copen-\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\n hagen, Denmark. Association for Computational\n Hoa Trang Dang, and Danilo Giampiccolo. 2009.\n Linguistics.\n The fifth PASCAL recognizing textual entailment\n challenge. In TAC. NIST.\n Andrew M Dai and Quoc V Le. 2015. Semi-supervised\n sequence learning. In Advances in neural informa-\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. tion processing systems, pages 3079\u20133087.\n 2006. Domain adaptation with structural correspon-\n dence learning. In Proceedings of the 2006 confer- J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\n ence on empirical methods in natural language pro- Fei. 2009. ImageNet: A Large-Scale Hierarchical\n cessing, pages 120\u2013128. Association for Computa- Image Database. In CVPR09.\n tional Linguistics.\n William B Dolan and Chris Brockett. 2005. Automati-\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, cally constructing a corpus of sentential paraphrases.\n and Christopher D. Manning. 2015. A large anno- In Proceedings of the Third International Workshop\n tated corpus for learning natural language inference. on Paraphrasing (IWP2005).\n In EMNLP. Association for Computational Linguis-\n tics. William Fedus, Ian Goodfellow, and Andrew M Dai.\n 2018. Maskgan: Better text generation via filling in\nPeter F Brown, Peter V Desouza, Robert L Mercer, the . arXiv preprint arXiv:1801.07736.\n Vincent J Della Pietra, and Jenifer C Lai. 1992.\n Class-based n-gram models of natural language. Dan Hendrycks and Kevin Gimpel. 2016. Bridging\n Computational linguistics, 18(4):467\u2013479. nonlinearities and stochastic regularizers with gaus-\n sian error linear units. CoRR, abs/1606.08415.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\n Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Gazpio, and Lucia Specia. 2017. Semeval-2017\n Learning distributed representations of sentences task 1: Semantic textual similarity multilingual and\n from unlabelled data. In Proceedings of the 2016 crosslingual focused evaluation. In Proceedings\n Conference of the North American Chapter of the of the 11th International Workshop on Semantic\n Association for Computational Linguistics: Human Evaluation (SemEval-2017), pages 1\u201314, Vancou-\n Language Technologies. Association for Computa- ver, Canada. Association for Computational Lin-\n tional Linguistics. guistics.\n\n Jeremy Howard and Sebastian Ruder. 2018. Universal\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, language model fine-tuning for text classification. In\n Thorsten Brants, Phillipp Koehn, and Tony Robin- ACL. Association for Computational Linguistics.\n son. 2013. One billion word benchmark for measur-\n ing progress in statistical language modeling. arXiv Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\n preprint arXiv:1312.3005. Furu Wei, and Ming Zhou. 2018. Reinforced\n mnemonic reader for machine reading comprehen-\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. sion. In IJCAI.\n Quora question pairs.\n Yacine Jernite, Samuel R. Bowman, and David Son-\nChristopher Clark and Matt Gardner. 2018. Simple tag. 2017. Discourse-based objectives for fast un-\n and effective multi-paragraph reading comprehen- supervised sentence representation learning. CoRR,\n sion. In ACL. abs/1705.00557.\n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Matthew Peters, Mark Neumann, Luke Zettlemoyer,\n Zettlemoyer. 2017. Triviaqa: A large scale distantly and Wen-tau Yih. 2018b. Dissecting contextual\n supervised challenge dataset for reading comprehen- word embeddings: Architecture and representation.\n sion. In ACL. In Proceedings of the 2018 Conference on Empiri-\n cal Methods in Natural Language Processing, pages\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, 1499\u20131509.\n Richard Zemel, Raquel Urtasun, Antonio Torralba,\n and Sanja Fidler. 2015. Skip-thought vectors. In Alec Radford, Karthik Narasimhan, Tim Salimans, and\n Advances in neural information processing systems, Ilya Sutskever. 2018. Improving language under-\n pages 3294\u20133302. standing with unsupervised learning. Technical re-\n port, OpenAI.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\n resentations of sentences and documents. In Inter- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\n national Conference on Machine Learning, pages Percy Liang. 2016. Squad: 100,000+ questions for\n 1188\u20131196. machine comprehension of text. In Proceedings of\n the 2016 Conference on Empirical Methods in Nat-\nHector J Levesque, Ernest Davis, and Leora Morgen- ural Language Processing, pages 2383\u20132392.\n stern. 2011. The winograd schema challenge. In\n Aaai spring symposium: Logical formalizations of Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\n commonsense reasoning, volume 46, page 47. Hannaneh Hajishirzi. 2017. Bidirectional attention\n flow for machine comprehension. In ICLR.\nLajanugen Logeswaran and Honglak Lee. 2018. An\n efficient framework for learning sentence represen- Richard Socher, Alex Perelygin, Jean Wu, Jason\n tations. In International Conference on Learning Chuang, Christopher D Manning, Andrew Ng, and\n Representations. Christopher Potts. 2013. Recursive deep models\n for semantic compositionality over a sentiment tree-\nBryan McCann, James Bradbury, Caiming Xiong, and\n bank. In Proceedings of the 2013 conference on\n Richard Socher. 2017. Learned in translation: Con-\n empirical methods in natural language processing,\n textualized word vectors. In NIPS.\n pages 1631\u20131642.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2016. context2vec: Learning generic context em-\n 2018. U-net: Machine reading comprehension bedding with bidirectional LSTM. In CoNLL.\n with unanswerable questions. arXiv preprint\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- arXiv:1810.06638.\n rado, and Jeff Dean. 2013. Distributed representa-\n Wilson L Taylor. 1953. Cloze procedure: A new tions of words and phrases and their compositional-\n tool for measuring readability. Journalism Bulletin, ity. In Advances in Neural Information Processing\n 30(4):415\u2013433. Systems 26, pages 3111\u20133119. Curran Associates,\n Inc.\n Erik F Tjong Kim Sang and Fien De Meulder.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal- 2003. Introduction to the conll-2003 shared task:\n able hierarchical distributed language model. In Language-independent named entity recognition. In\n D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- CoNLL.\n tou, editors, Advances in Neural Information Pro-\n Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. cessing Systems 21, pages 1081\u20131088. Curran As-\n Word representations: A simple and general method\n sociates, Inc.\n for semi-supervised learning. In Proceedings of the\nAnkur P Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and 48th Annual Meeting of the Association for Compu-\n Jakob Uszkoreit. 2016. A decomposable attention tational Linguistics, ACL \u201910, pages 384\u2013394.\n model for natural language inference. In EMNLP.\n Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nJeffrey Pennington, Richard Socher, and Christo- Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\n pher D. Manning. 2014. Glove: Global vectors for Kaiser, and Illia Polosukhin. 2017. Attention is all\n word representation. In Empirical Methods in Nat- you need. In Advances in Neural Information Pro-\n ural Language Processing (EMNLP), pages 1532\u2013 cessing Systems, pages 6000\u20136010.\n 1543.\n Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nMatthew Peters, Waleed Ammar, Chandra Bhagavat- Pierre-Antoine Manzagol. 2008. Extracting and\n ula, and Russell Power. 2017. Semi-supervised se- composing robust features with denoising autoen-\n quence tagging with bidirectional language models. coders. In Proceedings of the 25th international\n In ACL. conference on Machine learning, pages 1096\u20131103.\n ACM.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\n Gardner, Christopher Clark, Kenton Lee, and Luke Alex Wang, Amanpreet Singh, Julian Michael, Fe-\n Zettlemoyer. 2018a. Deep contextualized word rep- lix Hill, Omer Levy, and Samuel Bowman. 2018a.\n resentations. In NAACL. Glue: A multi-task benchmark and analysis platform\n\nfor natural language understanding. In Proceedings \u2022 Additional details for our experiments are\n of the 2018 EMNLP Workshop BlackboxNLP: An- presented in Appendix B; and\n alyzing and Interpreting Neural Networks for NLP,\n pages 353\u2013355. \u2022 Additional ablation studies are presented in\n Appendix C.Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\n granularity hierarchical attention fusion networks We present additional ablation studies for\n for reading comprehension and question answering.\n BERT including: In Proceedings of the 56th Annual Meeting of the As-\n sociation for Computational Linguistics (Volume 1: \u2013 Effect of Number of Training Steps; and\n Long Papers). Association for Computational Lin-\n guistics. \u2013 Ablation for Different Masking Proce-\n dures.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\n man. 2018. Neural network acceptability judg- A Additional Details for BERT\n ments. arXiv preprint arXiv:1805.12471.\n A.1 Illustration of the Pre-training Tasks\nAdina Williams, Nikita Nangia, and Samuel R Bow-\n man. 2018. A broad-coverage challenge corpus We provide examples of the pre-training tasks in\n for sentence understanding through inference. In the following.\n NAACL.\n Masked LM and the Masking Procedure As-\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V suming the unlabeled sentence is my dog is\n Le, Mohammad Norouzi, Wolfgang Macherey,\n hairy, and during the random masking procedure Maxim Krikun, Yuan Cao, Qin Gao, Klaus\n Macherey, et al. 2016. Google\u2019s neural ma- we chose the 4-th token (which corresponding to\n chine translation system: Bridging the gap between hairy), our masking procedure can be further il-\n human and machine translation. arXiv preprint lustrated by\n arXiv:1609.08144.\n \u2022 80% of the time: Replace the word with the\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\n [MASK] token, e.g., my dog is hairy \u2192 Lipson. 2014. How transferable are features in deep\n neural networks? In Advances in neural information my dog is [MASK]\n processing systems, pages 3320\u20133328.\n \u2022 10% of the time: Replace the word with a\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui random word, e.g., my dog is hairy \u2192my\n Zhao, Kai Chen, Mohammad Norouzi, and Quoc V\n dog is apple\n Le. 2018. QANet: Combining local convolution\n with global self-attention for reading comprehen-\n \u2022 10% of the time: Keep the word un- sion. In ICLR.\n changed, e.g., my dog is hairy \u2192my dog\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin is hairy. The purpose of this is to bias the\n Choi. 2018. Swag: A large-scale adversarial dataset representation towards the actual observed\n for grounded commonsense inference. In Proceed-\n word. ings of the 2018 Conference on Empirical Methods\n in Natural Language Processing (EMNLP).\n The advantage of this procedure is that the\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- Transformer encoder does not know which words\n dinov, Raquel Urtasun, Antonio Torralba, and Sanja it will be asked to predict or which have been re-\n Fidler. 2015. Aligning books and movies: Towards placed by random words, so it is forced to keep\n story-like visual explanations by watching movies\n a distributional contextual representation of ev- and reading books. In Proceedings of the IEEE\n international conference on computer vision, pages ery input token. Additionally, because random\n 19\u201327. replacement only occurs for 1.5% of all tokens\n (i.e., 10% of 15%), this does not seem to harm\n Appendix for \u201cBERT: Pre-training of the model\u2019s language understanding capability. In\n Deep Bidirectional Transformers for Section C.2, we evaluate the impact this proce-\n Language Understanding\u201d dure.\n We organize the appendix into three sections: Compared to standard langauge model training,\n the masked LM only make predictions on 15% of\n \u2022 Additional implementation details for BERT tokens in each batch, which suggests that more\n are presented in Appendix A; pre-training steps may be required for the model\n\nBERT (Ours) OpenAI GPT ELMo\n\n T1 T2 ... TN T1 T2 ... TN T1 T2 ... TN\n\n Trm Trm ... Trm Trm Trm ... Trm\n Lstm Lstm ... Lstm Lstm Lstm ... Lstm\n\n Trm Trm ... Trm Trm Trm ... Trm\n Lstm Lstm ... Lstm Lstm Lstm ... Lstm\n\n E1 E2 ... EN E1 E2 ... EN E1 E2 ... EN\n\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\n\nto converge. In Section C.1 we demonstrate that epochs over the 3.3 billion word corpus. We\nMLM does converge marginally slower than a left- use Adam with learning rate of 1e-4, \u03b21 = 0.9,\nto-right model (which predicts every token), but \u03b22 = 0.999, L2 weight decay of 0.01, learning\nthe empirical improvements of the MLM model rate warmup over the first 10,000 steps, and linear\nfar outweigh the increased training cost. decay of the learning rate. We use a dropout prob-\n ability of 0.1 on all layers. We use a gelu acti-\nNext Sentence Prediction The next sentence vation (Hendrycks and Gimpel, 2016) rather than\nprediction task can be illustrated in the following the standard relu, following OpenAI GPT. The\nexamples. training loss is the sum of the mean masked LM\n likelihood and the mean next sentence prediction\nInput = [CLS] the man went to [MASK] store [SEP]\n likelihood.\n he bought a gallon [MASK] milk [SEP] Training of BERTBASE was performed on 4\nLabel = IsNext Cloud TPUs in Pod configuration (16 TPU chips\n total).13 Training of BERTLARGE was performed\n on 16 Cloud TPUs (64 TPU chips total). Each pre-Input = [CLS] the man [MASK] to the store [SEP]\n training took 4 days to complete.\n penguin [MASK] are flight ##less birds [SEP]\n Longer sequences are disproportionately expen-\nLabel = NotNext sive because attention is quadratic to the sequence\n length. To speed up pretraing in our experiments,\nA.2 Pre-training Procedure we pre-train the model with sequence length of\nTo generate each training input sequence, we sam- 128 for 90% of the steps. Then, we train the rest\nple two spans of text from the corpus, which we 10% of the steps of sequence of 512 to learn the\nrefer to as \u201csentences\u201d even though they are typ- positional embeddings.\nically much longer than single sentences (but can\nbe shorter also). The first sentence receives the A A.3 Fine-tuning Procedure\nembedding and the second receives the B embed- For fine-tuning, most model hyperparameters are\nding. 50% of the time B is the actual next sentence the same as in pre-training, with the exception of\nthat follows A and 50% of the time it is a random the batch size, learning rate, and number of train-\nsentence, which is done for the \u201cnext sentence pre- ing epochs. The dropout probability was always\ndiction\u201d task. They are sampled such that the com- kept at 0.1. The optimal hyperparameter values\nbined length is \u2264512 tokens. The LM masking is are task-specific, but we found the following range\napplied after WordPiece tokenization with a uni- of possible values to work well across all tasks:\nform masking rate of 15%, and no special consid-\neration given to partial word pieces. \u2022 Batch size: 16, 32\n We train with batch size of 256 sequences (256\n 13https://cloudplatform.googleblog.com/2018/06/Cloud-\nsequences * 512 tokens = 128,000 tokens/batch) TPU-now-offers-preemptible-pricing-and-global-\nfor 1,000,000 steps, which is approximately 40 availability.html\n\n\u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5 To isolate the effect of these differences, we per-\n \u2022 Number of epochs: 2, 3, 4 form ablation experiments in Section 5.1 which\n demonstrate that the majority of the improvements\n We also observed that large data sets (e.g., are in fact coming from the two pre-training tasks\n100k+ labeled training examples) were far less and the bidirectionality they enable.\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea- A.5 Illustrations of Fine-tuning on Different\nsonable to simply run an exhaustive search over Tasks\nthe above parameters and choose the model that The illustration of fine-tuning BERT on different\nperforms best on the development set. tasks can be seen in Figure 4. Our task-specific\n models are formed by incorporating BERT with\nA.4 Comparison of BERT, ELMo ,and one additional output layer, so a minimal num-\n OpenAI GPT ber of parameters need to be learned from scratch.\nHere we studies the differences in recent popular Among the tasks, (a) and (b) are sequence-level\nrepresentation learning models including ELMo, tasks while (c) and (d) are token-level tasks. In\nOpenAI GPT and BERT. The comparisons be- the figure, E represents the input embedding, Ti\ntween the model architectures are shown visually represents the contextual representation of token i,\nin Figure 3. Note that in addition to the architec- [CLS] is the special symbol for classification out-\nture differences, BERT and OpenAI GPT are fine- put, and [SEP] is the special symbol to separate\ntuning approaches, while ELMo is a feature-based non-consecutive token sequences.\napproach.\n B Detailed Experimental Setup The most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a B.1 Detailed Descriptions for the GLUE\nleft-to-right Transformer LM on a large text cor- Benchmark Experiments.\npus. In fact, many of the design decisions in BERT\n Our GLUE results in Table1 are obtained\nwere intentionally made to make it as close to\n from https://gluebenchmark.com/\nGPT as possible so that the two methods could be\n leaderboard and https://blog.\nminimally compared. The core argument of this\n openai.com/language-unsupervised.\nwork is that the bi-directionality and the two pre-\n The GLUE benchmark includes the following\ntraining tasks presented in Section 3.1 account for\n datasets, the descriptions of which were originally\nthe majority of the empirical improvements, but\n summarized in Wang et al. (2018a):\nwe do note that there are several other differences\nbetween how BERT and GPT were trained: MNLI Multi-Genre Natural Language Inference\n is a large-scale, crowdsourced entailment classifi-\n \u2022 GPT is trained on the BooksCorpus (800M cation task (Williams et al., 2018). Given a pair of\n words); BERT is trained on the BooksCor- sentences, the goal is to predict whether the sec-\n pus (800M words) and Wikipedia (2,500M ond sentence is an entailment, contradiction, or\n words). neutral with respect to the first one.\n\n \u2022 GPT uses a sentence separator ([SEP]) and QQP Quora Question Pairs is a binary classifi-\n classifier token ([CLS]) which are only in- cation task where the goal is to determine if two\n troduced at fine-tuning time; BERT learns questions asked on Quora are semantically equiv-\n [SEP], [CLS] and sentence A/B embed- alent (Chen et al., 2018).\n dings during pre-training.\n QNLI Question Natural Language Inference is\n a version of the Stanford Question Answering \u2022 GPT was trained for 1M steps with a batch\n Dataset (Rajpurkar et al., 2016) which has been size of 32,000 words; BERT was trained for\n converted to a binary classification task (Wang 1M steps with a batch size of 128,000 words.\n et al., 2018a). The positive examples are (ques-\n \u2022 GPT used the same learning rate of 5e-5 for tion, sentence) pairs which do contain the correct\n all fine-tuning experiments; BERT chooses a answer, and the negative examples are (question,\n task-specific fine-tuning learning rate which sentence) from the same paragraph which do not\n performs the best on the development set. contain the answer.\n\nClass Class\n Label Label\n\n C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 T2 ... TN\n\n BERT BERT\n\n E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 E2 ... EN\n\n [CLS] Tok1 ... TokN [SEP] Tok1 ... TokM [CLS][CLS] TokTok 11 Tok 2 ... Tok N\n\n Sentence 1 Sentence 2 Single Sentence\n\n Start/End Span O B-PER ... O\n\n C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 T2 ... TN\n\n BERT BERT\n\n E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 E2 ... EN\n\n [CLS] Tok1 ... TokN [SEP] Tok1 ... TokM [CLS] Tok 1 Tok 2 ... Tok N\n\n Question Paragraph Single Sentence\n\n Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n\nSST-2 The Stanford Sentiment Treebank is a for whether the sentences in the pair are semanti-\nbinary single-sentence classification task consist- cally equivalent (Dolan and Brockett, 2005).\ning of sentences extracted from movie reviews\n RTE Recognizing Textual Entailment is a bi-with human annotations of their sentiment (Socher\n nary entailment task similar to MNLI, but withet al., 2013).\n much less training data (Bentivogli et al., 2009).14\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classification task, where WNLI Winograd NLI is a small natural lan-\nthe goal is to predict whether an English sentence guage inference dataset (Levesque et al., 2011).\nis linguistically \u201cacceptable\u201d or not (Warstadt The GLUE webpage notes that there are issues\net al., 2018). with the construction of this dataset, 15 and every\n trained system that\u2019s been submitted to GLUE has\nSTS-B The Semantic Textual Similarity Bench- performed worse than the 65.1 baseline accuracy\nmark is a collection of sentence pairs drawn from of predicting the majority class. We therefore ex-\nnews headlines and other sources (Cer et al., clude this set to be fair to OpenAI GPT. For our\n2017). They were annotated with a score from 1 GLUE submission, we always predicted the ma-\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning. 14Note that we only report single-task fine-tuning results\n in this paper. A multitask fine-tuning approach could poten-\nMRPC Microsoft Research Paraphrase Corpus tially push the performance even further. For example, we\n did observe substantial improvements on RTE from multi-\nconsists of sentence pairs automatically extracted task training with MNLI.\nfrom online news sources, with human annotations 15https://gluebenchmark.com/faq\n\njority class. Note that the purpose of the masking strategies\n is to reduce the mismatch between pre-training\nC Additional Ablation Studies and fine-tuning, as the [MASK] symbol never ap-\n pears during the fine-tuning stage. We report the\nC.1 Effect of Number of Training Steps\n Dev results for both MNLI and NER. For NER,\nFigure 5 presents MNLI Dev accuracy after fine- we report both fine-tuning and feature-based ap-\ntuning from a checkpoint that has been pre-trained proaches, as we expect the mismatch will be am-\nfor k steps. This allows us to answer the following plified for the feature-based approach as the model\nquestions: will not have the chance to adjust the representa-\n tions.\n 1. Question: Does BERT really need such\n a large amount of pre-training (128,000 Masking Rates Dev Set Results\n words/batch * 1,000,000 steps) to achieve MASK SAME RND MNLI NER\n high fine-tuning accuracy? Fine-tune Fine-tune Feature-based\n Answer: Yes, BERTBASE achieves almost 80% 10% 10% 84.2 95.4 94.9\n 100% 0% 0% 84.3 94.9 94.0\n 1.0% additional accuracy on MNLI when 80% 0% 20% 84.1 95.2 94.6\n trained on 1M steps compared to 500k steps. 80% 20% 0% 84.4 95.2 94.7\n 0% 20% 80% 83.7 94.8 94.6\n 2. Question: Does MLM pre-training converge 0% 0% 100% 83.6 94.9 94.6\n slower than LTR pre-training, since only 15%\n of words are predicted in each batch rather Table 8: Ablation over different masking strategies.\n than every word?\n Answer: The MLM model does converge The results are presented in Table 8. In the table,\n slightly slower than the LTR model. How- MASK means that we replace the target token with\n ever, in terms of absolute accuracy the MLM the [MASK] symbol for MLM; SAME means that\n model begins to outperform the LTR model we keep the target token as is; RND means that\n almost immediately. we replace the target token with another random\n token.\nC.2 Ablation for Different Masking The numbers in the left part of the table repre-\n Procedures sent the probabilities of the specific strategies used\n during MLM pre-training (BERT uses 80%, 10%,In Section 3.1, we mention that BERT uses a\n 10%). The right part of the paper represents themixed strategy for masking the target tokens when\n Dev set results. For the feature-based approach,pre-training with the masked language model\n we concatenate the last 4 layers of BERT as the(MLM) objective. The following is an ablation\n features, which was shown to be the best approachstudy to evaluate the effect of different masking\n in Section 5.3.strategies.\n From the table it can be seen that fine-tuning is\n surprisingly robust to different masking strategies.\n 84 However, as expected, using only the MASK strat-\n egy was problematic when applying the feature-\n 82 based approach to NER. Interestingly, using only Accuracy\n 80 the RND strategy performs much worse than our\n Dev strategy as well.\n 78 MNLI\n BERTBASE (Masked LM)\n 76 BERTBASE (Left-to-Right)\n\n 200 400 600 800 1,000\n\n Pre-training Steps (Thousands)\n\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after fine-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.",
      "section_type": "conclusion",
      "page_numbers": [
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ],
      "confidence": 0.6
    }
  },
  "abstract_summary": null,
  "introduction_summary": "in this paper , we consider the problem of learning a language from a set of data . <n> we show that , in the context of a language model , the probability of a word being of the form @xmath0 $ ] is proportional to the fine-@xmath1 , ] ] we present an algorithm for the prediction of the size of the next round of the _ cosmos _ tournament , and we present a novel approach for the classification of arbitrary tasks .",
  "methods_summary": null,
  "results_summary": null,
  "discussion_summary": null,
  "conclusion_summary": "this is the first of a series of papers reporting the results of an open - ended investigation into the nature of the classification problem . <n> our method is based on the following three ideas : ( 1 ) we present a method for learning the language of a language class , ( 2 ) the problem of learning a word is , in general , np - hard , and ( 3 ) a new way of learning from text is presented . # <n> 1#2#3#4#1 * # 2 * , # 3 ( # 4 )",
  "key_concepts": [
    {
      "term": "cls",
      "definition": "position of the [CLS] token.",
      "importance_score": 0.81,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "Left-to-right language model- C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM Figure 1: Overall pre-training and fine-tuning procedures for BERT."
    },
    {
      "term": "sep",
      "definition": "To mitigate this, we do\ntoken ([SEP]).",
      "importance_score": 0.79,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "Left-to-right language model- C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM Figure 1: Overall pre-training and fine-tuning procedures for BERT."
    },
    {
      "term": "e1",
      "definition": "Input [CLS] my dog is cute [SEP] he likes play ##ing [SEP] Token\n Embeddings E[CLS] Emy Edog Eis Ecute E[SEP] Ehe Elikes Eplay E##ing E[SEP] Segment\n Embeddings EA EA EA EA EA EA EB EB EB EB EB Position\n Embeddings E0 E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 Figure 2: BERT input representation.",
      "importance_score": 0.62,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "Left-to-right language model- C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM Figure 1: Overall pre-training and fine-tuning procedures for BERT."
    },
    {
      "term": "t1",
      "definition": "Left-to-right language model- C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM Figure 1: Overall pre-training and fine-tuning procedures for BERT.",
      "importance_score": 0.59,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "Left-to-right language model- C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 C T1 ... TN T[SEP] T1\u2019 ... TM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 E[CLS] E1 ... EN E[SEP] E1\u2019 ... EM\u2019 [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM Figure 1: Overall pre-training and fine-tuning procedures for BERT."
    },
    {
      "term": "fine",
      "definition": "During fine-tuning, all parameters are fine-tuned.",
      "importance_score": 1.0,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "strictions are sub-optimal for sentence-level tasks,\n Language model pre-training has been shown to and could be very harmful when applying fine-\n be effective for improving many natural language tuning based approaches to token-level tasks sucharXiv:1810.04805v2\n processing tasks (Dai and Le, 2015; Peters et al., as question answering, where it is crucial to incor-\n 2018a; Radford et al., 2018; Howard and Ruder, porate context from both directions."
    },
    {
      "term": "pre",
      "definition": "dings during pre-training.",
      "importance_score": 1.0,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "strictions are sub-optimal for sentence-level tasks,\n Language model pre-training has been shown to and could be very harmful when applying fine-\n be effective for improving many natural language tuning based approaches to token-level tasks sucharXiv:1810.04805v2\n processing tasks (Dai and Le, 2015; Peters et al., as question answering, where it is crucial to incor-\n 2018a; Radford et al., 2018; Howard and Ruder, porate context from both directions."
    },
    {
      "term": "embeddings",
      "definition": "We compare variations of this\nsegment, and position embeddings.",
      "importance_score": 0.61,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "Unlike left-to- These approaches have been generalized to\nright language model pre-training, the MLM ob- coarser granularities, such as sentence embed-\njective enables the representation to fuse the left dings (Kiros et al., 2015; Logeswaran and Lee,\nand the right context, which allows us to pre- 2018) or paragraph embeddings (Le and Mikolov,\ntrain a deep bidirectional Transformer."
    },
    {
      "term": "model",
      "definition": "v1.1 BERT model for this task.",
      "importance_score": 1.0,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "strictions are sub-optimal for sentence-level tasks,\n Language model pre-training has been shown to and could be very harmful when applying fine-\n be effective for improving many natural language tuning based approaches to token-level tasks sucharXiv:1810.04805v2\n processing tasks (Dai and Le, 2015; Peters et al., as question answering, where it is crucial to incor-\n 2018a; Radford et al., 2018; Howard and Ruder, porate context from both directions."
    },
    {
      "term": "bert",
      "definition": "In\nOpenAI GPT and BERT.",
      "importance_score": 1.0,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "These include sentence-level tasks such as In this paper, we improve the fine-tuning based\n natural language inference (Bowman et al., 2015; approaches by proposing BERT: Bidirectional\n Williams et al., 2018) and paraphrasing (Dolan Encoder Representations from Transformers."
    },
    {
      "term": "training",
      "definition": "of held-out training data.",
      "importance_score": 1.0,
      "source_sections": [
        "conclusion",
        "introduction"
      ],
      "context": "strictions are sub-optimal for sentence-level tasks,\n Language model pre-training has been shown to and could be very harmful when applying fine-\n be effective for improving many natural language tuning based approaches to token-level tasks sucharXiv:1810.04805v2\n processing tasks (Dai and Le, 2015; Peters et al., as question answering, where it is crucial to incor-\n 2018a; Radford et al., 2018; Howard and Ruder, porate context from both directions."
    }
  ],
  "keywords": [
    "cls",
    "sep",
    "e1",
    "t1",
    "fine",
    "pre",
    "embeddings",
    "model",
    "bert",
    "training",
    "al",
    "sentence",
    "et",
    "et al",
    "tuning"
  ],
  "full_summary": "**Introduction:** in this paper , we consider the problem of learning a language from a set of data . <n> we show that , in the context of a language model , the probability of a word being of the form @xmath0 $ ] is proportional to the fine-@xmath1 , ] ] we present an algorithm for the prediction of the size of the next round of the _ cosmos _ tournament , and we present a novel approach for the classification of arbitrary tasks .\n\n**Conclusion:** this is the first of a series of papers reporting the results of an open - ended investigation into the nature of the classification problem . <n> our method is based on the following three ideas : ( 1 ) we present a method for learning the language of a language class , ( 2 ) the problem of learning a word is , in general , np - hard , and ( 3 ) a new way of learning from text is presented . # <n> 1#2#3#4#1 * # 2 * , # 3 ( # 4 )",
  "significance": "in this paper , we present a method for learning the language of a language . <n> the method is based on the idea of using a set of _ <n> language classes_. we show that the problem of learning a word is , in general , np - hard . we also show that <n> our method is a new way of learning from text ."
}