In this work, we presented the Transformer, the first sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers.
On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art.
In the former task our best
model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks.
We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video.
Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor.
Acknowledgements
We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization.
arXiv preprint
arXiv:1607.06450, 2016.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly
learning to align and translate.
CoRR, abs/1409.0473, 2014.
[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.
Le.
Massive exploration of neural
machine translation architectures.
CoRR, abs/1703.03906, 2017.
[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.
Long short-term memory-networks for machine
reading.
arXiv preprint arXiv:1601.06733, 2016.
10
[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio.
Learning phrase representations using rnn encoder-decoder for statistical
machine translation.
CoRR, abs/1406.1078, 2014.
[6] Francois Chollet.
Xception: Deep learning with depthwise separable convolutions.
arXiv
preprint arXiv:1610.02357, 2016.
[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.
Empirical evaluation
of gated recurrent neural networks on sequence modeling.
CoRR, abs/1412.3555, 2014.
[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A.
Smith.
Recurrent neural
network grammars.
In Proc.
of NAACL, 2016.
[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.
Dauphin.
Convolu-
tional sequence to sequence learning.
arXiv preprint arXiv:1705.03122v2, 2017.
[10] Alex Graves.
Generating sequences with recurrent neural networks.
arXiv preprint
arXiv:1308.0850, 2013.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for im-
age recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.
Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.
[13] Sepp Hochreiter and Jürgen Schmidhuber.
Long short-term memory.
Neural computation,
9(8):1735–1780, 1997.
[14] Zhongqiang Huang and Mary Harper.
Self-training PCFG grammars with latent annotations
across languages.
In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 832–841.
ACL, August 2009.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
Exploring
the limits of language modeling.
arXiv preprint arXiv:1602.02410, 2016.
[16] Łukasz Kaiser and Samy Bengio.
Can active memory replace attention?
In Advances in Neural
Information Processing Systems, (NIPS), 2016.
[17] Łukasz Kaiser and Ilya Sutskever.
Neural GPUs learn algorithms.
In International Conference
on Learning Representations (ICLR), 2016.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-
ray Kavukcuoglu.
Neural machine translation in linear time.
arXiv preprint arXiv:1610.10099v2,
2017.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.
Rush.
Structured attention networks.
In International Conference on Learning Representations, 2017.
[20] Diederik Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
In ICLR, 2015.
[21] Oleksii Kuchaiev and Boris Ginsburg.
Factorization tricks for LSTM networks.
arXiv preprint
arXiv:1703.10722, 2017.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio.
A structured self-attentive sentence embedding.
arXiv preprint
arXiv:1703.03130, 2017.
[23] Minh-Thang Luong, Quoc V.
Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.
Multi-task
sequence to sequence learning.
arXiv preprint arXiv:1511.06114, 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.
Effective approaches to attention-
based neural machine translation.
arXiv preprint arXiv:1508.04025, 2015.
11
[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
Building a large annotated
corpus of english: The penn treebank.
Computational linguistics, 19(2):313–330, 1993.
[26] David McClosky, Eugene Charniak, and Mark Johnson.
Effective self-training for parsing.
In
Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,
pages 152–159.
ACL, June 2006.
[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.
A decomposable attention
model.
In Empirical Methods in Natural Language Processing, 2016.
[28] Romain Paulus, Caiming Xiong, and Richard Socher.
A deep reinforced model for abstractive
summarization.
arXiv preprint arXiv:1705.04304, 2017.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
Learning accurate, compact,
and interpretable tree annotation.
In Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440.
ACL, July
2006.
[30] Ofir Press and Lior Wolf.
Using the output embedding to improve language models.
arXiv
preprint arXiv:1608.05859, 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch.
Neural machine translation of rare words
with subword units.
arXiv preprint arXiv:1508.07909, 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean.
Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer.
arXiv preprint arXiv:1701.06538, 2017.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov.
Dropout: a simple way to prevent neural networks from overfitting.
Journal of Machine
Learning Research, 15(1):1929–1958, 2014.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
End-to-end memory
networks.
In C.
Cortes, N.
D.
Lawrence, D.
D.
Lee, M.
Sugiyama, and R.
Garnett, editors,
Advances in Neural Information Processing Systems 28, pages 2440–2448.
Curran Associates,
Inc., 2015.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
Sequence to sequence learning with neural
networks.
In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision.
CoRR, abs/1512.00567, 2015.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.
Grammar as a foreign language.
In
Advances in Neural Information Processing Systems, 2015.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
Google’s neural machine
translation system: Bridging the gap between human and machine translation.
arXiv preprint
arXiv:1609.08144, 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.
Deep recurrent models with
fast-forward connections for neural machine translation.
CoRR, abs/1606.04199, 2016.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.
Fast and accurate
shift-reduce constituent parsing.
In Proceedings of the 51st Annual Meeting of the ACL (Volume
1: Long Papers), pages 434–443.
ACL, August 2013.
12
Attention Visualizations
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Figure 3: An example of the attention mechanism following long-distance dependencies in the
encoder self-attention in layer 5 of 6.
Many of the attention heads attend to a distant dependency of
the verb ‘making’, completing the phrase ‘making...more difficult’.
Attentions here shown only for
the word ‘making’.
Different colors represent different heads.
Best viewed in color.
13
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.
Top:
Full attentions for head 5.
Bottom: Isolated attentions from just the word ‘its’ for attention heads 5
and 6.
Note that the attentions are very sharp for this word.
14
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence.
We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6.
The heads clearly learned to perform different tasks.
15
We introduced BART, a pre-training approach that
learns to map corrupted documents to the original.
BART achieves similar performance to RoBERTa on
discriminative tasks, while achieving new state-of-the-
art results on a number of text generation tasks.
Fu-
ture work should explore new methods for corrupting
documents for pre-training, perhaps tailoring them to
speciﬁc end tasks.
References
Eneko Agirre, Llu’is M‘arquez, and Richard Wicen-
towski (eds.).
Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007).
Association for Computational Linguistics,
Prague, Czech Republic, June 2007.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
The PASCAL recognising textual entailment chal-
lenge.
In Machine learning challenges.
evaluat-
ing predictive uncertainty, visual object classiﬁca-
tion, and recognising tectual entailment, pp.
177–
190.
Springer, 2006.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
BERT: Pre-training of deep
bidirectional transformers for language understand-
ing.
In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pp.
4171–
4186, Minneapolis, Minnesota, June 2019.
Associa-
tion for Computational Linguistics.
doi: 10.18653/
v1/N19-1423.
URL https://www.aclweb.
org/anthology/N19-1423.
Emily Dinan, Varvara Logacheva, Valentin Malykh,
Alexander Miller, Kurt Shuster, Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
Lowe,
et al.
The second conversational in-
telligence challenge (convai2).
arXiv preprint
arXiv:1902.00098, 2019.
William B Dolan and Chris Brockett.
Automatically
constructing a corpus of sentential paraphrases.
In
Proceedings of the International Workshop on Para-
phrasing, 2005.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon.
Uniﬁed language model pre-
training for natural language understanding and gen-
eration.
arXiv preprint arXiv:1905.03197, 2019.
Sergey Edunov, Alexei Baevski, and Michael Auli.
Pre-trained language model representations for lan-
guage generation.
In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers), 2019.
Angela Fan, David Grangier, and Michael Auli.
Con-
trollable abstractive summarization.
arXiv preprint
arXiv:1711.05217, 2017.
Angela Fan, Yacine Jernite, Ethan Perez, David
Grangier, Jason Weston, and Michael Auli.
Eli5:
Long form question answering.
arXiv preprint
arXiv:1907.09190, 2019.
Dan Hendrycks and Kevin Gimpel.
Gaussian error lin-
ear units (gelus).
arXiv preprint arXiv:1606.08415,
2016.
Karl Moritz Hermann,
Tomas Kocisky,
Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom.
Teaching machines to
read and comprehend.
In Advances in neural infor-
mation processing systems, pp.
1693–1701, 2015.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,
Luke Zettlemoyer, and Omer Levy.
Spanbert: Im-
proving pre-training by representing and predicting
spans.
arXiv preprint arXiv:1907.10529, 2019.
Guillaume Lample and Alexis Conneau.
Cross-
lingual language model pretraining.
arXiv preprint
arXiv:1901.07291, 2019.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut.
Albert: A lite bert for self-supervised learn-
ing of language representations.
arXiv preprint
arXiv:1909.11942, 2019.
Hector J Levesque, Ernest Davis, and Leora Morgen-
stern.
The Winograd schema challenge.
In AAAI
Spring Symposium: Logical Formalizations of Com-
monsense Reasoning, volume 46, pp.
47, 2011.
Yang Liu and Mirella Lapata.
Text summariza-
tion with pretrained encoders.
arXiv preprint
arXiv:1908.08345, 2019.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov.
Roberta:
A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692, 2019.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean.
Efﬁcient estimation of word representations
in vector space.
arXiv preprint arXiv:1301.3781,
2013.
Shashi Narayan, Shay B Cohen, and Mirella Lapata.
Don’t give me the details, just the summary!
topic-
aware convolutional neural networks for extreme
summarization.
arXiv preprint arXiv:1808.08745,
2018.
Gabriel Pereyra,
George Tucker,
Jan Chorowski,
Łukasz Kaiser, and Geoffrey Hinton.
Regularizing
neural networks by penalizing conﬁdent output dis-
tributions.
arXiv preprint arXiv:1701.06548, 2017.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer.
Deep contextualized word representa-
tions.
arXiv preprint arXiv:1802.05365, 2018.
Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever.
Improving language un-
derstanding by generative pre-training.
URL
https://s3-us-west-2.
amazonaws.
com/openai-
assets/researchcovers/languageunsupervised/language
understanding paper.
pdf, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever.
Language mod-
els are unsupervised multitask learners.
OpenAI
Blog, 1(8), 2019.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang.
Squad: 100,000+ questions for
machine comprehension of text.
arXiv preprint
arXiv:1606.05250, 2016.
Abigail
See,
Peter
J
Liu,
and
Christopher
D
Manning.
Get to the point:
Summarization
with pointer-generator networks.
arXiv preprint
arXiv:1704.04368, 2017.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
Edinburgh neural machine translation systems for
WMT 16.
In Proceedings of the First Conference
on Machine Translation: Volume 2, Shared Task Pa-
pers, 2016.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts.
Recursive deep models for se-
mantic compositionality over a sentiment treebank.
In Proceedings of EMNLP, pp.
1631–1642, 2013.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu.
Mass: Masked sequence to sequence pre-
training for language generation.
In International
Conference on Machine Learning, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin.
Attention is all you
need.
In Advances in neural information processing
systems, pp.
5998–6008, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman.
Glue:
A multi-task benchmark and analysis platform for
natural language understanding.
arXiv preprint
arXiv:1804.07461, 2018.
Alex Warstadt, Amanpreet Singh, and Samuel R.
Bowman.
Neural network acceptability judgments.
arXiv preprint 1805.12471, 2018.
Adina Williams, Nikita Nangia, and Samuel R Bow-
man.
A broad-coverage challenge corpus for
sentence understanding through inference.
arXiv
preprint arXiv:1704.05426, 2017.
Adina Williams, Nikita Nangia, and Samuel R.
Bow-
man.
A broad-coverage challenge corpus for sen-
tence understanding through inference.
In Proceed-
ings of NAACL-HLT, 2018.
Zhilin Yang,
Zihang Dai,
Yiming Yang,
Jaime
Carbonell, Ruslan Salakhutdinov, and Quoc V
Le.
Xlnet:
Generalized autoregressive pretrain-
ing for language understanding.
arXiv preprint
arXiv:1906.08237, 2019.
Recent empirical improvements due to transfer
learning with language models have demonstrated
that rich, unsupervised pre-training is an integral
part of many language understanding systems.
In
particular, these results enable even low-resource
tasks to beneﬁt from deep unidirectional architec-
tures.
Our major contribution is further general-
izing these ﬁndings to deep bidirectional architec-
tures, allowing the same pre-trained model to suc-
cessfully tackle a broad set of NLP tasks.
References
Alan Akbik, Duncan Blythe, and Roland Vollgraf.
2018.
Contextual string embeddings for sequence
labeling.
In Proceedings of the 27th International
Conference on Computational Linguistics, pages
1638–1649.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy
Guo, and Llion Jones.
2018.
Character-level lan-
guage modeling with deeper self-attention.
arXiv
preprint arXiv:1808.04444.
Rie Kubota Ando and Tong Zhang.
2005.
A framework
for learning predictive structures from multiple tasks
and unlabeled data.
Journal of Machine Learning
Research, 6(Nov):1817–1853.
Luisa Bentivogli,
Bernardo Magnini,
Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo.
2009.
The ﬁfth PASCAL recognizing textual entailment
challenge.
In TAC.
NIST.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006.
Domain adaptation with structural correspon-
dence learning.
In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing, pages 120–128.
Association for Computa-
tional Linguistics.
Samuel R.
Bowman, Gabor Angeli, Christopher Potts,
and Christopher D.
Manning.
2015.
A large anno-
tated corpus for learning natural language inference.
In EMNLP.
Association for Computational Linguis-
tics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia.
2017.
Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation.
In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval-2017), pages 1–14, Vancou-
ver, Canada.
Association for Computational Lin-
guistics.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son.
2013.
One billion word benchmark for measur-
ing progress in statistical language modeling.
arXiv
preprint arXiv:1312.3005.
Z.
Chen, H.
Zhang, X.
Zhang, and L.
Zhao.
2018.
Quora question pairs.
Christopher Clark and Matt Gardner.
2018.
Simple
and effective multi-paragraph reading comprehen-
sion.
In ACL.
Kevin Clark, Minh-Thang Luong, Christopher D Man-
ning, and Quoc Le.
2018.
Semi-supervised se-
quence modeling with cross-view training.
In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1914–
1925.
Ronan Collobert and Jason Weston.
2008.
A uniﬁed
architecture for natural language processing: Deep
neural networks with multitask learning.
In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167.
ACM.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes.
2017.
Supervised
learning of universal sentence representations from
natural language inference data.
In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark.
Association for Computational
Linguistics.
Andrew M Dai and Quoc V Le.
2015.
Semi-supervised
sequence learning.
In Advances in neural informa-
tion processing systems, pages 3079–3087.
J.
Deng, W.
Dong, R.
Socher, L.-J.
Li, K.
Li, and L.
Fei-
Fei.
2009.
ImageNet: A Large-Scale Hierarchical
Image Database.
In CVPR09.
William B Dolan and Chris Brockett.
2005.
Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
William Fedus, Ian Goodfellow, and Andrew M Dai.
2018.
Maskgan: Better text generation via ﬁlling in
the .
arXiv preprint arXiv:1801.07736.
Dan Hendrycks and Kevin Gimpel.
2016.
Bridging
nonlinearities and stochastic regularizers with gaus-
sian error linear units.
CoRR, abs/1606.08415.
Felix Hill, Kyunghyun Cho, and Anna Korhonen.
2016.
Learning distributed representations of sentences
from unlabelled data.
In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
Association for Computa-
tional Linguistics.
Jeremy Howard and Sebastian Ruder.
2018.
Universal
language model ﬁne-tuning for text classiﬁcation.
In
ACL.
Association for Computational Linguistics.
Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou.
2018.
Reinforced
mnemonic reader for machine reading comprehen-
sion.
In IJCAI.
Yacine Jernite, Samuel R.
Bowman, and David Son-
tag.
2017.
Discourse-based objectives for fast un-
supervised sentence representation learning.
CoRR,
abs/1705.00557.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer.
2017.
Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion.
In ACL.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler.
2015.
Skip-thought vectors.
In
Advances in neural information processing systems,
pages 3294–3302.
Quoc Le and Tomas Mikolov.
2014.
Distributed rep-
resentations of sentences and documents.
In Inter-
national Conference on Machine Learning, pages
1188–1196.
Hector J Levesque, Ernest Davis, and Leora Morgen-
stern.
2011.
The winograd schema challenge.
In
Aaai spring symposium: Logical formalizations of
commonsense reasoning, volume 46, page 47.
Lajanugen Logeswaran and Honglak Lee.
2018.
An
efﬁcient framework for learning sentence represen-
tations.
In International Conference on Learning
Representations.
Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher.
2017.
Learned in translation: Con-
textualized word vectors.
In NIPS.
Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016.
context2vec: Learning generic context em-
bedding with bidirectional LSTM.
In CoNLL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean.
2013.
Distributed representa-
tions of words and phrases and their compositional-
ity.
In Advances in Neural Information Processing
Systems 26, pages 3111–3119.
Curran Associates,
Inc.
Andriy Mnih and Geoffrey E Hinton.
2009.
A scal-
able hierarchical distributed language model.
In
D.
Koller, D.
Schuurmans, Y.
Bengio, and L.
Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 1081–1088.
Curran As-
sociates, Inc.
Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit.
2016.
A decomposable attention
model for natural language inference.
In EMNLP.
Jeffrey Pennington, Richard Socher, and Christo-
pher D.
Manning.
2014.
Glove: Global vectors for
word representation.
In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.
Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power.
2017.
Semi-supervised se-
quence tagging with bidirectional language models.
In ACL.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer.
2018a.
Deep contextualized word rep-
resentations.
In NAACL.
Matthew Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih.
2018b.
Dissecting contextual
word embeddings: Architecture and representation.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1499–1509.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever.
2018.
Improving language under-
standing with unsupervised learning.
Technical re-
port, OpenAI.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang.
2016.
Squad: 100,000+ questions for
machine comprehension of text.
In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi.
2017.
Bidirectional attention
ﬂow for machine comprehension.
In ICLR.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts.
2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.
Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.
2018.
U-net:
Machine reading comprehension
with unanswerable questions.
arXiv preprint
arXiv:1810.06638.
Wilson L Taylor.
1953.
Cloze procedure:
A new
tool for measuring readability.
Journalism Bulletin,
30(4):415–433.
Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition.
In
CoNLL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: A simple and general method
for semi-supervised learning.
In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’10, pages 384–394.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin.
2017.
Attention is all
you need.
In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol.
2008.
Extracting and
composing robust features with denoising autoen-
coders.
In Proceedings of the 25th international
conference on Machine learning, pages 1096–1103.
ACM.
Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman.
2018a.
Glue: A multi-task benchmark and analysis platform
for natural language understanding.
In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP,
pages 353–355.
Wei Wang, Ming Yan, and Chen Wu.
2018b.
Multi-
granularity hierarchical attention fusion networks
for reading comprehension and question answering.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers).
Association for Computational Lin-
guistics.
Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man.
2018.
Neural network acceptability judg-
ments.
arXiv preprint arXiv:1805.12471.
Adina Williams, Nikita Nangia, and Samuel R Bow-
man.
2018.
A broad-coverage challenge corpus
for sentence understanding through inference.
In
NAACL.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, et al.
2016.
Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation.
arXiv preprint
arXiv:1609.08144.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson.
2014.
How transferable are features in deep
neural networks?
In Advances in neural information
processing systems, pages 3320–3328.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le.
2018.
QANet: Combining local convolution
with global self-attention for reading comprehen-
sion.
In ICLR.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi.
2018.
Swag: A large-scale adversarial dataset
for grounded commonsense inference.
In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler.
2015.
Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books.
In Proceedings of the IEEE
international conference on computer vision, pages
19–27.
Appendix for “BERT: Pre-training of
Deep Bidirectional Transformers for
Language Understanding”
We organize the appendix into three sections:
• Additional implementation details for BERT
are presented in Appendix A;
• Additional details for our experiments are
presented in Appendix B; and
• Additional ablation studies are presented in
Appendix C.
We present additional ablation studies for
BERT including:
– Effect of Number of Training Steps; and
– Ablation for Different Masking Proce-
dures.
A
Additional Details for BERT
A.1
Illustration of the Pre-training Tasks
We provide examples of the pre-training tasks in
the following.
Masked LM and the Masking Procedure
As-
suming the unlabeled sentence is
my dog is
hairy, and during the random masking procedure
we chose the 4-th token (which corresponding to
hairy), our masking procedure can be further il-
lustrated by
• 80% of the time: Replace the word with the
[MASK] token, e.g., my dog is hairy →
my dog is [MASK]
• 10% of the time: Replace the word with a
random word, e.g., my dog is hairy →my
dog is apple
• 10% of the time:
Keep the word un-
changed, e.g., my dog is hairy →my dog
is hairy.
The purpose of this is to bias the
representation towards the actual observed
word.
The advantage of this procedure is that the
Transformer encoder does not know which words
it will be asked to predict or which have been re-
placed by random words, so it is forced to keep
a distributional contextual representation of ev-
ery input token.
Additionally, because random
replacement only occurs for 1.5% of all tokens
(i.e., 10% of 15%), this does not seem to harm
the model’s language understanding capability.
In
Section C.2, we evaluate the impact this proce-
dure.
Compared to standard langauge model training,
the masked LM only make predictions on 15% of
tokens in each batch, which suggests that more
pre-training steps may be required for the model
BERT (Ours)
Trm
Trm
Trm
Trm
Trm
Trm
...
...
Trm
Trm
Trm
Trm
Trm
Trm
...
...
OpenAI GPT
Lstm
ELMo
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
 T1
T2
 TN
...
...
...
...
...
E1
E2
 EN
...
T1
T2
TN
...
E1
E2
 EN
...
T1
T2
 TN
...
E1
E2
 EN
...
Figure 3: Differences in pre-training model architectures.
BERT uses a bidirectional Transformer.
OpenAI GPT
uses a left-to-right Transformer.
ELMo uses the concatenation of independently trained left-to-right and right-to-
left LSTMs to generate features for downstream tasks.
Among the three, only BERT representations are jointly
conditioned on both left and right context in all layers.
In addition to the architecture differences, BERT and
OpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.
to converge.
In Section C.1 we demonstrate that
MLM does converge marginally slower than a left-
to-right model (which predicts every token), but
the empirical improvements of the MLM model
far outweigh the increased training cost.
Next Sentence Prediction
The next sentence
prediction task can be illustrated in the following
examples.
Input = [CLS] the man went to [MASK] store [SEP]
he bought a gallon [MASK] milk [SEP]
Label = IsNext
Input = [CLS] the man [MASK] to the store [SEP]
penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
A.2
Pre-training Procedure
To generate each training input sequence, we sam-
ple two spans of text from the corpus, which we
refer to as “sentences” even though they are typ-
ically much longer than single sentences (but can
be shorter also).
The ﬁrst sentence receives the A
embedding and the second receives the B embed-
ding.
50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the “next sentence pre-
diction” task.
They are sampled such that the com-
bined length is ≤512 tokens.
The LM masking is
applied after WordPiece tokenization with a uni-
form masking rate of 15%, and no special consid-
eration given to partial word pieces.
We train with batch size of 256 sequences (256
sequences * 512 tokens = 128,000 tokens/batch)
for 1,000,000 steps, which is approximately 40
epochs over the 3.3 billion word corpus.
We
use Adam with learning rate of 1e-4, β1 = 0.9,
β2 = 0.999, L2 weight decay of 0.01, learning
rate warmup over the ﬁrst 10,000 steps, and linear
decay of the learning rate.
We use a dropout prob-
ability of 0.1 on all layers.
We use a gelu acti-
vation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT.
The
training loss is the sum of the mean masked LM
likelihood and the mean next sentence prediction
likelihood.
Training of BERTBASE was performed on 4
Cloud TPUs in Pod conﬁguration (16 TPU chips
total).13 Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total).
Each pre-
training took 4 days to complete.
Longer sequences are disproportionately expen-
sive because attention is quadratic to the sequence
length.
To speed up pretraing in our experiments,
we pre-train the model with sequence length of
128 for 90% of the steps.
Then, we train the rest
10% of the steps of sequence of 512 to learn the
positional embeddings.
A.3
Fine-tuning Procedure
For ﬁne-tuning, most model hyperparameters are
the same as in pre-training, with the exception of
the batch size, learning rate, and number of train-
ing epochs.
The dropout probability was always
kept at 0.1.
The optimal hyperparameter values
are task-speciﬁc, but we found the following range
of possible values to work well across all tasks:
• Batch size: 16, 32
13https://cloudplatform.googleblog.com/2018/06/Cloud-
TPU-now-offers-preemptible-pricing-and-global-
availability.html
• Learning rate (Adam): 5e-5, 3e-5, 2e-5
• Number of epochs: 2, 3, 4
We also observed that large data sets (e.g.,
100k+ labeled training examples) were far less
sensitive to hyperparameter choice than small data
sets.
Fine-tuning is typically very fast, so it is rea-
sonable to simply run an exhaustive search over
the above parameters and choose the model that
performs best on the development set.
A.4
Comparison of BERT, ELMo ,and
OpenAI GPT
Here we studies the differences in recent popular
representation learning models including ELMo,
OpenAI GPT and BERT.
The comparisons be-
tween the model architectures are shown visually
in Figure 3.
Note that in addition to the architec-
ture differences, BERT and OpenAI GPT are ﬁne-
tuning approaches, while ELMo is a feature-based
approach.
The most comparable existing pre-training
method to BERT is OpenAI GPT, which trains a
left-to-right Transformer LM on a large text cor-
pus.
In fact, many of the design decisions in BERT
were intentionally made to make it as close to
GPT as possible so that the two methods could be
minimally compared.
The core argument of this
work is that the bi-directionality and the two pre-
training tasks presented in Section 3.1 account for
the majority of the empirical improvements, but
we do note that there are several other differences
between how BERT and GPT were trained:
• GPT is trained on the BooksCorpus (800M
words); BERT is trained on the BooksCor-
pus (800M words) and Wikipedia (2,500M
words).
• GPT uses a sentence separator ([SEP]) and
classiﬁer token ([CLS]) which are only in-
troduced at ﬁne-tuning time; BERT learns
[SEP], [CLS] and sentence A/B embed-
dings during pre-training.
• GPT was trained for 1M steps with a batch
size of 32,000 words; BERT was trained for
1M steps with a batch size of 128,000 words.
• GPT used the same learning rate of 5e-5 for
all ﬁne-tuning experiments; BERT chooses a
task-speciﬁc ﬁne-tuning learning rate which
performs the best on the development set.
To isolate the effect of these differences, we per-
form ablation experiments in Section 5.1 which
demonstrate that the majority of the improvements
are in fact coming from the two pre-training tasks
and the bidirectionality they enable.
A.5
Illustrations of Fine-tuning on Different
Tasks
The illustration of ﬁne-tuning BERT on different
tasks can be seen in Figure 4.
Our task-speciﬁc
models are formed by incorporating BERT with
one additional output layer, so a minimal num-
ber of parameters need to be learned from scratch.
Among the tasks, (a) and (b) are sequence-level
tasks while (c) and (d) are token-level tasks.
In
the ﬁgure, E represents the input embedding, Ti
represents the contextual representation of token i,
[CLS] is the special symbol for classiﬁcation out-
put, and [SEP] is the special symbol to separate
non-consecutive token sequences.
B
Detailed Experimental Setup
B.1
Detailed Descriptions for the GLUE
Benchmark Experiments.
Our
GLUE
Content Selection
Bottom-Up Attention
Figure 2: Overview of the selection and generation pro-
cesses described throughout Section 4.
nism (Vinyals et al., 2015) to copy words from
the source.
Copy models extend the decoder by
predicting a binary soft switch zj that determines
whether the model copies or generates.
The copy
distribution is a probability distribution over the
source text, and the joint distribution is computed
as a convex combination of the two parts of the
model,
p(yj | y1:j-1, x) =
p(zj = 1 | y1:j-1, x) × p(yj | zj = 1, y1:j-1, x)+
p(zj = 0 | y1:j-1, x) × p(yj | zj = 0, y1:j-1, x)
(1)
where the two parts represent copy and generation
distribution respectively.
Following the pointer-
generator model of See et al.
(2017), we reuse the
attention p(aj|x, y1:j−1) distribution as copy dis-
tribution, i.e.
the copy probability of a token in the
source w through the copy attention is computed
as the sum of attention towards all occurrences of
w.
During training, we maximize marginal likeli-
hood with the latent switch variable.
4
Bottom-Up Attention
We next consider techniques for incorporating a
content selection into abstractive summarization,
illustrated in Figure 2.
4.1
Content Selection
We deﬁne the content selection problem as a word-
level extractive summarization task.
While there
has been signiﬁcant work on custom extractive
summarization (see related work), we make a sim-
plifying assumption and treat it as a sequence tag-
ging problem.
Let t1, .
.
.
, tn denote binary tags
for each of the source tokens, i.e.
1 if a word is
copied in the target sequence and 0 otherwise.
While there is no supervised data for this task,
we can generate training data by aligning the sum-
maries to the document.
We deﬁne a word xi as
copied if (1) it is part of the longest possible sub-
sequence of tokens s = xi−j:i:i+k, for integers
j ≤i; k ≤(n −i), if s ∈x and s ∈y, and
(2) there exists no earlier sequence u with s = u.
We use a standard bidirectional LSTM model
trained with maximum likelihood for the sequence
labeling problem.
Recent results have shown that
better word representations can lead to signiﬁ-
cantly improved performance in sequence tagging
tasks (Peters et al., 2017).
Therefore, we ﬁrst
map each token wi into two embedding channels
e(w)
i
and e(c)
i .
The e(w) embedding represents a
static channel of pre-trained word embeddings,
e.g.
GLoVE (Pennington et al., 2014).
The e(c) are
contextual embeddings from a pretrained language
model, e.g.
ELMo (Peters et al., 2018) which uses
a character-aware token embedding (Kim et al.,
2016) followed by two bidirectional LSTM lay-
ers h(1)
i
and h(2)
i .
The contextual embeddings are
ﬁne-tuned to learn a task-speciﬁc embedding e(c)
i
as a linear combination of the states of each LSTM
layer and the token embedding,
e(c)
i
= γ ×
2
X
ℓ=0
sj × h(ℓ)
i ,
with γ and s0,1,2 as trainable parameters.
Since
these embeddings only add four additional param-
eters to the tagger, it remains very data-efﬁcient
despite the high-dimensional embedding space.
Both embeddings are concatenated into a sin-
gle vector that is used as input to a bidirectional
LSTM, which computes a representation hi for a
word wi.
We can then calculate the probability
qi that the word is selected as σ(Wshi + bs) with
trainable parameters Ws and bs.
4.2
Bottom-Up Copy Attention
Inspired by work in bottom-up attention for im-
ages (Anderson et al., 2017) which restricts atten-
tion to predetermined bounding boxes within an
image, we use these attention masks to limit the
available selection of the pointer-generator model.
As shown in Figure 1, a common mistake made
by neural copy models is copying very long se-
quences or even whole sentences.
In the base-
line model, over 50% of copied tokens are part
of copy sequences that are longer than 10 tokens,
whereas this number is only 10% for reference
summaries.
While bottom-up attention could also
be used to modify the source encoder representa-
tions, we found that a standard encoder over the
full text was effective at aggregation and therefore
limit the bottom-up step to attention masking.
Concretely, we ﬁrst train a pointer-generator
model on the full dataset as well as the content
selector deﬁned above.
At inference time, to gen-
erate the mask, the content selector computes se-
lection probabilities q1:n for each token in a source
document.
The selection probabilities are used to
modify the copy attention distribution to only in-
clude tokens identiﬁed by the selector.
Let ai
j de-
note the attention at decoding step j to encoder
word i.
Given a threshold ϵ, the selection is ap-
plied as a hard mask, such that
p(˜ai
j|x, y1:j−1) =
(
p(ai
j|x, y1:j−1)
qi > ϵ
0
ow.
To ensure that Eq.
1 still yields a correct probabil-
ity distribution, we ﬁrst multiply p(˜aj|x, y1:j−1)
by a normalization parameter λ and then renor-
malize the distribution.
The resulting normalized
distribution can be used to directly replace a as the
new copy probabilities.
4.3
End-to-End Alternatives
Two-step BOTTOM-UP attention has the advan-
tage of training simplicity.
In theory, though, stan-
dard copy attention should be able to learn how to
perform content selection as part of the end-to-end
training.
We consider several other end-to-end ap-
proaches for incorporating content selection into
neural training.
Method 1: (MASK ONLY): We ﬁrst consider
whether the alignment used in the bottom-up ap-
proach could help a standard summarization sys-
tem.
Inspired by Nallapati et al.
(2017), we in-
vestigate whether aligning the summary and the
source during training and ﬁxing the gold copy at-
tention to pick the ”correct” source word is beneﬁ-
cial.
We can think of this approach as limiting the
set of possible copies to a ﬁxed source word.
Here
the training is changed, but no mask is used at test
time.
Method 2 (MULTI-TASK): Next, we investigate
whether the content selector can be trained along-
side the abstractive system.
We ﬁrst test this hy-
pothesis by posing summarization as a multi-task
problem and training the tagger and summariza-
tion model with the same features.
For this setup,
we use a shared encoder for both abstractive sum-
marization and content selection.
At test time, we
apply the same masking method as bottom-up at-
tention.
Method 3 (DIFFMASK):
Finally we con-
sider training the full system end-to-end with
the mask during training.
Here we jointly op-
timize both objectives, but use predicted selec-
tion probabilities to softly mask the copy attention
p(˜ai
j|x, y1:j−1) = p(ai
j|x, y1:j−1)×qi, which leads
to a fully differentiable model.
This model is used
with the same soft mask at test time.
5
Inference
Several authors have noted that longer-form neural
generation still has signiﬁcant issues with incor-
rect length and repeated words than in short-form
problems like translation.
Proposed solutions in-
clude modifying models with extensions such as a
coverage mechanism (Tu et al., 2016; See et al.,
2017) or intra-sentence attention
(Cheng et al.,
2016; Paulus et al., 2017).
We instead stick to
the theme of modifying inference, and modify
the scoring function to include a length penalty
lp and a coverage penalty cp, and is deﬁned as
s(x, y) = log p(y|x)/lp(x) + cp(x; y).
Length: To encourage the generation of longer
sequences, we apply length normalizations during
beam search.
We use the length penalty by Wu
et al.
(2016), which is formulated as
lp(y) = (5 + |y|)α
(5 + 1)α ,
with a tunable parameter α, where increasing α
leads to longer summaries.
We additionally set a
minimum length based on the training data.
Repeats: Copy models often repeatedly attend
to the same source tokens, generating the same
phrase multiple times.
We introduce a new sum-
mary speciﬁc coverage penalty,
cp(x; y) = β

−n +
n
X
i=1
max

1.0,
m
X
j=1
aj
i



.
Intuitively, this penalty increases whenever the
decoder directs more than 1.0 of total attention
within a sequence towards a single encoded to-
ken.
By selecting a sufﬁciently high β, this penalty
blocks summaries whenever they would lead to
repetitions.
Additionally, we follow (Paulus et al.,
2017) and restrict the beam search to never repeat
trigrams.
6
Data and Experiments
We evaluate our approach on the CNN-DM cor-
pus (Hermann et al., 2015; Nallapati et al., 2016a),
and the NYT corpus (Sandhaus, 2008), which are
both standard corpora for news summarization.
The summaries for the CNN-DM corpus are bul-
let points for the articles shown on their respective
websites, whereas the NYT corpus contains sum-
maries written by library scientists.
CNN-DM
summaries are full sentences, with on average 66
tokens (σ = 26) and 4.9 bullet points.
NYT sum-
maries are not always complete sentences and are
shorter, with on average 40 tokens (σ = 27) and
1.9 bullet points.
Following See et al.
(2017), we
use the non-anonymized version of the CNN-DM
corpus and truncate source documents to 400 to-
kens and the target summaries to 100 tokens in
training and validation sets.
For experiments with
the NYT corpus, we use the preprocessing de-
scribed by Paulus et al.
(2017), and additionally
remove author information and truncate source
documents to 400 tokens instead of 800.
These
changes lead to an average of 326 tokens per arti-
cle, a decrease from the 549 tokens with 800 token
truncated articles.
The target (non-copy) vocabu-
lary is limited to 50,000 tokens for all models.
The content selection model uses pre-trained
GloVe embeddings of size 100, and ELMo with
size 1024.
The bi-LSTM has two layers and a hid-
den size of 256.
Dropout is set to 0.5, and the
model is trained with Adagrad, an initial learning
rate of 0.15, and an initial accumulator value of
0.1.
We limit the number of training examples to
100,000 on either corpus, which only has a small
impact on performance.
For the jointly trained
content selection models, we use the same conﬁg-
uration as the abstractive model.
For the base model, we re-implemented the
Pointer-Generator model as described by See et al.
(2017).
To have a comparable number of parame-
ters to previous work, we use an encoder with 256
hidden states for both directions in the one-layer
LSTM, and 512 for the one-layer decoder.
The
embedding size is set to 128.
The model is trained
with the same Adagrad conﬁguration as the con-
tent selector.
Additionally, the learning rate halves
after each epoch once the validation perplexity
does not decrease after an epoch.
We do not use
dropout and use gradient-clipping with a maxi-
mum norm of 2.
We found that increasing model
size or using the Transformer (Vaswani et al.,
Method
R-1
R-2
R-L
Pointer-Generator (See et al., 2017)
36.44
15.66
33.42
Pointer-Generator + Coverage (See et al., 2017)
39.53
17.28
36.38
ML + Intra-Attention (Paulus et al., 2017)
38.30
14.81
35.49
ML + RL (Paulus et al., 2017)
39.87
15.82
36.90
Saliency + Entailment reward (Pasunuru and Bansal, 2018)
40.43
18.00
37.10
Key information guide network (Li et al., 2018a)
38.95
17.12
35.68
Inconsistency loss (Hsu et al., 2018)
40.68
17.97
37.13
Sentence Rewriting (Chen and Bansal, 2018)
40.88
17.80
38.54
Pointer-Generator (our implementation)
36.25
16.17
33.41
Pointer-Generator + Coverage Penalty
39.12
17.35
36.12
CopyTransformer + Coverage Penalty
39.25
17.54
36.45
Pointer-Generator + Mask Only
37.70
15.63
35.49
Pointer-Generator + Multi-Task
37.67
15.59
35.47
Pointer-Generator + DiffMask
38.45
16.88
35.81
Bottom-Up Summarization
41.22
18.68
38.34
Bottom-Up Summarization (CopyTransformer)
40.96
18.38
38.16
Table 1: Results of abstractive summarizers on the CNN-DM dataset.2 The ﬁrst section shows encoder-decoder
abstractive baselines trained with cross-entropy.
The second section describes reinforcement-learning based ap-
proaches.
The third section presents our baselines and the attention masking methods described in this work.
2017) can lead to slightly improved performance,
but at the cost of increased training time and pa-
rameters.
We report numbers of a Transformer
with copy-attention, which we denote CopyTrans-
former.
In this model, we randomly choose one
of the attention-heads as the copy-distribution, and
otherwise follow the parameters of the big Trans-
former by Vaswani et al.
(2017).
All inference parameters are tuned on a 200 ex-
ample subset of the validation set.
Length penalty
parameter α and copy mask ϵ differ across mod-
els, with α ranging from 0.6 to 1.4, and ϵ ranging
from 0.1 to 0.2.
The minimum length of the gen-
erated summary is set to 35 for CNN-DM and 6
for NYT.
While the Pointer-Generator uses a beam
size of 5 and does not improve with a larger beam,
we found that bottom-up attention requires a larger
beam size of 10.
The coverage penalty parameter
β is set to 10, and the copy attention normaliza-
tion parameter λ to 2 for both approaches.
We
use AllenNLP (Gardner et al., 2018) for the con-
tent selector, and OpenNMT-py for the abstractive
models (Klein et al., 2017).3.
3Code and reproduction instructions can be found at
https://github.com/sebastianGehrmann/
bottom-up-summary
3These results compare on the non-anonymized version of
this corpus used by (See et al., 2017).
The best results on the
anonymized version are R1:41.69 R2:19.47 RL:37.92 from
This work presents a simple but accurate con-
tent selection model for summarization that iden-
tiﬁes phrases within a document that are likely in-
cluded in its summary.
We showed that this con-
tent selector can be used for a bottom-up atten-
tion that restricts the ability of abstractive sum-
marizers to copy words from the source.
The
combined bottom-up summarization system leads
to improvements in ROUGE scores of over two
points on both the CNN-DM and NYT corpora.
A
comparison to end-to-end trained methods showed
that this particular problem cannot be easily solved
with a single model, but instead requires ﬁne-
tuned inference restrictions.
Finally, we showed
that this technique, due to its data-efﬁciency, can
be used to adjust a trained model with few data
points, making it easy to transfer to a new do-
main.
Preliminary work that investigates similar
bottom-up approaches in other domains that re-
quire a content selection, such as grammar correc-
tion, or data-to-text generation, have shown some
promise and will be investigated in future work.
Acknowledgements
We would like to thank Barbara J.
Grosz for help-
ful discussions and feedback on early stages of
this work.
We further thank the three anonymous
reviewers.
This work was supported by a Sam-
sung Research Award.
YD was funded in part by
a Bloomberg Research Award.
SG was funded in
part by NIH grant 5R01CA204585-02.
References
Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang.
2017.
Bottom-up and top-down attention
for image captioning and vqa.
arXiv preprint
arXiv:1707.07998.
Valerie Anderson and Suzanne Hidi.
1988.
Teach-
ing students to summarize.
Educational leadership,
46(4):26–28.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio.
2014.
Neural machine translation by jointly
learning to align and translate.
arXiv preprint
arXiv:1409.0473.
Duy Duc An Bui, Guilherme Del Fiol, John F Hurdle,
and Siddhartha Jonnalagadda.
2016.
Extractive text
summarization system to aid data extraction from
full text in systematic review development.
Journal
of biomedical informatics, 64:265–272.
Timothy J Buschman and Earl K Miller.
2007.
Top-
down versus bottom-up control of attention in the
prefrontal and posterior parietal cortices.
science,
315(5820):1860–1862.
Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi.
2018.
Deep communicating agents for
abstractive summarization.
In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), volume 1, pages 1662–1675.
Yen-Chun Chen and Mohit Bansal.
2018.
Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting.
arXiv preprint arXiv:1805.11080.
Jianpeng Cheng, Li Dong, and Mirella Lapata.
2016.
Long short-term memory-networks for machine
reading.
arXiv preprint arXiv:1601.06733.
Jianpeng Cheng and Mirella Lapata.
2016.
Neural
summarization by extracting sentences and words.
arXiv preprint arXiv:1603.07252.
Sumit Chopra, Michael Auli, and Alexander M Rush.
2016.
Abstractive sentence summarization with at-
tentive recurrent neural networks.
In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.
Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, and Nazli
Goharian.
2018.
A discourse-aware attention model
for abstractive summarization of long documents.
In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), volume 2, pages 615–621.
Alexander Dlikman and Mark Last.
2016.
Using
machine learning methods and linguistic features
in single-document extractive summarization.
In
DMNLP@ PKDD/ECML, pages 1–8.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003.
Hedge trimmer: A parse-and-trim approach
to headline generation.
In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1–8.
Association for Computational Lin-
guistics.
Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016.
Learning-based single-document summariza-
tion with compression and anaphoricity constraints.
arXiv preprint arXiv:1603.08887.
Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe-
ters, Michael Schmitz, and Luke Zettlemoyer.
2018.
Allennlp: A deep semantic natural language pro-
cessing platform.
arXiv preprint arXiv:1803.07640.
Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li.
2016.
Incorporating copying mechanism in
sequence-to-sequence learning.
arXiv preprint
arXiv:1603.06393.
Karl Moritz Hermann,
Tomas Kocisky,
Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom.
2015.
Teaching ma-
chines to read and comprehend.
In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.
Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun.
2018.
A uni-
ﬁed model for extractive and abstractive summa-
rization using inconsistency loss.
arXiv preprint
arXiv:1805.06266.
Hongyan Jing and Kathleen R McKeown.
1999.
The decomposition of human-written summary sen-
tences.
In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 129–136.
Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush.
2016.
Character-aware neural language
models.
In AAAI, pages 2741–2749.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M Rush.
2017.
Opennmt:
Open-source toolkit for neural machine translation.
arXiv preprint arXiv:1701.02810.
Chenliang Li, Weiran Xu, Si Li, and Sheng Gao.
2018a.
Guiding generation for abstractive text summariza-
tion based on key information guide network.
In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), volume 2, pages 55–60.
Piji Li, Lidong Bing, and Wai Lam.
2018b.
Actor-
critic based training framework for abstractive sum-
marization.
arXiv preprint arXiv:1803.11070.
Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich,
Ryan Sepassi,
Lukasz Kaiser,
and
Noam Shazeer.
2018.
Generating wikipedia by
summarizing long sequences.
arXiv preprint
arXiv:1801.10198.
Ramesh Nallapati, Feifei Zhai, and Bowen Zhou.
2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments.
In AAAI, pages 3075–3081.
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al.
2016a.
Abstractive text sum-
marization using sequence-to-sequence rnns and be-
yond.
arXiv preprint arXiv:1602.06023.
Ramesh Nallapati, Bowen Zhou, and Mingbo Ma.
2016b.
Classify or select: Neural architectures for
extractive document summarization.
arXiv preprint
arXiv:1611.04244.
Ramakanth Pasunuru and Mohit Bansal.
2018.
Multi-
reward reinforced summarization with saliency and
entailment.
In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers), volume 2,
pages 646–653.
Romain Paulus, Caiming Xiong, and Richard Socher.
2017.
A deep reinforced model for abstractive sum-
marization.
arXiv preprint arXiv:1705.04304.
Jeffrey Pennington, Richard Socher, and Christopher
Manning.
2014.
Glove: Global vectors for word
representation.
In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.
Matthew E Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power.
2017.
Semi-supervised se-
quence tagging with bidirectional language models.
arXiv preprint arXiv:1705.00108.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer.
2018.
Deep contextualized word rep-
resentations.
arXiv preprint arXiv:1802.05365.
Alexander M Rush, Sumit Chopra, and Jason We-
ston.
2015.
A neural attention model for ab-
stractive sentence summarization.
arXiv preprint
arXiv:1509.00685.
Evan Sandhaus.
2008.
The new york times annotated
corpus.
Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Abigail See, Peter J Liu, and Christopher D Man-
ning.
2017.
Get to the point:
Summarization
with pointer-generator networks.
arXiv preprint
arXiv:1704.04368.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.
Sequence to sequence learning with neural net-
works.
In Advances in neural information process-
ing systems, pages 3104–3112.
Jiwei Tan, Xiaojun Wan, and Jianguo Xiao.
2017.
Abstractive document summarization with a graph-
based attentional neural model.
In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1:
Long Papers),
volume 1, pages 1171–1181.
Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua
Liu, and Hang Li.
2016.
Modeling coverage
for neural machine translation.
arXiv preprint
arXiv:1601.04811.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin.
2017.
Attention is all
you need.
In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015.
Pointer networks.
In Advances in Neural In-
formation Processing Systems, pages 2692–2700.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, et al.
2016.
Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation.
arXiv preprint
arXiv:1609.08144.
Wenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel
Urtasun.
2016.
Efﬁcient summarization with
read-again and copy mechanism.
arXiv preprint
arXiv:1611.03382.
Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,
Ming Zhou, and Tiejun Zhao.
2018.
Neural docu-
ment summarization by jointly learning to score and
select sentences.
In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
654–663.
Examples
Generated summary
Reference
green bay packers successful season is largely due to quarterback brett
favre
S2S
ahman green rushed for 000 yards in 00-00 victory over the giants .
true
, dorsey levens , good enough to start for most teams but now green ’s
backup , contributed kickoff returns of 00 , 00 and 00 yards .
Content Selection
playoff-bound green bay packers beat the giants in the 00-00 victory .
the
packers won three games and six of each other .
Reference
paul byers , pioneer of visual anthropology , dies at age 00
S2S
paul byers , an early practitioner of mead , died on dec.
00 at his home in
manhattan .
he enlisted in the navy , which trained him as a cryptanalyst
and stationed him in australia .
Content Selection
paul byers , an early practitioner of anthropology , pioneered with mar-
garet mead .
Table 7: Domain-transfer examples.
A
Domain Transfer Examples
We present two generated summaries for the
CNN-DM to NYT domain transfer experiment
in Table 7.
S2S refers to a Pointer-Generator
with Coverage Penalty trained on CNN-DM that
scores 20.6 ROUGE-L on the NYT dataset.
The
content-selection improves this to 27.7 ROUGE-L
without any ﬁne-tuning of the S2S model.
We presented DARTS, a simple yet efﬁcient architecture search algorithm for both convolutional and
recurrent networks.
By searching in a continuous space, DARTS is able to match or outperform the
state-of-the-art non-differentiable architecture search methods on image classiﬁcation and language
modeling tasks with remarkable efﬁciency improvement by several orders of magnitude.
There are many interesting directions to improve DARTS further.
For example, the current method
may suffer from discrepancies between the continuous architecture encoding and the derived discrete
architecture.
This could be alleviated, e.g., by annealing the softmax temperature (with a suitable
schedule) to enforce one-hot selection.
It would also be interesting to investigate performance-aware
architecture derivation schemes based on the shared parameters learned during the search process.
ACKNOWLEDGEMENTS
The authors would like to thank Zihang Dai, Hieu Pham and Zico Kolter for useful discussions.
REFERENCES
Karim Ahmed and Lorenzo Torresani.
Connectivity learning in multi-branch networks.
arXiv preprint
arXiv:1709.09582, 2017.
G Anandalingam and TL Friesz.
Hierarchical optimization: An introduction.
Annals of Operations
Research, 34(1):1–11, 1992.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.
Designing neural network architec-
tures using reinforcement learning.
ICLR, 2017.
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik.
Accelerating neural architecture
search using performance prediction.
ICLR Workshop, 2018.
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le.
Understanding
and simplifying one-shot architecture search.
In International Conference on Machine Learning,
pp.
549–558, 2018.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.
Smash: one-shot model
architecture search through hypernetworks.
ICLR, 2018.
Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang.
Efﬁcient architecture search by
network transformation.
AAAI, 2018.
Benoît Colson, Patrice Marcotte, and Gilles Savard.
An overview of bilevel optimization.
Annals of
operations research, 153(1):235–256, 2007.
Terrance DeVries and Graham W Taylor.
Improved regularization of convolutional neural networks
with cutout.
arXiv preprint arXiv:1708.04552, 2017.
9
Published as a conference paper at ICLR 2019
Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter.
Simple and efﬁcient architecture search for
convolutional neural networks.
arXiv preprint arXiv:1711.04528, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
Model-agnostic meta-learning for fast adaptation of
deep networks.
In ICML, pp.
1126–1135, 2017.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil.
Bilevel programming for
hyperparameter optimization and meta-learning.
ICML, 2018.
Yarin Gal and Zoubin Ghahramani.
A theoretically grounded application of dropout in recurrent
neural networks.
In NIPS, pp.
1019–1027, 2016.
Edouard Grave, Armand Joulin, and Nicolas Usunier.
Improving neural language models with a
continuous cache.
arXiv preprint arXiv:1612.04426, 2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam.
Mobilenets: Efﬁcient convolutional neural networks for
mobile vision applications.
arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten.
Densely connected
convolutional networks.
In CVPR, 2017.
Hakan Inan, Khashayar Khosravi, and Richard Socher.
Tying word vectors and word classiﬁers: A
loss framework for language modeling.
ICLR, 2017.
Sergey Ioffe and Christian Szegedy.
Batch normalization: Accelerating deep network training by
reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.
Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing.
Neural
architecture search with bayesian optimisation and optimal transport.
NIPS, 2018.
Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014.
Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals.
Dynamic evaluation of neural
sequence models.
arXiv preprint arXiv:1709.07432, 2017.
Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan
Huang, and Kevin Murphy.
Progressive neural architecture search.
ECCV, 2018a.
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu.
Hierar-
chical representations for efﬁcient architecture search.
ICLR, 2018b.
Ilya Loshchilov and Frank Hutter.
Sgdr: Stochastic gradient descent with warm restarts.
arXiv
preprint arXiv:1608.03983, 2016.
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko.
Scalable gradient-based tuning
of continuous regularization hyperparameters.
In ICML, pp.
2952–2960, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams.
Gradient-based hyperparameter optimization
through reversible learning.
In ICML, pp.
2113–2122, 2015.
Gábor Melis, Chris Dyer, and Phil Blunsom.
On the state of the art of evaluation in neural language
models.
ICLR, 2018.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher.
Regularizing and optimizing lstm
language models.
ICLR, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein.
Unrolled generative adversarial
networks.
ICLR, 2017.
Renato Negrinho and Geoff Gordon.
Deeparchitect: Automatically designing and training deep
architectures.
arXiv preprint arXiv:1704.08792, 2017.
10
Published as a conference paper at ICLR 2019
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in
pytorch.
In NIPS-W, 2017.
Fabian Pedregosa.
Hyperparameter optimization with approximate gradient.
In ICML, 2016.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean.
Authors’ implementation
of “Efﬁcient Neural Architecture Search via Parameter Sharing”.
https://github.com/
melodyguan/enas/tree/2734eb2657847f090e1bc5c51c2b9cbf0be51887,
2018a.
Accessed: 2018-04-05.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean.
Efﬁcient neural architecture
search via parameter sharing.
ICML, 2018b.
Boris T Polyak and Anatoli B Juditsky.
Acceleration of stochastic approximation by averaging.
SIAM
Journal on Control and Optimization, 30(4):838–855, 1992.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
Regularized evolution for image
classiﬁer architecture search.
arXiv preprint arXiv:1802.01548, 2018.
Shreyas Saxena and Jakob Verbeek.
Convolutional neural fabrics.
In NIPS, pp.
4053–4061, 2016.
Richard Shin, Charles Packer, and Dawn Song.
Differentiable neural network architecture search.
In
ICLR Workshop, 2018.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, Andrew Rabinovich, Jen-Hao Rick Chang, et al.
Going deeper with
convolutions.
In CVPR, 2015.
Tom Veniat and Ludovic Denoyer.
Learning time/memory-efﬁcient deep architectures with budgeted
super networks.
arXiv preprint arXiv:1706.00046, 2017.
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen.
Breaking the softmax
bottleneck: a high-rank rnn language model.
ICLR, 2018.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient
convolutional neural network for mobile devices.
arXiv preprint arXiv:1707.01083, 2017.
Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu.
Practical block-wise neural network
architecture generation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp.
2423–2432, 2018.
Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber.
Recurrent
highway networks.
arXiv preprint arXiv:1607.03474, 2016.
Barret Zoph and Quoc V Le.
Neural architecture search with reinforcement learning.
ICLR, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le.
Learning transferable architectures
for scalable image recognition.
CVPR, 2018.
11
Published as a conference paper at ICLR 2019
A
EXPERIMENTAL DETAILS
A.1
ARCHITECTURE SEARCH
A.1.1
CIFAR-10
Since the architecture will be varying throughout the search process, we always use batch-speciﬁc
statistics for batch normalization rather than the global moving average.
Learnable afﬁne parameters
in all batch normalizations are disabled during the search process to avoid rescaling the outputs of the
candidate operations.
To carry out architecture search, we hold out half of the CIFAR-10 training data as the validation
set.
A small network of 8 cells is trained using DARTS for 50 epochs, with batch size 64 (for both
the training and validation sets) and the initial number of channels 16.
The numbers were chosen to
ensure the network can ﬁt into a single GPU.
We use momentum SGD to optimize the weights w,
with initial learning rate ηw = 0.025 (annealed down to zero following a cosine schedule without
restart (Loshchilov & Hutter, 2016)), momentum 0.9, and weight decay 3 × 10−4.
We use zero
initialization for architecture variables (the α’s in both the normal and reduction cells), which implies
equal amount of attention (after taking the softmax) over all possible ops.
At the early stage this
ensures weights in every candidate op to receive sufﬁcient learning signal (more exploration).
We
use Adam (Kingma & Ba, 2014) as the optimizer for α, with initial learning rate ηα = 3 × 10−4,
momentum β = (0.5, 0.999) and weight decay 10−3.
The search takes one day on a single GPU3.
A.1.2
PTB
For architecture search, both the embedding and the hidden sizes are set to 300.
The linear transfor-
mation parameters across all incoming operations connected to the same node are shared (their shapes
are all 300 × 300), as the algorithm always has the option to focus on one of the predecessors and
mask away the others.
Tying the weights leads to memory savings and faster computation, allowing
us to train the continuous architecture using a single GPU.
Learnable afﬁne parameters in batch
normalizations are disabled, as we did for convolutional cells.
The network is then trained for 50
epochs using SGD without momentum, with learning rate ηw = 20, batch size 256, BPTT length
35, and weight decay 5 × 10−7.
We apply variational dropout (Gal & Ghahramani, 2016) of 0.2
to word embeddings, 0.75 to the cell input, and 0.25 to all the hidden nodes.
A dropout of 0.75 is
also applied to the output layer.
Other training settings are identical to those in Merity et al.
(2018);
Yang et al.
(2018).
Similarly to the convolutional architectures, we use Adam for the optimization of
α (initialized as zeros), with initial learning rate ηα = 3 × 10−3, momentum β = (0.9, 0.999) and
weight decay 10−3.
The search takes 6 hours on a single GPU.
A.2
ARCHITECTURE EVALUATION
A.2.1
CIFAR-10
A large network of 20 cells is trained for 600 epochs with batch size 96.
The initial number of
channels is increased from 16 to 36 to ensure our model size is comparable with other baselines in
the literature (around 3M).
Other hyperparameters remain the same as the ones used for architecture
search.
Following existing works (Pham et al., 2018b; Zoph et al., 2018; Liu et al., 2018a; Real
et al., 2018), additional enhancements include cutout (DeVries & Taylor, 2017), path dropout of
probability 0.2 and auxiliary towers with weight 0.4.
The training takes 1.5 days on a single GPU
with our implementation in PyTorch (Paszke et al., 2017).
Since the CIFAR results are subject to
high variance even with exactly the same setup (Liu et al., 2018b), we report the mean and standard
deviation of 10 independent runs for our full model.
To avoid any discrepancy between different implementations or training settings (e.g.
the batch sizes),
we incorporated the NASNet-A cell (Zoph et al., 2018) and the AmoebaNet-A cell (Real et al., 2018)
into our training framework and reported their results under the same settings as our cells.
3All of our experiments were performed using NVIDIA GTX 1080Ti GPUs.
12
Published as a conference paper at ICLR 2019
A.2.2
PTB
A single-layer recurrent network with the discovered cell is trained until convergence with batch size
64 using averaged SGD (Polyak & Juditsky, 1992) (ASGD), with learning rate ηw = 20 and weight
decay 8 × 10−7.
To speedup, we start with SGD and trigger ASGD using the same protocol as in
Yang et al.
(2018); Merity et al.
(2018).
Both the embedding and the hidden sizes are set to 850 to
ensure our model size is comparable with other baselines.
The token-wise dropout on the embedding
layer is set to 0.1.
Other hyperparameters remain exactly the same as those for architecture search.
For fair comparison, we do not ﬁnetune our model at the end of the optimization, nor do we use any
additional enhancements such as dynamic evaluation (Krause et al., 2017) or continuous cache (Grave
et al., 2016).
The training takes 3 days on a single 1080Ti GPU with our PyTorch implementation.
To
account for implementation discrepancies, we also incorporated the ENAS cell (Pham et al., 2018b)
into our codebase and trained their network under the same setup as our discovered cells.
A.2.3
IMAGENET
We consider the mobile setting where the input image size is 224×224 and the number of multiply-add
operations in the model is restricted to be less than 600M.
A network of 14 cells is trained for 250 epochs with batch size 128, weight decay 3 × 10−5 and initial
SGD learning rate 0.1 (decayed by a factor of 0.97 after each epoch).
Other hyperparameters follow
Zoph et al.
(2018); Real et al.
(2018); Liu et al.
(2018a)4.
The training takes 12 days on a single GPU.
A.2.4
WIKITEXT-2
We use embedding and hidden sizes 700, weight decay 5×10−7, and hidden-node variational dropout
0.15.
Other hyperparameters remain the same as in our PTB experiments.
B
SEARCH WITH INCREASED DEPTH
To better understand the effect of depth for architecture search, we conducted architecture search on
CIFAR-10 by increasing the number of cells in the stack from 8 to 20.
The initial number of channels
is reduced from 16 to 6 due to memory budget of a single GPU.
All the other hyperparameters remain
the same.
The search cost doubles and the resulting cell achieves 2.88 ± 0.09% test error, which is
slightly worse than 2.76 ± 0.09% obtained using a shallower network.
This particular setup may have
suffered from the enlarged discrepancy of the number of channels between architecture search and
ﬁnal evaluation.
Moreover, searching with a deeper model might require different hyperparameters
due to the increased number of layers to back-prop through.
C
COMPLEXITY ANALYSIS
In this section, we analyze the complexity of our search space for convolutional cells.
Each of our discretized cell allows Q4
k=1
(k+1)k
2
× (72) ≈109 possible DAGs without considering
graph isomorphism (recall we have 7 non-zero ops, 2 input nodes, 4 intermediate nodes with 2
predecessors each).
Since we are jointly learning both normal and reduction cells, the total number
of architectures is approximately (109)2 = 1018.
This is greater than the 5.6 × 1014 of PNAS (Liu
et al., 2018a) which learns only a single type of cell.
Also note that we retained the top-2 predecessors per node only at the very end, and our continuous
search space before this ﬁnal discretization step is even larger.
Speciﬁcally, each relaxed cell (a fully
connected graph) contains 2 + 3 + 4 + 5 = 14 learnable edges, allowing (7 + 1)14 ≈4 × 1012
possible conﬁgurations (+1 to include the zero op indicating a lack of connection).
Again, since
we are learning both normal and reduction cells, the total number of architectures covered by the
continuous space before discretization is (4 × 1012)2 ≈1025.
4We did not conduct extensive hyperparameter tuning.
13
Our proposed model “DeepLabv3+” employs the encoder-decoder structure where
DeepLabv3 is used to encode the rich contextual information and a simple yet
eﬀective decoder module is adopted to recover the object boundaries.
One could
also apply the atrous convolution to extract the encoder features at an arbitrary
resolution, depending on the available computation resources.
We also explore
the Xception model and atrous separable convolution to make the proposed
model faster and stronger.
Finally, our experimental results show that the pro-
posed model sets a new state-of-the-art performance on PASCAL VOC 2012 and
Cityscapes datasets.
Acknowledgments We would like to acknowledge the valuable discussions
with Haozhi Qi and Jifeng Dai about Aligned Xception, the feedback from Chen
Sun, and the support from Google Mobile Vision team.
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
15
References
1.
Everingham, M., Eslami, S.M.A., Gool, L.V., Williams, C.K.I., Winn, J., Zisser-
man, A.: The pascal visual object classes challenge a retrospective.
IJCV (2014)
2.
Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R.,
Yuille, A.: The role of context for object detection and semantic segmentation in
the wild.
In: CVPR.
(2014)
3.
Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding.
In: CVPR.
(2016)
4.
Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing
through ade20k dataset.
In: CVPR.
(2017)
5.
Caesar, H., Uijlings, J., Ferrari, V.: COCO-Stuﬀ: Thing and stuﬀclasses in context.
In: CVPR.
(2018)
6.
LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to
document recognition.
In: Proc.
IEEE.
(1998)
7.
Krizhevsky, A., Sutskever, I., Hinton, G.E.:
Imagenet classiﬁcation with deep
convolutional neural networks.
In: NIPS.
(2012)
8.
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat:
Integrated recognition, localization and detection using convolutional networks.
In:
ICLR.
(2014)
9.
Simonyan, K., Zisserman, A.:
Very deep convolutional networks for large-scale
image recognition.
In: ICLR.
(2015)
10.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions.
In: CVPR.
(2015)
11.
Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation.
In: CVPR.
(2015)
12.
He, X., Zemel, R.S., Carreira-Perpindn, M.: Multiscale conditional random ﬁelds
for image labeling.
In: CVPR.
(2004)
13.
Shotton, J., Winn, J., Rother, C., Criminisi, A.: Textonboost for image understand-
ing: Multi-class object recognition and segmentation by jointly modeling texture,
layout, and context.
IJCV (2009)
14.
Kohli, P., Torr, P.H., et al.: Robust higher order potentials for enforcing label
consistency.
IJCV 82(3) (2009) 302–324
15.
Ladicky, L., Russell, C., Kohli, P., Torr, P.H.:
Associative hierarchical crfs for
object class image segmentation.
In: ICCV.
(2009)
16.
Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and seman-
tically consistent regions.
In: ICCV.
(2009)
17.
Yao, J., Fidler, S., Urtasun, R.:
Describing the scene as a whole: Joint object
detection, scene classiﬁcation and semantic segmentation.
In: CVPR.
(2012)
18.
Grauman, K., Darrell, T.: The pyramid match kernel: Discriminative classiﬁcation
with sets of image features.
In: ICCV.
(2005)
19.
Lazebnik, S., Schmid, C., Ponce, J.:
Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In: CVPR.
(2006)
20.
He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional
networks for visual recognition.
In: ECCV.
(2014)
21.
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-
ical image segmentation.
In: MICCAI.
(2015)
22.
Badrinarayanan, V., Kendall, A., Cipolla, R.:
Segnet: A deep convolutional
encoder-decoder architecture for image segmentation.
PAMI (2017)
16
L.-C Chen, Y.
Zhu, G.
Papandreou, F.
Schroﬀ, and H.
Adam
23.
Chen, L.C., Papandreou, G., Schroﬀ, F., Adam, H.: Rethinking atrous convolution
for semantic image segmentation.
arXiv:1706.05587 (2017)
24.
Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network.
In:
CVPR.
(2017)
25.
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR.
(2016)
26.
Chollet, F.: Xception: Deep learning with depthwise separable convolutions.
In:
CVPR.
(2017)
27.
Sifre, L.: Rigid-motion scattering for image classiﬁcation.
PhD thesis (2014)
28.
Vanhoucke, V.: Learning visual representations at scale.
ICLR invited talk (2014)
29.
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.:
Mobilenets: Eﬃcient convolutional neural networks for
mobile vision applications.
arXiv:1704.04861 (2017)
30.
Zhang, X., Zhou, X., Lin, M., Sun, J.: Shuﬄenet: An extremely eﬃcient convolu-
tional neural network for mobile devices.
In: CVPR.
(2018)
31.
Qi, H., Zhang, Z., Xiao, B., Hu, H., Cheng, B., Wei, Y., Dai, J.:
Deformable
convolutional networks – coco detection and segmentation challenge 2017 entry.
ICCV COCO Challenge Workshop (2017)
32.
Mostajabi, M., Yadollahpour, P., Shakhnarovich, G.: Feedforward semantic seg-
mentation with zoom-out features.
In: CVPR.
(2015)
33.
Dai, J., He, K., Sun, J.: Convolutional feature masking for joint object and stuﬀ
segmentation.
In: CVPR.
(2015)
34.
Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features
for scene labeling.
PAMI (2013)
35.
Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with
a common multi-scale convolutional architecture.
In: ICCV.
(2015)
36.
Pinheiro, P., Collobert, R.:
Recurrent convolutional neural networks for scene
labeling.
In: ICML.
(2014)
37.
Lin, G., Shen, C., van den Hengel, A., Reid, I.: Eﬃcient piecewise training of deep
structured models for semantic segmentation.
In: CVPR.
(2016)
38.
Chen, L.C., Yang, Y., Wang, J., Xu, W., Yuille, A.L.: Attention to scale: Scale-
aware semantic image segmentation.
In: CVPR.
(2016)
39.
Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.:
Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs.
TPAMI (2017)
40.
Kr¨ahenb¨uhl, P., Koltun, V.: Eﬃcient inference in fully connected crfs with gaussian
edge potentials.
In: NIPS.
(2011)
41.
Adams, A., Baek, J., Davis, M.A.: Fast high-dimensional ﬁltering using the per-
mutohedral lattice.
In: Eurographics.
(2010)
42.
Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.:
Semantic
image segmentation with deep convolutional nets and fully connected crfs.
In:
ICLR.
(2015)
43.
Bell, S., Upchurch, P., Snavely, N., Bala, K.: Material recognition in the wild with
the materials in context database.
In: CVPR.
(2015)
44.
Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.: Conditional random ﬁelds as recurrent neural networks.
In: ICCV.
(2015)
45.
Liu, Z., Li, X., Luo, P., Loy, C.C., Tang, X.: Semantic image segmentation via
deep parsing network.
In: ICCV.
(2015)
46.
Papandreou, G., Chen, L.C., Murphy, K., Yuille, A.L.:
Weakly- and semi-
supervised learning of a dcnn for semantic image segmentation.
In: ICCV.
(2015)
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
17
47.
Schwing, A.G., Urtasun, R.:
Fully connected deep structured networks.
arXiv:1503.02351 (2015)
48.
Jampani, V., Kiefel, M., Gehler, P.V.: Learning sparse high dimensional ﬁlters:
Image ﬁltering, dense crfs and bilateral neural networks.
In: CVPR.
(2016)
49.
Vemulapalli, R., Tuzel, O., Liu, M.Y., Chellappa, R.: Gaussian conditional random
ﬁeld network for semantic segmentation.
In: CVPR.
(2016)
50.
Chandra, S., Kokkinos, I.: Fast, exact and multi-scale inference for semantic image
segmentation with deep Gaussian CRFs.
In: ECCV.
(2016)
51.
Chandra, S., Usunier, N., Kokkinos, I.: Dense and low-rank gaussian crfs using
deep embeddings.
In: ICCV.
(2017)
52.
Liu, W., Rabinovich, A., Berg, A.C.:
Parsenet: Looking wider to see better.
arXiv:1506.04579 (2015)
53.
Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti-
mation.
In: ECCV.
(2016)
54.
Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection.
In: CVPR.
(2017)
55.
Shrivastava, A., Sukthankar, R., Malik, J., Gupta, A.: Beyond skip connections:
Top-down modulation for object detection.
arXiv:1612.06851 (2016)
56.
Fu, C.Y., Liu, W., Ranga, A., Tyagi, A., Berg, A.C.: Dssd: Deconvolutional single
shot detector.
arXiv:1701.06659 (2017)
57.
Noh, H., Hong, S., Han, B.: Learning deconvolution network for semantic segmen-
tation.
In: ICCV.
(2015)
58.
Lin, G., Milan, A., Shen, C., Reid, I.: Reﬁnenet: Multi-path reﬁnement networks
with identity mappings for high-resolution semantic segmentation.
In: CVPR.
(2017)
59.
Pohlen, T., Hermans, A., Mathias, M., Leibe, B.: Full-resolution residual networks
for semantic segmentation in street scenes.
In: CVPR.
(2017)
60.
Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.:
Large kernel matters–improve
semantic segmentation by global convolutional network.
In: CVPR.
(2017)
61.
Islam, M.A., Rochan, M., Bruce, N.D., Wang, Y.:
Gated feedback reﬁnement
network for dense image labeling.
In: CVPR.
(2017)
62.
Wojna, Z., Ferrari, V., Guadarrama, S., Silberman, N., Chen, L.C., Fathi, A.,
Uijlings, J.: The devil is in the decoder.
In: BMVC.
(2017)
63.
Fu, J., Liu, J., Wang, Y., Lu, H.: Stacked deconvolutional network for semantic
segmentation.
arXiv:1708.04943 (2017)
64.
Zhang, Z., Zhang, X., Peng, C., Cheng, D., Sun, J.: Exfuse: Enhancing feature
fusion for semantic segmentation.
In: ECCV.
(2018)
65.
Xie, S., Girshick, R., Dollr, P., Tu, Z., He, K.: Aggregated residual transformations
for deep neural networks.
In: CVPR.
(2017)
66.
Jin, J., Dundar, A., Culurciello, E.: Flattened convolutional neural networks for
feedforward acceleration.
arXiv:1412.5474 (2014)
67.
Wang, M., Liu, B., Foroosh, H.: Design of eﬃcient convolutional layers using sin-
gle intra-channel convolution, topological subdivisioning and spatial ”bottleneck”
structure.
arXiv:1608.04337 (2016)
68.
Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures
for scalable image recognition.
In: CVPR.
(2018)
69.
Holschneider, M., Kronland-Martinet, R., Morlet, J., Tchamitchian, P.: A real-time
algorithm for signal analysis with the help of the wavelet transform.
In: Wavelets:
Time-Frequency Methods and Phase Space.
(1989) 289–297
70.
Giusti, A., Ciresan, D., Masci, J., Gambardella, L., Schmidhuber, J.: Fast image
scanning with deep max-pooling convolutional neural networks.
In: ICIP.
(2013)
18
L.-C Chen, Y.
Zhu, G.
Papandreou, F.
Schroﬀ, and H.
Adam
71.
Papandreou, G., Kokkinos, I., Savalle, P.A.: Modeling local and global deforma-
tions in deep learning: Epitomic convolution, multiple instance learning, and sliding
window detection.
In: CVPR.
(2015)
72.
Abadi, M., Agarwal, A., et al.: Tensorﬂow: Large-scale machine learning on het-
erogeneous distributed systems.
arXiv:1603.04467 (2016)
73.
Hariharan, B., Arbel´aez, P., Girshick, R., Malik, J.:
Hypercolumns for object
segmentation and ﬁne-grained localization.
In: CVPR.
(2015)
74.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge.
IJCV (2015)
75.
Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by
reducing internal covariate shift.
In: ICML.
(2015)
76.
Hariharan, B., Arbel´aez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours
from inverse detectors.
In: ICCV.
(2011)
77.
Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., Cottrell, G.: Under-
standing convolution for semantic segmentation.
arXiv:1702.08502 (2017)
78.
Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convo-
lutional networks.
In: ICCV.
(2017)
79.
Lin, T.Y., et al.: Microsoft COCO: Common objects in context.
In: ECCV.
(2014)
80.
Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
In: NIPS.
(2014)
81.
Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable eﬀectiveness
of data in deep learning era.
In: ICCV.
(2017)
82.
Li, X., Liu, Z., Luo, P., Loy, C.C., Tang, X.: Not all pixels are equal: Diﬃculty-
aware semantic segmentation via deep layer cascade.
In: CVPR.
(2017)
83.
Wu, Z., Shen, C., van den Hengel, A.: Wider or deeper: Revisiting the resnet model
for visual recognition.
arXiv:1611.10080 (2016)
84.
Wang, G., Luo, P., Lin, L., Wang, X.: Learning object interactions and descriptions
for semantic image segmentation.
In: CVPR.
(2017)
85.
Luo, P., Wang, G., Lin, L., Wang, X.:
Deep dual learning for semantic image
segmentation.
In: ICCV.
(2017)
86.
Bul`o, S.R., Porzi, L., Kontschieder, P.: In-place activated batchnorm for memory-
optimized training of dnns.
In: CVPR.
(2018)
We have shown that generating Wikipedia can be approached as a multi-document summarization
problem with a large, parallel dataset, and demonstrated a two-stage extractive-abstractive frame-
work for carrying it out.
The coarse extraction method used in the ﬁrst stage appears to have a sig-
niﬁcant effect on ﬁnal performance, suggesting further research on improving it would be fruitful.
We introduce a new, decoder-only sequence transduction model for the abstractive stage, capable of
handling very long input-output examples.
This model signiﬁcantly outperforms traditional encoder-
decoder architectures on long sequences, allowing us to condition on many reference documents and
to generate coherent and informative Wikipedia articles.
7
PUBLIC RELEASE OF DATASET AND CODE
To encourage further research on large-scale summarization, we will release the URLs used in our
experiments (the Wikipedia URL as well as the URLs of its references) that are available as part of
the CommonCrawl dataset4, which is freely available for download.
We use the open-source tensor2tensor5 library for training abstractive models and will be
releasing our abstractive modeling code extensions.
Further details are available at https://
goo.gl/wSuuS9.
4http://commoncrawl.org
5https://github.com/tensorflow/tensor2tensor
11
Published as a conference paper at ICLR 2018
ACKNOWLEDGMENTS
We thank Samy Bengio, Jeff Dean, Claire Cui, Fred Bertsch, Chad Whipkey, Anurag Rana, Ashish
Vaswani, Llion Jones, and the tensorflow/tensor2tensor contributors for help with the
project.
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly
learning to align and translate.
arXiv preprint arXiv:1409.0473, 2014.
Sumit Chopra, Michael Auli, and Alexander M Rush.
Abstractive sentence summarization with
attentive recurrent neural networks.
In Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
93–98, 2016.
Hoa Trang Dang.
Overview of duc 2005.
In Proceedings of the document understanding conference,
volume 2005, pp.
1–12, 2005.
David Graff and Christopher Cieri.
English gigaword 2003.
Linguistic Data Consortium, Philade-
plhia, 2003.
Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han,
Matthew Kelcey, and David Berthelot.
Wikireading: A novel large-scale language understanding
task over wikipedia.
arXiv preprint arXiv:1608.03542, 2016.
R´emi Lebret, David Grangier, and Michael Auli.
Neural text generation from structured data with
application to the biography domain.
In Proceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp.
1203–1213, 2016.
URL http://aclweb.org/anthology/D/D16/D16-1128.pdf.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S¨oren Auer, et al.
Dbpedia–a large-
scale, multilingual knowledge base extracted from wikipedia.
Semantic Web, 6(2):167–195, 2015.
Chin-Yew Lin.
Rouge: A package for automatic evaluation of summaries.
In Text summarization
branches out: Proceedings of the ACL-04 workshop, volume 8.
Barcelona, Spain, 2004.
Rada Mihalcea and Paul Tarau.
Textrank: Bringing order into text.
In Proceedings of the 2004
conference on empirical methods in natural language processing, 2004.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C¸ a glar Gulc¸ehre, and Bing Xiang.
Abstractive
text summarization using sequence-to-sequence rnns and beyond.
CoNLL 2016, pp.
280, 2016.
Ani Nenkova and Lucy Vanderwende.
The impact of frequency on summarization.
Microsoft Re-
search, Redmond, Washington, Tech.
Rep.
MSR-TR-2005, 101, 2005.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
The pagerank citation ranking:
Bringing order to the web.
Technical report, Stanford InfoLab, 1999.
Romain Paulus, Caiming Xiong, and Richard Socher.
A deep reinforced model for abstractive
summarization.
arXiv preprint arXiv:1705.04304, 2017.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
Squad: 100,000+ questions
for machine comprehension of text.
arXiv preprint arXiv:1606.05250, 2016.
Juan Ramos et al.
Using tf-idf to determine word relevance in document queries.
In Proceedings of
the ﬁrst instructional conference on machine learning, volume 242, pp.
133–142, 2003.
Alexander M.
Rush, Sumit Chopra, and Jason Weston.
A neural attention model for abstractive
sentence summarization.
In Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp.
379–389,
2015.
URL http://aclweb.org/anthology/D/D15/D15-1044.pdf.
12
Published as a conference paper at ICLR 2018
Christina Sauper and Regina Barzilay.
Automatically generating wikipedia articles: A structure-
aware approach.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1, ACL ’09, pp.
208–216, Stroudsburg, PA, USA, 2009.
Association for Com-
putational Linguistics.
ISBN 978-1-932432-45-9.
URL http://dl.acm.org/citation.
cfm?id=1687878.1687909.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean.
Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
arXiv preprint arXiv:1706.03762,
2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation.
arXiv preprint
arXiv:1609.08144, 2016.
13
Published as a conference paper at ICLR 2018
A
APPENDIX
A.1
EXAMPLES OF FULL WIKIPEDIA GENERATED SAMPLES
Figure 6: An example decoded from a T-DMCA model trained to produce an entire Wikipedia
article, conditioned on 8192 reference document tokens.
14
Published as a conference paper at ICLR 2018
Figure 7: Three different samples a T-DMCA model trained to produce an entire Wikipedia article,
conditioned only on the title.
Samples 1 and 3 are truncated due to space constraints.
15
Published as a conference paper at ICLR 2018
A.2
IMPLEMENTATION DETAILS
A.2.1
WIKIPEDIA CLONE DETECTION
For detecting whether a reference document, d, is a Wikipedia article clone we compute the maxi-
mum recall of unigrams for each section of the Wikipedia article, a:
r(d, a) =
max
s∈sections(a)
|unigrams(d) ∩unigrams(s)|
|unigrams(s)|
and detect a clone if r > 0.5.
A.3
HUMAN EVALUATION EXPERIMENT
Figure 8: Screenshot of DUC-style linguistic quality human evaluation tool.
To assess linguistic quality, we randomly selected samples generated by models from the test set
and ask raters to choose a score from 1 to 5 (higher is better) for ﬁve dimensions: Grammaticality,
Non-redundancy, Referential clarity, Focus, and Structure and Coherence.
These were used in the
past at DUC for evaluating summaries (Dang, 2005).
For each model we selected 25 examples and
averaged the scores for each question across 3 raters (out of pool of 7).
To compare two models by human evaluation, we randomly select examples from the test set and
show model outputs side-by-side in the interface shown in Figure 9.
Which side a model appears
on is randomized per example and rater.
For the experiments in Table 6 we had 3 raters score 25
examples each and computed the ratio of ratings preferring one model over the other.
A.4
EXAMPLE ABSTRACTIVE MODEL INPUT
16
Published as a conference paper at ICLR 2018
Figure 9: Screenshot of side-by-side human evaluation tool.
Raters are asked whether they prefer
model output on the left or right, given a ground truth Wikipedia text.
17
Published as a conference paper at ICLR 2018
Figure 10: Example extractive-output/abstractive-input for models in ”dewey & lebeouf” example.
The extractive method used is tf-idf.
18
We proposed three parameter sharing strategies:
SEQUENCE, CYCLE, and CYCLE (REV), for the
internal layers in Transformers.
In contrast to the
previous strategy, which prepares parameters for
only one layer and shares them across layers such
as Universal Transformers (Dehghani et al., 2019),
the proposed strategies prepare parameters for M
layers to construct N layers.
The proposed strate-
gies stack layers whose weight matrices are smaller
than ones of Universal Transformers to raise expres-
siveness power while saving computational time.
Experimental results in the standard machine
translation setting show that the proposed strate-
gies achieved slightly better BLEU scores to those
of Universal with a small computational time when
we prepared almost the same parameters for each
method (M = 6 and N = 12).
In addition, the
proposed strategies outperformed Universal under
the same computational budgets (M = 6 and
N = 18).
Thus, the proposed strategies are ef-
ficient in terms of the parameter size and compu-
tational time.
Through additional experiments, we
indicated that the proposed strategies are also more
efficient than Universal in the high resource set-
ting, other language pairs, and another modality
(speech-to-text).
Limitations
As described in Section 1, the purpose of this study
is to relax the existing parameter sharing strategy
which shares the parameters of one layer with all
layers (Dehghani et al., 2019; Dabre and Fujita,
2019; Lan et al., 2020).
Experimental results in-
dicate that the proposed simple parameter sharing
strategies can be a better alternative to the existing
method.
As many studies on neural methods, this
study also depend on empirical observations.
In
other words, this study lacks theoretical justifica-
tions for proposed parameter sharing strategies.
We conducted experiments on various situations.
We mainly focused on sequence-to-sequence tasks
and trained each model from scratch.
Our con-
ducted experiments indicated the efficiency of the
proposed strategies but we did not conduct experi-
ments on the pre-training and then fine-tuning con-
figuration such as comparison with BERT (Devlin
et al., 2019) due to the limitation of our computa-
tional budgets.
Thus, it is difficult to claim that the
proposed strategies are also more efficient in such
configuration.
In addition, we have to investigate
the effectiveness in a more realistic situation.
For
example, we will investigate the performance of
the combination of our proposed method, which is
the parameter efficient way for internal layers, and
a parameter efficient embedding such as Takase
and Kobayashi (2020).
Through experiments in various configurations,
it is difficult to conclude which strategy is the
best.
Experimental results imply that the best strat-
egy depends on the task and Transformer architec-
ture (Post-LN or Pre-LN).
Such phenomena are
reported in previous studies (Press et al., 2020; Gu-
lati et al., 2020).
In fact, the architecture explored
by Press et al.
(2020) is better in the language mod-
eling task but ineffective in the machine transla-
tion task.
Since it is intractable to investigate a
tremendous amount of possible parameter assign-
ment way due to the limitation of computational
budgets, there might be a superior way to three sim-
ple strategies proposed in this paper.
However, we
emphasize that all our proposed strategies are more
efficient than the Universal configuration.
Because
the purpose of our experiments is not to detect the
best parameter sharing strategy but to indicate that
our proposed parameter sharing strategies are more
efficient than the Universal configuration, we con-
sider that our conducted experiments are sufficient
to verify our claims.
Ethics Statement
As discussed in Strubell et al.
(2019), recent neural
models require substantial energy consumption.
To
address this issue, we explore a parameter efficient
way for Transformers in this study.
We believe that
our proposed strategies are effective to reduce the
energy consumption.
On the other hand, we spent a large amount of
computational costs to investigate the usefulness of
our proposed strategies in various situations.
Ap-
pendix B indicates our used GPUs and the number
of updates that correspond to the computational
costs.
Acknowledgements
We thank the anonymous reviewers for their insight-
ful suggestions.
A part of this work was supported
by JSPS KAKENHI Grant Number JP21K17800
and JST ACT-X Grant Number JPMJAX200I.
References
Alexei Baevski and Michael Auli.
2019.
Adaptive input
representations for neural language modeling.
In
Proceedings of ICLR.
Thorsten Brants, Ashok C.
Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean.
2007.
Large language models
in machine translation.
In Proceedings of EMNLP-
CoNLL, pages 858–867.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.
2020.
Language models are few-shot learners.
In NeurIPS,
pages 1877–1901.
Raj Dabre and Atsushi Fujita.
2019.
Recurrent stack-
ing of layers for compact neural machine translation
models.
Proceedings of AAAI, 33:6292–6299.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser.
2019.
Universal
transformers.
In Proceedings of ICLR.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of NAACL-HLT, pages
4171–4186.
Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier.
2018.
Understanding back-translation at
scale.
In Proceedings of EMNLP, pages 489–500.
Alex Graves.
2017.
Adaptive computation time for
recurrent neural networks.
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki
Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, and Ruom-
ing Pang.
2020.
Conformer: Convolution-augmented
transformer for speech recognition.
In Proceed-
ings of the 21st Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 5036–5040.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun.
2016.
Deep residual learning for image recogni-
tion.
In Proceedings of CVPR, pages 770–778.
Shun Kiyono, Takumi Ito, Ryuto Konno, Makoto Mor-
ishita, and Jun Suzuki.
2020.
Tohoku-AIP-NTT at
WMT 2020 news translation task.
In Proceedings of
WMT, pages 145–155.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020.
ALBERT: A lite bert for self-supervised learn-
ing of language representations.
In Proceedings of
ICLR.
Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du,
Tong Xiao, Huizhen Wang, and Jingbo Zhu.
2020.
Shallow-to-deep training for neural machine transla-
tion.
In Proceedings of EMNLP, pages 995–1005.
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen,
and Jiawei Han.
2020.
Understanding the difficulty
of training transformers.
In Proceedings of EMNLP,
pages 5747–5763.
Gábor Melis, Chris Dyer, and Phil Blunsom.
2018.
On
the state of the art of evaluation in neural language
models.
Proceedings of ICLR.
Stephen Merity, Nitish Shirish Keskar, and Richard
Socher.
2018.
Regularizing and Optimizing LSTM
Language Models.
In Proceedings of ICLR.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher.
2017.
Pointer sentinel mixture mod-
els.
In Proceedings of ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean.
2013.
Distributed representa-
tions of words and phrases and their compositionality.
In NIPS, volume 26.
Toan Q.
Nguyen and Julian Salazar.
2019.
Transformers
without tears: Improving the normalization of self-
attention.
In Proceedings of IWSLT.
Myle Ott, Sergey Edunov, David Grangier, and Michael
Auli.
2018.
Scaling neural machine translation.
In
Proceedings of WMT, pages 1–9.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-
jeev Khudanpur.
2015.
Librispeech: An asr corpus
based on public domain audio books.
In ICASSP,
pages 5206–5210.
Matt Post.
2018.
A call for clarity in reporting BLEU
scores.
In Proceedings of WMT, pages 186–191.
Ofir Press, Noah A.
Smith, and Omer Levy.
2020.
Im-
proving transformer models by reordering their sub-
layers.
In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 2996–3005.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a.
Improving neural machine translation models
with monolingual data.
In Proceedings of ACL, pages
86–96.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b.
Neural machine translation of rare words
with subword units.
In Proceedings of ACL, pages
1715–1725.
Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum.
2019.
Energy and policy considerations for
deep learning in NLP.
In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 3645–3650.
Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data.
In Proceedings of ACL,
pages 665–673.
Sho Takase and Shun Kiyono.
2021.
Rethinking per-
turbations in encoder-decoders for fast training.
In
Proceedings of NAACL-HLT, pages 5767–5780.
Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun
Suzuki.
2022.
B2t connection: Serving stability and
performance in deep transformers.
arXiv preprint
arXiv:2206.00330.
Sho Takase and Sosuke Kobayashi.
2020.
All word em-
beddings from one embedding.
In Advances in Neu-
ral Information Processing Systems 33 (NeurIPS),
pages 3775–3785.
Sho Takase, Jun Suzuki, and Masaaki Nagata.
2018.
Direct output connection for a high-rank language
model.
In Proceedings of EMNLP, pages 4599–4609.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin.
2017.
Attention is all
you need.
In NIPS, pages 5998–6008.
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu,
Dmytro Okhonko, and Juan Pino.
2020.
Fairseq
S2T: Fast speech-to-text modeling with fairseq.
In
Proceedings of AACL-IJCNLP, pages 33–39.
Qiang Wang,
Bei Li,
Tong Xiao,
Jingbo Zhu,
Changliang Li, Derek F.
Wong, and Lidia S.
Chao.
2019.
Learning deep transformer models for ma-
chine translation.
In Proceedings of ACL, pages
1810–1822.
Yingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and
Tao Qin.
2019.
Tied transformers: Neural machine
translation with shared encoder and decoder.
Pro-
ceedings of AAAI, 33(01):5466–5473.
Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and
Tongran Liu.
2019.
Sharing attention weights for
fast transformer.
In Proceedings of IJCAI, pages
5292–5298.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng,
Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan
Lan, Liwei Wang, and Tie-Yan Liu.
2020.
On layer
normalization in the transformer architecture.
In Pro-
ceedings of ICML.
A
Experiments on Language Modeling
A.1
Dataset
We focused Transformer-based encoder-decoders
in the main experiments of this paper.
However, re-
cent studies often employed the decoder side only
as a pre-trained model.
Thus, we conduct exper-
iments on the language modeling task to investi-
gate the efficiency of our proposed strategies when
we use the decoder side only.
We used Wikitext-
103 (Merity et al., 2017) which contains a large
amount of training data.
We measured perplexity
of validation and test sets.
A.2
The conventional approach to neural machine translation, called an encoder–decoder approach, en-
codes a whole input sentence into a ﬁxed-length vector from which a translation will be decoded.
We conjectured that the use of a ﬁxed-length context vector is problematic for translating long sen-
tences, based on a recent empirical study reported by Cho et al.
(2014b) and Pouget-Abadie et al.
(2014).
In this paper, we proposed a novel architecture that addresses this issue.
We extended the basic
encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations com-
puted by an encoder, when generating each target word.
This frees the model from having to encode
a whole source sentence into a ﬁxed-length vector, and also lets the model focus only on information
relevant to the generation of the next target word.
This has a major positive impact on the ability
of the neural machine translation system to yield good results on longer sentences.
Unlike with
the traditional machine translation systems, all of the pieces of the translation system, including
the alignment mechanism, are jointly trained towards a better log-probability of producing correct
translations.
We tested the proposed model, called RNNsearch, on the task of English-to-French translation.
The
experiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder
model (RNNencdec) signiﬁcantly, regardless of the sentence length and that it is much more ro-
bust to the length of a source sentence.
From the qualitative analysis where we investigated the
(soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor-
rectly align each target word with the relevant words, or their annotations, in the source sentence as
it generated a correct translation.
Perhaps more importantly, the proposed approach achieved a translation performance comparable to
the existing phrase-based statistical machine translation.
It is a striking result, considering that the
proposed architecture, or the whole family of neural machine translation, has only been proposed
as recently as this year.
We believe the architecture proposed here is a promising step toward better
machine translation and a better understanding of natural languages in general.
One of challenges left for the future is to better handle unknown, or rare words.
This will be required
for the model to be more widely used and to match the performance of current state-of-the-art
machine translation systems in all contexts.
9
Published as a conference paper at ICLR 2015
ACKNOWLEDGMENTS
The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al.,
2012).
We acknowledge the support of the following agencies for research funding and computing
support: NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs and CIFAR.
Bah-
danau thanks the support from Planet Intelligent Systems GmbH.
We also thank Felix Hill, Bart van
Merri´enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.
REFERENCES
Axelrod, A., He, X., and Gao, J.
(2011).
Domain adaptation via pseudo in-domain data selection.
In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 355–362.
Association for Computational Linguistics.
Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I.
J., Bergeron, A., Bouchard, N.,
and Bengio, Y.
(2012).
Theano: new features and speed improvements.
Deep Learning and
Unsupervised Feature Learning NIPS 2012 Workshop.
Bengio, Y., Simard, P., and Frasconi, P.
(1994).
Learning long-term dependencies with gradient
descent is difﬁcult.
IEEE Transactions on Neural Networks, 5(2), 157–166.
Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C.
(2003).
A neural probabilistic language model.
J.
Mach.
Learn.
Res., 3, 1137–1155.
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-
Farley, D., and Bengio, Y.
(2010).
Theano: a CPU and GPU math expression compiler.
In
Proceedings of the Python for Scientiﬁc Computing Conference (SciPy).
Oral Presentation.
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P.
(2013).
Audio chord recognition with
recurrent neural networks.
In ISMIR.
Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y.
(2014a).
Learning phrase representations using RNN encoder-decoder for statistical machine translation.
In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014).
to
appear.
Cho, K., van Merri¨enboer, B., Bahdanau, D., and Bengio, Y.
(2014b).
On the properties of neural
machine translation: Encoder–Decoder approaches.
In Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation.
to appear.
Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J.
(2014).
Fast and robust
neural network joint models for statistical machine translation.
In Association for Computational
Linguistics.
Forcada, M.
L.
and ˜Neco, R.
P.
(1997).
Recursive hetero-associative memories for translation.
In
J.
Mira, R.
Moreno-D´ıaz, and J.
Cabestany, editors, Biological and Artiﬁcial Computation: From
Neuroscience to Technology, volume 1240 of Lecture Notes in Computer Science, pages 453–462.
Springer Berlin Heidelberg.
Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y.
(2013).
Maxout net-
works.
In Proceedings of The 30th International Conference on Machine Learning, pages 1319–
1327.
Graves, A.
(2012).
Sequence transduction with recurrent neural networks.
In Proceedings of the
29th International Conference on Machine Learning (ICML 2012).
Graves, A.
(2013).
Generating sequences with recurrent neural networks.
arXiv:1308.0850
[cs.NE].
Graves, A., Jaitly, N., and Mohamed, A.-R.
(2013).
Hybrid speech recognition with deep bidirec-
tional LSTM.
In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work-
shop on, pages 273–278.
10
Published as a conference paper at ICLR 2015
Hermann, K.
and Blunsom, P.
(2014).
Multilingual distributed representations without word align-
ment.
In Proceedings of the Second International Conference on Learning Representations (ICLR
2014).
Hochreiter, S.
(1991).
Untersuchungen zu dynamischen neuronalen Netzen.
Diploma thesis, Institut
f¨ur Informatik, Lehrstuhl Prof.
Brauer, Technische Universit¨at M¨unchen.
Hochreiter, S.
and Schmidhuber, J.
(1997).
Long short-term memory.
Neural Computation, 9(8),
1735–1780.
Kalchbrenner, N.
and Blunsom, P.
(2013).
Recurrent continuous translation models.
In Proceedings
of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1700–1709.
Association for Computational Linguistics.
Koehn, P.
(2010).
Statistical Machine Translation.
Cambridge University Press, New York, NY,
USA.
Koehn, P., Och, F.
J., and Marcu, D.
(2003).
Statistical phrase-based translation.
In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg,
PA, USA.
Association for Computational Linguistics.
Pascanu, R., Mikolov, T., and Bengio, Y.
(2013a).
On the difﬁculty of training recurrent neural
networks.
In ICML’2013.
Pascanu, R., Mikolov, T., and Bengio, Y.
(2013b).
On the difﬁculty of training recurrent neural
networks.
In Proceedings of the 30th International Conference on Machine Learning (ICML
2013).
Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y.
(2014).
How to construct deep recurrent neural
networks.
In Proceedings of the Second International Conference on Learning Representations
(ICLR 2014).
Pouget-Abadie, J., Bahdanau, D., van Merri¨enboer, B., Cho, K., and Bengio, Y.
(2014).
Overcoming
the curse of sentence length for neural machine translation using automatic segmentation.
In
Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.
to appear.
Schuster, M.
and Paliwal, K.
K.
(1997).
Bidirectional recurrent neural networks.
Signal Processing,
IEEE Transactions on, 45(11), 2673–2681.
Schwenk, H.
(2012).
Continuous space translation models for phrase-based statistical machine
translation.
In M.
Kay and C.
Boitet, editors, Proceedings of the 24th International Conference on
Computational Linguistics (COLIN), pages 1071–1080.
Indian Institute of Technology Bombay.
Schwenk, H., Dchelotte, D., and Gauvain, J.-L.
(2006).
Continuous space language models for
statistical machine translation.
In Proceedings of the COLING/ACL on Main conference poster
sessions, pages 723–730.
Association for Computational Linguistics.
Sutskever, I., Vinyals, O., and Le, Q.
(2014).
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems (NIPS 2014).
Zeiler, M.
D.
(2012).
ADADELTA: An adaptive learning rate method.
arXiv:1212.5701
[cs.LG].
11
Published as a conference paper at ICLR 2015
A
MODEL ARCHITECTURE
A.1
ARCHITECTURAL CHOICES
The proposed scheme in Section 3 is a general framework where one can freely deﬁne, for instance,
the activation functions f of recurrent neural networks (RNN) and the alignment model a.
Here, we
describe the choices we made for the experiments in this paper.
A.1.1
RECURRENT NEURAL NETWORK
For the activation function f of an RNN, we use the gated hidden unit recently proposed by Cho
et al.
(2014a).
The gated hidden unit is an alternative to the conventional simple units such as an
element-wise tanh.
This gated unit is similar to a long short-term memory (LSTM) unit proposed
earlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn
long-term dependencies.
This is made possible by having computation paths in the unfolded RNN
for which the product of derivatives is close to 1.
These paths allow gradients to ﬂow backward
easily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al., 1994;
Pascanu et al., 2013a).
It is therefore possible to use LSTM units instead of the gated hidden unit
described here, as was done in a similar context by Sutskever et al.
(2014).
The new state si of the RNN employing n gated hidden units8 is computed by
si = f(si−1, yi−1, ci) = (1 −zi) ◦si−1 + zi ◦˜si,
where ◦is an element-wise multiplication, and zi is the output of the update gates (see below).
The
proposed updated state ˜si is computed by
˜si = tanh (We(yi−1) + U [ri ◦si−1] + Cci) ,
where e(yi−1) ∈Rm is an m-dimensional embedding of a word yi−1, and ri is the output of the
reset gates (see below).
When yi is represented as a 1-of-K vector, e(yi) is simply a column of an
embedding matrix E ∈Rm×K.
Whenever possible, we omit bias terms to make the equations less
cluttered.
The update gates zi allow each hidden unit to maintain its previous activation, and the reset gates ri
control how much and what information from the previous state should be reset.
We compute them
by
zi = σ (Wze(yi−1) + Uzsi−1 + Czci) ,
ri = σ (Wre(yi−1) + Ursi−1 + Crci) ,
where σ (·) is a logistic sigmoid function.
At each step of the decoder, we compute the output probability (Eq.
(4)) as a multi-layered func-
tion (Pascanu et al., 2014).
We use a single hidden layer of maxout units (Goodfellow et al., 2013)
and normalize the output probabilities (one for each word) with a softmax function (see Eq.
(6)).
A.1.2
ALIGNMENT MODEL
The alignment model should be designed considering that the model needs to be evaluated Tx × Ty
times for each sentence pair of lengths Tx and Ty.
In order to reduce computation, we use a single-
layer multilayer perceptron such that
a(si−1, hj) = v⊤
a tanh (Wasi−1 + Uahj) ,
where Wa ∈Rn×n, Ua ∈Rn×2n and va ∈Rn are the weight matrices.
Since Uahj does not
depend on i, we can pre-compute it in advance to minimize the computational cost.
8 Here, we show the formula of the decoder.
The same formula can be used in the encoder by simply
ignoring the context vector ci and the related terms.
12
Published as a conference paper at ICLR 2015
A.2
DETAILED DESCRIPTION OF THE MODEL
A.2.1
ENCODER
In this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the
experiments (see Sec.
4–5).
From here on, we omit all bias terms in order to increase readability.
The model takes a source sentence of 1-of-K coded word vectors as input
x = (x1, .
.
.
, xTx), xi ∈RKx
and outputs a translated sentence of 1-of-K coded word vectors
y = (y1, .
.
.
, yTy), yi ∈RKy,
where Kx and Ky are the vocabulary sizes of source and target languages, respectively.
Tx and Ty
respectively denote the lengths of source and target sentences.
First, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:
−→h i =
(
(1 −−→z i) ◦−→h i−1 + −→z i ◦−→h i
, if i > 0
0
, if i = 0
where
−→h i = tanh
−→
WExi + −→
U
h−→r i ◦−→h i−1
i
−→z i =σ
−→
W zExi + −→
U z
−→h i−1

−→r i =σ
−→
W rExi + −→
U r
−→h i−1

.
E ∈Rm×Kx is the word embedding matrix.
−→
W, −→
W z, −→
W r ∈Rn×m, −→
U , −→
U z, −→
U r ∈Rn×n are
weight matrices.
m and n are the word embedding dimensionality and the number of hidden units,
respectively.
σ(·) is as usual a logistic sigmoid function.
The backward states (←−h 1, · · · , ←−h Tx) are computed similarly.
We share the word embedding matrix
E between the forward and backward RNNs, unlike the weight matrices.
We concatenate the forward and backward states to to obtain the annotations (h1, h2, · · · , hTx),
where
hi =
" −→h i
←−h i
#
(7)
A.2.2
DECODER
The hidden state si of the decoder given the annotations from the encoder is computed by
si =(1 −zi) ◦si−1 + zi ◦˜si,
where
˜si = tanh (WEyi−1 + U [ri ◦si−1] + Cci)
zi =σ (WzEyi−1 + Uzsi−1 + Czci)
ri =σ (WrEyi−1 + Ursi−1 + Crci)
E is the word embedding matrix for the target language.
W, Wz, Wr ∈Rn×m, U, Uz, Ur ∈Rn×n,
and C, Cz, Cr ∈Rn×2n are weights.
Again, m and n are the word embedding dimensionality
and the number of hidden units, respectively.
The initial hidden state s0 is computed by s0 =
tanh

Ws
←−h 1

, where Ws ∈Rn×n.
The context vector ci are recomputed at each step by the alignment model:
ci =
Tx
X
j=1
αijhj,
13
Published as a conference paper at ICLR 2015
Model
Updates (×105)
Epochs
Hours
GPU
Train NLL
Dev.
NLL
RNNenc-30
8.46
6.4
109
TITAN BLACK
28.1
53.0
RNNenc-50
6.00
4.5
108
Quadro K-6000
44.0
43.6
RNNsearch-30
4.71
3.6
113
TITAN BLACK
26.7
47.2
RNNsearch-50
2.88
2.2
111
Quadro K-6000
40.7
38.1
RNNsearch-50⋆
6.67
5.0
252
Quadro K-6000
36.7
35.2
Table 2: Learning statistics and relevant information.
Each update corresponds to updating the
parameters once using a single minibatch.
One epoch is one pass through the training set.
NLL is
the average conditional log-probabilities of the sentences in either the training set or the development
set.
Note that the lengths of the sentences differ.
where
αij =
exp (eij)
PTx
k=1 exp (eik)
eij =v⊤
a tanh (Wasi−1 + Uahj) ,
and hj is the j-th annotation in the source sentence (see Eq.
(7)).
va ∈Rn′, Wa ∈Rn′×n and
Ua ∈Rn′×2n are weight matrices.
Note that the model becomes RNN Encoder–Decoder (Cho
et al., 2014a), if we ﬁx ci to −→h Tx.
With the decoder state si−1, the context ci and the last generated word yi−1, we deﬁne the probability
of a target word yi as
p(yi|si, yi−1, ci) ∝exp
 y⊤
i Woti

,
where
ti =

max
˜ti,2j−1, ˜ti,2j
	⊤
j=1,...,l
and ˜ti,k is the k-th element of a vector ˜ti which is computed by
˜ti =Uosi−1 + VoEyi−1 + Coci.
Wo ∈RKy×l, Uo ∈R2l×n, Vo ∈R2l×m and Co ∈R2l×2n are weight matrices.
This can be under-
stood as having a deep output (Pascanu et al., 2014) with a single maxout hidden layer (Goodfellow
et al., 2013).
A.2.3
MODEL SIZE
For all the models used in this paper, the size of a hidden layer n is 1000, the word embedding
dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500.
The
number of hidden units in the alignment model n′ is 1000.
B
TRAINING PROCEDURE
B.1
PARAMETER INITIALIZATION
We initialized the recurrent weight matrices U, Uz, Ur, ←−
U , ←−
U z, ←−
U r, −→
U , −→
U z and −→
U r as random or-
thogonal matrices.
For Wa and Ua, we initialized them by sampling each element from the Gaussian
distribution of mean 0 and variance 0.0012.
All the elements of Va and all the bias vectors were ini-
tialized to zero.
Any other weight matrix was initialized by sampling from the Gaussian distribution
of mean 0 and variance 0.012.
B.2
TRAINING
We used the stochastic gradient descent (SGD) algorithm.
Adadelta (Zeiler, 2012) was used to
automatically adapt the learning rate of each parameter (ϵ = 10−6 and ρ = 0.95).
We explicitly
14
Published as a conference paper at ICLR 2015
normalized the L2-norm of the gradient of the cost function each time to be at most a predeﬁned
threshold of 1, when the norm was larger than the threshold (Pascanu et al., 2013b).
Each SGD
update direction was computed with a minibatch of 80 sentences.
At each update our implementation requires time proportional to the length of the longest sentence in
a minibatch.
Hence, to minimize the waste of computation, before every 20-th update, we retrieved
1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches.
The
training data was shufﬂed once before training and was traversed sequentially in this manner.
In Tables 2 we present the statistics related to training all the models used in the experiments.
C
TRANSLATIONS OF LONG SENTENCES
Source
An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre
to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.
Reference
Le privil`ege d’admission est le droit d’un m´edecin, en vertu de son statut de membre soignant
d’un hˆopital, d’admettre un patient dans un hˆopital ou un centre m´edical aﬁn d’y d´elivrer un
diagnostic ou un traitement.
RNNenc-50
Un privil`ege d’admission est le droit d’un m´edecin de reconnaˆıtre un patient `a l’hˆopital ou un
centre m´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant´e.
RNNsearch-50
Un privil`ege d’admission est le droit d’un m´edecin d’admettre un patient `a un hˆopital ou un
centre m´edical pour effectuer un diagnostic ou une proc´edure, selon son statut de travailleur des
soins de sant´e `a l’hˆopital.
Google
Translate
Un privil`ege admettre est le droit d’un m´edecin d’admettre un patient dans un hˆopital ou un
centre m´edical pour effectuer un diagnostic ou une proc´edure, fond´ee sur sa situation en tant
que travailleur de soins de sant´e dans un hˆopital.
Source
This kind of experience is part of Disney’s efforts to ”extend the lifetime of its series and build
new relationships with audiences via digital platforms that are becoming ever more important,”
he added.
Reference
Ce type d’exp´erience entre dans le cadre des efforts de Disney pour ”´etendre la dur´ee de
vie de ses s´eries et construire de nouvelles relations avec son public grˆace `a des plateformes
num´eriques qui sont de plus en plus importantes”, a-t-il ajout´e.
RNNenc-50
Ce type d’exp´erience fait partie des initiatives du Disney pour ”prolonger la dur´ee de vie de
ses nouvelles et de d´evelopper des liens avec les lecteurs num´eriques qui deviennent plus com-
plexes.
RNNsearch-50
Ce genre d’exp´erience fait partie des efforts de Disney pour ”prolonger la dur´ee de vie de ses
s´eries et cr´eer de nouvelles relations avec des publics via des plateformes num´eriques de plus
en plus importantes”, a-t-il ajout´e.
Google
Translate
Ce genre d’exp´erience fait partie des efforts de Disney `a “´etendre la dur´ee de vie de sa s´erie et
construire de nouvelles relations avec le public par le biais des plates-formes num´eriques qui
deviennent de plus en plus important”, at-il ajout´e.
Source
In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might
constitute a ”reasonable motive” that could lead to criminal charges being brought against the
mayor.
Reference
En conf´erence de presse, jeudi, M.
Blair a afﬁrm´e qu’il n’y avait rien dans cette vid´eo qui puisse
constituer des ”motifs raisonnables” pouvant mener au d´epˆot d’une accusation criminelle contre
le maire.
RNNenc-50
Lors de la conf´erence de presse de jeudi, M.
Blair a dit qu’il n’y avait rien dans cette vid´eo qui
pourrait constituer une ”motivation raisonnable” pouvant entraˆıner des accusations criminelles
port´ees contre le maire.
RNNsearch-50
Lors d’une conf´erence de presse jeudi, M.
Blair a d´eclar´e qu’il n’y avait rien dans cette vid´eo qui
pourrait constituer un ”motif raisonnable” qui pourrait conduire `a des accusations criminelles
contre le maire.
Google
Translate
Lors d’une conf´erence de presse jeudi, M.
Blair a d´eclar´e qu’il n’y avait rien dans cette vido
qui pourrait constituer un ”motif raisonnable” qui pourrait mener `a des accusations criminelles
portes contre le maire.
Table 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences
(30 words or more) selected from the test set.
For each source sentence, we also show the gold-
standard translation.
The translations by Google Translate were made on 27 August 2014.
15
3.1
Autoregressive LM
Assume we have an autoregressive language model
pφ(y | x) based on the Transformer (Vaswani et al.,
2017) architecture (e.g., GPT-2; Radford et al.,
2019) and parametrized by φ.
As shown in Fig-
ure 2 (top), let z = [x; y] be the concatenation of x
and y; let Xidx denote the sequence of indices that
corresponds to x, and Yidx denote the same for y.
The activation at time step i is hi ∈Rd, where
hi = [h(1)
i ; · · · ; h(n)
i
] is a concatenation of all acti-
vation layers at this time step, and h(j)
i
is the acti-
vation of the j-th Transformer layer at time step i.1
The autoregressive Transformer model computes
hi as a function of zi and the past activations in its
left context, as follows:
hi = LMφ(zi, h<i),
(1)
where the last layer of hi is used to compute the
distribution for the next token: pφ(zi+1 | h≤i) =
softmax(Wφ h(n)
i
) and Wφ is a pretrained matrix
that map h(n)
i
to logits over the vocabulary.
3.2
Encoder-Decoder Architecture
We can also use an encoder-decoder architecture
(e.g., BART; Lewis et al., 2020) to model pφ(y | x),
where x is encoded by the bidirectional encoder,
and the decoder predicts y autoregressively (condi-
tioned on the encoded x and its left context).
We
use the same indexing and activation notation, as
shown in Figure 2 (bottom).
hi for all i ∈Xidx
is computed by the bidirectional Transformer en-
coder; hi for all i ∈Yidx is computed by the au-
toregressive decoder using the same equation (1).
3.3
Method: Fine-tuning
In the ﬁne-tuning framework, we initialize with the
pretrained parameters φ.
Here pφ is a trainable lan-
guage model distribution and we perform gradient
updates on the following log-likelihood objective:
max
φ
log pφ(y | x) =
X
i∈Yidx
log pφ(zi | h<i).
(2)
4
Preﬁx-Tuning
We propose preﬁx-tuning as an alternative to
ﬁne-tuning for conditional generation tasks.
We
ﬁrst provide intuition in §4.1 before deﬁning our
method formally in §4.2.
1h(n)
i
is composed of a key-value pair.
In GPT-2, the
dimension of each key and value is 1024.
Figure 2: An annotated example of preﬁx-tuning using an autoregressive LM (top) and an encoder-decoder model
(bottom).
The preﬁx activations ∀i ∈Pidx, hi are drawn from a trainable matrix Pθ.
The remaining activations are
computed by the Transformer.
4.1
Intuition
Based on intuition from prompting, we believe that
having a proper context can steer the LM without
changing its parameters.
For example, if we want
the LM to generate a word (e.g., Obama), we can
prepend its common collocations as context (e.g.,
Barack), and the LM will assign much higher prob-
ability to the desired word.
Extending this intuition
beyond generating a single word or sentence, we
want to ﬁnd a context that steers the LM to solve
an NLG task.
Intuitively, the context can inﬂu-
ence the encoding of x by guiding what to extract
from x; and can inﬂuence the generation of y by
steering the next token distribution.
However, it’s
non-obvious whether such a context exists.
Natural
language task instructions (e.g., “summarize the
following table in one sentence”) might guide an
expert annotator to solve the task, but fail for most
pretrained LMs.2 Data-driven optimization over
the discrete instructions might help, but discrete
optimization is computationally challenging.
Instead of optimizing over discrete tokens, we
can optimize the instruction as continuous word em-
beddings, whose effects will be propagated upward
to all Transformer activation layers and rightward
to subsequent tokens.
This is strictly more expres-
sive than a discrete prompt which requires match-
ing the embedding of a real word.
Meanwhile,
this is less expressive than intervening all layers of
the activations (§7.2), which avoids long-range de-
pendencies and includes more tunable parameters.
Preﬁx-tuning, therefore, optimizes all layers of the
preﬁx.
2In our preliminary experiments, GPT-2 and BART fail in
this setting; the only exception is GPT-3.
4.2
Method
Preﬁx-tuning prepends a preﬁx for an autoregres-
sive LM to obtain z = [PREFIX; x; y], or prepends
preﬁxes for both encoder and encoder to obtain
z = [PREFIX; x; PREFIX′; y], as shown in Figure 2.
Here, Pidx denotes the sequence of preﬁx indices,
and we use |Pidx| to denote the length of the preﬁx.
We follow the recurrence relation in equa-
tion (1), except that the preﬁx are free parame-
ters.
Preﬁx-tuning initializes a trainable matrix Pθ
(parametrized by θ) of dimension |Pidx| × dim(hi)
to store the preﬁx parameters.
hi =
(
Pθ[i, :],
if i ∈Pidx,
LMφ(zi, h<i),
otherwise.
(3)
The training objective is the same as equation (2),
but the set of trainable parameters changes: the lan-
guage model parameters φ are ﬁxed and the preﬁx
parameters θ are the only trainable parameters.
Here, hi (for all i) is a function of the trainable
Pθ.
When i ∈Pidx, this is clear because hi copies
directly from Pθ.
When i ̸∈Pidx, hi still depends
on Pθ, because the preﬁx activations are always
in the left context and will therefore affect any
activations to its right.
4.3
Parametrization of Pθ
Empirically, directly updating the Pθ parameters
leads to unstable optimization and a slight drop
in performance.3 So we reparametrize the matrix
Pθ[i, :] = MLPθ(P ′
θ[i, :]) by a smaller matrix (P ′
θ)
composed with a large feedforward neural network
(MLPθ).
Note that Pθ and P ′
θ has the same rows
3We ﬁnd in preliminary experiments that directly opti-
mizing the preﬁx is very sensitive to the learning rate and
initialization.
dimension (i.e.
the preﬁx length), but different
columns dimension.4 Once training is complete,
these reparametrization parameters can be dropped,
and only the preﬁx (Pθ) needs to be saved.
5
Experimental Setup
5.1
Datasets and Metrics
We evaluate on three standard neural generation
datasets for the table-to-text task: E2E (Novikova
et al., 2017), WebNLG (Gardent et al., 2017), and
DART (Radev et al., 2020).
The datasets are or-
dered by increasing complexity and size.
E2E only
has 1 domain (i.e.
restaurant reviews); WebNLG
has 14 domains, and DART is open-domain, using
open-domain tables from Wikipedia.
The E2E dataset contains approximately 50K
examples with 8 distinct ﬁelds; it contains multiple
test references for one source table, and the average
output length is 22.9.
We use the ofﬁcial evaluation
script, which reports BLEU (Papineni et al., 2002),
NIST (Belz and Reiter, 2006), METEOR (Lavie
and Agarwal, 2007), ROUGE-L (Lin, 2004), and
CIDEr (Vedantam et al., 2015).
The WebNLG (Gardent et al., 2017) dataset con-
sists of 22K examples, and the input x is a sequence
of (subject, property, object) triples.
The average
output length is 22.5.
In the training and validation
splits, the input describes entities from 9 distinct
DBpedia categories (e.g., Monument).
The test
split consists of two parts: the ﬁrst half contains
DB categories seen in training data, and the sec-
ond half contains 5 unseen categories.
These un-
seen categories are used to evaluate extrapolation.
We use the ofﬁcial evaluation script, which reports
BLEU, METEOR and TER (Snover et al., 2006).
DART (Radev et al., 2020) is an open domain
table-to-text dataset, with similar input format
(entity-relation-entity triples) as WebNLG.
The av-
erage output length is 21.6.
It consists of 82K ex-
amples from WikiSQL, WikiTableQuestions, E2E,
and WebNLG and applies some manual or auto-
mated conversion.
We use the ofﬁcial evaluation
script and report BLEU, METEOR, TER, Mover-
Score (Zhao et al., 2019), BERTScore (Zhang et al.,
2020b) and BLEURT (Sellam et al., 2020).
For the summarization task, we use the XSUM
(Narayan et al., 2018) dataset, which is an abstrac-
4Pθ has a dimension of |Pidx| × dim(hi) while Pθ has
a dimension of |Pidx| × k, where we choose k = 512 for
table-to-text and 800 for summarization.
MLPθ maps from
dimension k to dim(hi)
tive summarization dataset on news articles.
There
are 225K examples.
The average length of the ar-
ticles is 431 words and the average length of the
summaries is 23.3.
We report ROUGE-1, ROUGE-
2 and ROUGE-L.
5.2
We have proposed preﬁx-tuning, a lightweight al-
ternative to ﬁne-tuning that prepends a trainable
continuous preﬁx for NLG tasks.
We discover that
despite learning 1000x fewer parameters than ﬁne-
tuning, preﬁx-tuning can maintain a comparable
performance in a full data setting and outperforms
ﬁne-tuning in both low-data and extrapolation set-
tings.
References
Armen Aghajanyan, Luke Zettlemoyer, and Sonal
Gupta.
2020.
Intrinsic dimensionality explains the
effectiveness of language model ﬁne-tuning.
Anja Belz and Ehud Reiter.
2006.
Comparing auto-
matic and human evaluation of NLG systems.
In
11th Conference of the European Chapter of the
Association for Computational Linguistics, Trento,
Italy.
Association for Computational Linguistics.
Tom B.
Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei.
2020.
Language models are few-shot learn-
ers.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
Hung, Eric Frank, Piero Molino, Jason Yosinski, and
Rosanne Liu.
2020.
Plug and play language mod-
els: A simple approach to controlled text generation.
In International Conference on Learning Represen-
tations.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota.
Associ-
ation for Computational Linguistics.
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini.
2017.
The WebNLG
challenge: Generating text from RDF data.
In Pro-
ceedings of the 10th International Conference on
Natural Language Generation, pages 124–133, San-
tiago de Compostela, Spain.
Association for Compu-
tational Linguistics.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019.
Parameter-efﬁcient transfer learning for NLP.
In Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pages 2790–2799,
Long Beach, California, USA.
PMLR.
Zhengbao Jiang, Frank F.
Xu, Jun Araki, and Graham
Neubig.
2020.
How can we know what language
models know?
Transactions of the Association for
Computational Linguistics, 8:423–438.
Mihir Kale.
2020.
Text-to-text pre-training for data-to-
text tasks.
N.
Keskar, B.
McCann, L.
R.
Varshney, Caiming Xiong,
and R.
Socher.
2019.
Ctrl: A conditional trans-
former language model for controllable generation.
ArXiv, abs/1909.05858.
Ben Krause, Akhilesh Deepak Gotmare, Bryan Mc-
Cann, Nitish Shirish Keskar, Shaﬁq Joty, Richard
Socher, and Nazneen Fatema Rajani.
2020.
GeDi:
Generative Discriminator Guided Sequence Genera-
tion.
arXiv preprint arXiv:2009.06367.
Alon Lavie and Abhaya Agarwal.
2007.
Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments.
In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, pages 228–231, Strouds-
burg, PA, USA.
Association for Computational Lin-
guistics.
Mike
Lewis,
Yinhan
Liu,
Naman
Goyal,
Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020.
BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension.
In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871–7880, Online.
Association
for Computational Linguistics.
Chin-Yew Lin.
2004.
ROUGE: A package for auto-
matic evaluation of summaries.
In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Zhaojiang Lin, Andrea Madotto, and Pascale Fung.
2020.
Exploring versatile generative language
model via parameter-efﬁcient transfer learning.
In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020, pages 441–459, Online.
As-
sociation for Computational Linguistics.
Yang Liu and Mirella Lapata.
2019.
Text summariza-
tion with pretrained encoders.
In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,
China.
Association for Computational Linguistics.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer.
2020.
Multilingual denoising
pre-training for neural machine translation.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov.
2019.
Roberta: A robustly optimized BERT pretraining ap-
proach.
CoRR, abs/1907.11692.
Ilya Loshchilov and Frank Hutter.
2019.
Decoupled
weight decay regularization.
In International Con-
ference on Learning Representations.
H.
Brendan McMahan, Eider Moore, Daniel Ramage,
and Blaise Ag¨uera y Arcas.
2016.
Federated learn-
ing of deep networks using model averaging.
Pro-
ceedings of the 20 th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS) 2017,
abs/1602.05629.
Shashi Narayan, Shay B.
Cohen, and Mirella Lapata.
2018.
Don’t give me the details, just the summary!
Topic-aware convolutional neural networks for ex-
treme summarization.
In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium.
Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
2017.
The E2E dataset: New challenges for end-to-
end generation.
CoRR, abs/1706.09254.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu.
2002.
Bleu: A method for automatic eval-
uation of machine translation.
In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA.
Association for Computa-
tional Linguistics.
Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e,
Kyunghyun
Cho,
and
Iryna
Gurevych.
2020.
Adapterfusion:
Non-destructive task composition
for transfer learning.
Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand
Sivaprasad, Chiachun Hsieh, Nazneen Fatema Ra-
jani, Xiangru Tang, Aadit Vyas, Neha Verma,
Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto,
Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori
Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu,
Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and
Richard Socher.
2020.
Dart: Open-domain struc-
tured data record to text generation.
A.
Radford, Jeffrey Wu, R.
Child, David Luan, Dario
Amodei, and Ilya Sutskever.
2019.
Language mod-
els are unsupervised multitask learners.
Evani Radiya-Dixit and Xin Wang.
2020.
How ﬁne can
ﬁne-tuning be?
learning efﬁcient language models.
In Proceedings of the Twenty Third International
Conference on Artiﬁcial Intelligence and Statistics,
volume 108 of Proceedings of Machine Learning Re-
search, pages 2435–2443, Online.
PMLR.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J.
Liu.
2020.
Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer.
Journal of Machine Learning Re-
search, 21(140):1–67.
Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea
Vedaldi.
2017.
Learning multiple visual domains
with residual adapters.
In Advances in Neural Infor-
mation Processing Systems, volume 30, pages 506–
516.
Curran Associates, Inc.
Timo Schick and Hinrich Sch¨utze.
2020.
Exploiting
cloze questions for few shot text classiﬁcation and
natural language inference.
Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020.
BLEURT: Learning robust metrics for text
generation.
In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7881–7892, Online.
Association for Computa-
tional Linguistics.
Sheng Shen, Daniel Fried, Jacob Andreas, and Dan
Klein.
2019.
Pragmatically informative text gen-
eration.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4060–4067, Minneapolis, Minnesota.
Associ-
ation for Computational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L.
Logan IV
au2, Eric Wallace, and Sameer Singh.
2020.
Auto-
prompt: Eliciting knowledge from language models
with automatically generated prompts.
Reza Shokri and Vitaly Shmatikov.
2015.
Privacy-
preserving deep learning.
In Proceedings of
the 22nd ACM SIGSAC Conference on Computer
and Communications Security, CCS ’15, page
1310–1321, New York, NY, USA.
Association for
Computing Machinery.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and Ralph Weischedel.
2006.
A study
of translation error rate with targeted human annota-
tion.
In In Proceedings of the Association for Ma-
chine Transaltion in the Americas (AMTA 2006.
Asa
Cooper
Stickland,
Xian
Li,
and
Marjan
Ghazvininejad.
2020.
Recipes
for
adapting
pre-trained monolingual and multilingual models to
machine translation.
Nishant
Subramani,
Samuel
R.
Bowman,
and
Kyunghyun Cho.
2020.
Can unconditional lan-
guage models recover arbitrary sentences?
Fan-Keng Sun and Cheng-I Lai.
2020.
Conditioned
natural language generation using only uncondi-
tioned language model: An exploration.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin.
2017.
Attention is all
you need.
In Advances in Neural Information Pro-
cessing Systems, volume 30, pages 5998–6008.
Cur-
ran Associates, Inc.
Ramakrishna Vedantam, C.
Lawrence Zitnick, and
Devi Parikh.
2015.
Cider: Consensus-based image
description evaluation.
In CVPR, pages 4566–4575.
IEEE Computer Society.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M.
Rush.
2020.
Transformers: State-of-the-art natural language pro-
cessing.
In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online.
Asso-
ciation for Computational Linguistics.
Jeffrey
O
Zhang,
Alexander
Sax,
Amir
Zamir,
Leonidas Guibas, and Jitendra Malik.
2020a.
Side-
tuning: A baseline for network adaptation via addi-
tive side networks.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi.
2020b.
BERTScore:
Evaluating text generation with bert.
In Interna-
tional Conference on Learning Representations.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan.
2020c.
DIALOGPT : Large-
scale generative pre-training for conversational re-
sponse generation.
In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics:
System Demonstrations, pages 270–
278, Online.
Association for Computational Linguis-
tics.
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-
rich Sch¨utze.
2020.
Masking as an efﬁcient alterna-
tive to ﬁnetuning for pretrained language models.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M.
Meyer, and Steffen Eger.
2019.
MoverScore:
Text generation evaluating with contextualized em-
beddings and earth mover distance.
In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 563–578, Hong
Kong, China.
Association for Computational Lin-
guistics.
Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,
Xipeng Qiu, and Xuanjing Huang.
2020.
Extrac-
tive summarization as text matching.
In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 6197–6208, On-
line.
Association for Computational Linguistics.
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tieyan Liu.
2020.
Incorporating bert into neural machine translation.
In International Conference on Learning Represen-
tations.
learning rate
# epoch
batch size
preﬁx length
Preﬁx:
E2E
8e-05
5
10
5
WebNLG
5e-05
5
5
5
DART
5e-05
10
5
10
XSUM
5e-05
30
14
100
Adapter:
E2E (3%)
5e-05
5
5
-
E2E (0.1%)
8e-05
10
5
WebNLG (3%)
5e-05
5
5
-
WebNLG (0.1%)
5e-05
10
5
-
DART (3%)
5e-05
5
5
-
DART (0.1%)
8e-05
5
5
-
Fine-tune:
E2E
5e-05
5
10
-
WebNLG
1e-05
10
6
-
DART
1e-05
10
6
-
FT-top2:
E2E
5e-05
5
10
-
WebNLG
5e-05
10
9
-
DART
5e-05
5
5
-
Table 5: Hyperparameter settings for our method and
baseline methods.
A
Supplementary Material
A.1
Hyperparameters
In Table 5, we report the hyperparameters used
to train the models documented in the experiment
section.
A.2
Additional Results for Low-data Settings
Figure 6 supplements the low-data performance
curves in Figure 3 by plotting the relationship be-
tween training size and generation metrics for both
preﬁx-tuning and ﬁne-tuning.
A.3
Additional Results for the Initialization
Experiment
Figure 7 supplements Figure 3 by plotting addi-
tional metrics for our initialization technique §7.4.
It validates that random initialization (from a uni-
form (0,1) distirbution) signiﬁcantly underperforms
initializing with real words; Additionally, initializ-
ing with task-relevant words (e.g., “summarization”
and “table-to-text”) attains slightly better gener-
ation scores than initializing with task-irrelevant
words (e.g., “elephant” and “banana”).
A.4
Qualitative Examples for Extrapolation
Table 6 contains qualitative examples from both
seen and unseen categories in WebNLG.
We ﬁnd
that for unseen categories, both preﬁx-tuning and
ﬁne-tuning tend to undergenerate (generated out-
put do not cover full table contents) or generate
untruthfully (generated output is inconsistent with
table contents).
In particular, preﬁx-tuning tends to
undergenerate whereas ﬁne-tuning tends to gener-
ate untruthfully.
For seen categories, both perform
fairly well in terms of coverage and truthfulness.
100
200
300
400
500
training_data_size
32
34
36
rouge-1
method
FT
PT
100
200
300
400
500
training_data_size
10
11
12
13
14
15
rouge-2
method
FT
PT
100
200
300
400
500
training_data_size
24
26
28
rouge-L
method
FT
PT
100
200
300
400
500
training_data_size
3
4
5
6
7
NIST
method
FT
PT
100
200
300
400
500
training_data_size
0.32
0.34
0.36
0.38
METEOR
method
FT
PT
100
200
300
400
500
training_data_size
1.2
1.4
1.6
1.8
CIDEr
method
FT
PT
Figure 6: Preﬁx-tuning (orange) outperforms ﬁne-tuning (blue) in low-data regimes in addition to requiring many
fewer parameters.
The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and
ROUGE-L.
The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr.
The
x-axis is the training size and the y-axis is the evaluation metric (higher is better).
random init
"active"
"elephant"
"summarize"
"table-to-text:"
"banana"
"beautiful"
"divide"
"keep"
2
3
4
5
6
7
NIST
random init
"active"
"elephant"
"summarize"
"table-to-text:"
"banana"
"beautiful"
"divide"
"keep"
0.30
0.32
0.34
0.36
0.38
METEOR
random init
"active"
"elephant"
"summarize"
"table-to-text:"
"banana"
"beautiful"
"divide"
"keep"
0.58
0.60
0.62
0.64
0.66
ROUGE
random init
"active"
"elephant"
"summarize"
"table-to-text:"
"banana"
"beautiful"
"divide"
"keep"
1.0
1.2
1.4
1.6
1.8
CIDEr
Figure 7: Initializing the preﬁx with activations of real words signiﬁcantly outperforms random initialization, in a
low-data setting with 100 training data.
Source [Unseen, Athelete]
(Al Kharaitiyat SC, club, Alaa Abdul-Zahra), (Al Khor, ground, Al Kharaitiyat SC), (Shabab Al-Ordon Club,
club, Alaa Abdul-Zahra) (Amar Osim, manager, Al Kharaitiyat SC)
Preﬁx-tuning
Al Kharaitiyat SC are managed by Amar Osim and play at their ground at Al Khor.
Al Kharaitiyat SC are also
the club for which Alaa Abdul-Zahra is a player.
Fine-tuning
Alaa Abdul-Zahra plays for Al-Kharaitiyat SC and Shabab Al-Ordon Club.
He also plays for Al-Khor and
manages Al-Kharaitiyat SC.
Reference
Alaa Abdul Zahra plays for Al Kharaitiyat SC which is located at Al Khor and managed by Amar Osim.
The
Shabab Al-Ordon club is associated with Alaa Abdul-Zahra.
Source [Unseen, Transportation]
(Genoa, location, Costa Crociere), (AIDA Cruises, operator, AIDAstella), (Costa Crociere, owner, AIDAstella)
Preﬁx-tuning
AID Astella is operated by Aida Cruises and is owned by the Costa Rican tourist resort of Genoa.
Fine-tuning
AID Astella, operated by AIDA-Cruises, is located in Genoa and is owned by the Costa Rican government.
Reference
Costa Crociere is the owner of the AIDAstella and are based in Genoa.
The operator of AIDAstella is AIDA
Cruises.
Source [Unseen, Politician]
(Euro, currency, Netherlands), (Stellendam, birthPlace, Ab Klink ), (Netherlands, nationality, Ab Klink)
Preﬁx-tuning
Ab Klink was born in Stellendam and is a national of the Netherlands where the currency is the Euro.
Fine-tuning
Ab Klink is a national of the Netherlands where the currency is the Euro.
He was born in Stellendam.
Reference
Ab Klink was born in Stellendam in the Netherlands, where the national currency is the euro.
Source [Unseen, Politician]
(Robert E, Lee, commander, Battle of Salem Church), (American Civil War, isPartOfMilitaryConﬂict, Battle of
Salem Church), (Battle of Salem Church, battles, Aaron S.
Daggett)
Preﬁx-tuning
Robert E.
Lee was the commander of the Battle of Salem Church which was part of the military conﬂict in the
American Civil war.
Fine-tuning
The Battle of Salem Church is part of the American Civil War and was commanded by Robert E.
Lee.
Reference
Robert E Lee was a commander in the Battle of Salem Church, which was one of the military conﬂicts in the
American Civil War.
Aaron S Daggett fought in the same battle.
Source [Unseen, Artist]
(Christian alternative rock, musicSubgenre, Alternative rock), (Alternative rock, genre, Andrew White (musi-
cian))
Preﬁx-tuning
Andrew White is a Christian alternative rock musician.
Fine-tuning
Andrew White, a Christian alternative rocker, performs.
Reference
The musician Andrew White’s genre is alternative rock, the genre which has the sub genre Christian alternative
rock.
Source [Unseen, Artist]
(Hip hop music, genre, Allen Forrest), (solo singer, background, Allen Forrest)
Preﬁx-tuning
Allen Forrest is a solo singer.
Fine-tuning
Born in
Reference
Allen Forrest is a solo singer whose genre is Hip Hop music.
Source [Seen, ComicsCharacter]
(Americans, nationality, Ducan Rouleau), (Ducan Rouleau, creator, Baymax),(Alan Tudyk, starring, Big Hero 6
(ﬁlm)), (Steven T Segle, creator, Baymax), (Big Hero 6 (ﬁlm), serires, Baymax)
Preﬁx-tuning
Baymax is a character in Big Hero 6 which stars Alan Tudyk.
He was created by Steven T.
Seagle and the
American, Duncan Rouleau.
Fine-tuning
Alan Tudyk stars in the ﬁlm Big Hero 6 in which Baymax is a character created by Steven T.
Seagle and the
American, Duncan Rouleau.
Reference
Baymax is a character who appeared in Big Hero 6 starring Alan Tudyk.
It was created by Steven T Seagle and
the American, Duncan Rouleau.
Source [Seen, City]
(Washington, D.C., capital, United States), (White Americans, ethnicGroup, United States), (United States,
country, New Jersey), (New York City, largest City, United States), (New Jersy, isPartOf, Atlantic City)
Preﬁx-tuning
Washington D.C.
is the capital of the United States where the largest city is New York City and the White
Americans are an ethnic group.
Atlantic City, New Jersey is also part of the United States.
Fine-tuning
Atlantic City, New Jersey is part of New Jersey in the United States.
The capital city is Washington D.C.
and
one of the ethnic groups is White Americans.
Reference
New York City (NYC) is the largest U.S.
city.
Atlantic City, New Jersey are also part of the United States with
its capital as Washington, DC and home to White Americans.
Table 6: Qualitative examples from WebNLG.
The ﬁrst 6 examples are from the unseen categories, labeled next
to source; the last two examples are from the seen categories.
For unseen categories, both preﬁx-tuning and ﬁne-
tuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated
output is inconsistent with table contents).
In particular, preﬁx-tuning tends to undergenerate more often than
generate untruthfully whereas ﬁne-tuning tends to generate untruthfully.
For seen categories, both perform fairly
well in terms of coverage and truthfulness.
In this section, we have discussed the design principles behind graph networks: ﬂexible representa-
tions, conﬁgurable within-block structure, and composable multi-block architectures.
These three
design principles combine in our framework which is extremely ﬂexible and applicable to a wide range
of domains ranging from perception, language, and symbolic reasoning.
And, as we will see in the
remainder of this paper, the strong relational inductive bias possessed by graph networks supports
combinatorial generalization, thus making it a powerful tool both in terms of implementation and
theory.
Recent advances in AI, propelled by deep learning, have been transformative across many important
domains.
Despite this, a vast gap between human and machine intelligence remains, especially with
respect to eﬃcient, generalizable learning.
We argue for making combinatorial generalization a top
priority for AI, and advocate for embracing integrative approaches which draw on ideas from human
cognition, traditional computer science, standard engineering practice, and modern deep learning.
Here we explored ﬂexible learning-based approaches which implement strong relational inductive
biases to capitalize on explicitly structured representations and computations, and presented a
framework called graph networks, which generalize and extend various recent approaches for neural
networks applied to graphs.
Graph networks are designed to promote building complex architectures
using customizable graph-to-graph building blocks, and their relational inductive biases promote
combinatorial generalization and improved sample eﬃciency over other standard machine learning
building blocks.
Despite their beneﬁts and potential, however, learnable models which operate on graphs are
only a stepping stone on the path toward human-like intelligence.
We are optimistic about a
number of other relevant, and perhaps underappreciated, research directions, including marrying
24
learning-based approaches with programs (Ritchie et al., 2016; Andreas et al., 2016; Gaunt et al.,
2016; Evans and Grefenstette, 2018; Evans et al., 2018), developing model-based approaches with an
emphasis on abstraction (Kansky et al., 2017; Konidaris et al., 2018; Zhang et al., 2018; Hay et al.,
2018), investing more heavily in meta-learning (Wang et al., 2016, 2018a; Finn et al., 2017), and
exploring multi-agent learning and interaction as a key catalyst for advanced intelligence (Nowak,
2006; Ohtsuki et al., 2006).
These directions each involve rich notions of entities, relations, and
combinatorial generalization, and can potentially beneﬁt, and beneﬁt from, greater interaction with
approaches for learning relational reasoning over explicitly structured representations.
Acknowledgements
We thank Tobias Pfaﬀ, Danilo Rezende, Nando de Freitas, Murray Shanahan, Thore Graepel, John
Jumper, Demis Hassabis, and the broader DeepMind and Google communities for valuable feedback
and support.
References
Allamanis, M., Brockschmidt, M., and Khademi, M.
(2018).
Learning to represent programs with
graphs.
In Proceedings of the International Conference on Learning Representations (ICLR).
Allamanis, M., Chanthirasegaran, P., Kohli, P., and Sutton, C.
(2017).
Learning continuous
semantic representations of symbolic expressions.
In Proceedings of the International Conference
on Machine Learning (ICML).
Anderson, J.
R.
(1982).
Acquisition of cognitive skill.
Psychological Review, 89(4):369.
Andreas, J., Klein, D., and Levine, S.
(2017).
Modular multitask reinforcement learning with policy
sketches.
In Proceedings of the International Conference on Machine Learning (ICML).
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D.
(2016).
Neural module networks.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 39–48.
Ba, J.
L., Kiros, J.
R., and Hinton, G.
E.
(2016).
Layer normalization.
arXiv preprint
arXiv:1607.06450.
Bahdanau, D., Cho, K., and Bengio, Y.
(2015).
Neural machine translation by jointly learning to
align and translate.
In Proceedings of the International Conference on Learning Representations
(ICLR).
Battaglia, P., Pascanu, R., Lai, M., Rezende, D.
J., et al.
(2016).
Interaction networks for learning
about objects, relations and physics.
In Advances in Neural Information Processing Systems,
pages 4502–4510.
Battaglia, P.
W., Hamrick, J.
B., and Tenenbaum, J.
B.
(2013).
Simulation as an engine of physical
scene understanding.
Proceedings of the National Academy of Sciences, 110(45):18327–18332.
Bello, I., Pham, H., Le, Q.
V., Norouzi, M., and Bengio, S.
(2016).
Neural combinatorial optimization
with reinforcement learning.
arXiv preprint arXiv:1611.09940.
Bobrow, D.
G.
and Hinton, G.
E., editors (1990).
Artiﬁcial Intelligence, volume 46.
Elsevier Science
Publishers Ltd., Essex, UK.
Special Issue 1-2: On Connectionist Symbol Processing.
25
Bojchevski, A., Shchur, O., Z¨ugner, D., and G¨unnemann, S.
(2018).
Netgan: Generating graphs via
random walks.
arXiv preprint arXiv:1803.00816.
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O.
(2013).
Translating
embeddings for modeling multi-relational data.
In Advances in Neural Information Processing
Systems, pages 2787–2795.
Botvinick, M.
M.
(2008).
Hierarchical models of behavior and prefrontal function.
Trends in
Cognitive Sciences, 12(5):201–208.
Bronstein, M.
M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P.
(2017).
Geometric deep
learning: going beyond euclidean data.
IEEE Signal Processing Magazine, 34(4):18–42.
Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y.
(2014).
Spectral networks and locally connected
networks on graphs.
In Proceedings of the International Conference on Learning Representations
(ICLR).
Chang, M.
B., Ullman, T., Torralba, A., and Tenenbaum, J.
B.
(2017).
A compositional object-
based approach to learning physical dynamics.
In Proceedings of the International Conference on
Learning Representations (ICLR).
Chen, X., Li, L., Fei-Fei, L., and Gupta, A.
(2018a).
Iterative visual reasoning beyond convolutions.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Chen, X., Liu, C., and Song, D.
(2018b).
Tree-to-tree neural networks for program translation.
In
Workshops of the International Conference on Learning Representations (ICLR).
Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio,
Y.
(2014).
Learning phrase representations using rnn encoder-decoder for statistical machine
translation.
In Proceeding of the Conference on Empirical Methods in Natural Language Processing
(EMNLP).
Chomsky, N.
(1957).
Syntactic Structures.
Mouton & Co.
Chomsky, N.
(1965).
Aspects of the Theory of Syntax.
MIT Press.
Cohen, T.
and Welling, M.
(2016).
Group equivariant convolutional networks.
In International
Conference on Machine Learning, pages 2990–2999.
Craik, K.
J.
W.
(1943).
The Nature of Explanation.
Cambridge University Press.
Cui, Z., Henrickson, K., Ke, R., and Wang, Y.
(2018).
High-order graph convolutional recurrent
neural network: A deep learning framework for network-scale traﬃc learning and forecasting.
arXiv preprint arXiv:1802.07007.
Dai, H., Dai, B., and Song, L.
(2016).
Discriminative embeddings of latent variable models for
structured data.
In Proceedings of the International Conference on Machine Learning (ICML).
Dai, H., Khalil, E.
B., Zhang, Y., Dilkina, B., and Song, L.
(2017).
Learning combinatorial
optimization algorithms over graphs.
In Advances in Neural Information Processing Systems.
De Cao, N.
and Kipf, T.
(2018).
MolGAN: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973.
26
Deﬀerrard, M., Bresson, X., and Vandergheynst, P.
(2016).
Convolutional neural networks on graphs
with fast localized spectral ﬁltering.
In Advances in Neural Information Processing Systems, pages
3844–3852.
Denil, M., Colmenarejo, S.
G., Cabi, S., Saxton, D., and de Freitas, N.
(2017).
Programmable
agents.
arXiv preprint arXiv:1706.06383.
Devlin, J., Uesato, J., Singh, R., and Kohli, P.
(2017).
Semantic code repair using neuro-symbolic
transformation networks.
arXiv preprint arXiv:1710.11054.
Duvenaud, D.
K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and
Adams, R.
P.
(2015).
Convolutional networks on graphs for learning molecular ﬁngerprints.
In
Advances in Neural Information Processing Systems, pages 2224–2232.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N.
A.
(2015).
Transition-based
dependency parsing with stack long short-term memory.
In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL).
Dˇzeroski, S., De Raedt, L., and Driessens, K.
(2001).
Relational reinforcement learning.
Machine
Learning, 43(1-2):7–52.
Edwards, H.
and Storkey, A.
(2016).
Towards a neural statistician.
arXiv preprint arXiv:1606.02185.
Eliasmith, C.
(2013).
How to build a brain: A neural architecture for biological cognition.
Oxford
University Press.
Elman, J.
L.
(1990).
Finding structure in time.
Cognitive Science, 14(2):179–211.
Elman, J.
L.
(1991).
Distributed representations, simple recurrent networks, and grammatical
structure.
Machine Learning, 7(2-3):195–225.
Eslami, S.
A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Hinton, G.
E., et al.
(2016).
Attend,
infer, repeat: Fast scene understanding with generative models.
In Advances in Neural Information
Processing Systems, pages 3225–3233.
Evans, R.
and Grefenstette, E.
(2018).
Learning explanatory rules from noisy data.
Journal of
Artiﬁcial Intelligence Research, 61:1–64.
Evans, R., Saxton, D., Amos, D., Kohli, P., and Grefenstette, E.
(2018).
Can neural networks
understand logical entailment?
In Proceedings of the International Conference on Learning
Representations (ICLR).
Farquhar, G., Rockt¨aschel, T., Igl, M., and Whiteson, S.
(2018).
TreeQN and ATreeC: Diﬀerentiable
tree planning for deep reinforcement learning.
In Proceedings of the International Conference on
Learning Representations (ICLR).
Finn, C., Abbeel, P., and Levine, S.
(2017).
Model-agnostic meta-learning for fast adaptation of
deep networks.
arXiv preprint arXiv:1703.03400.
Fodor, J.
A.
(1975).
The Language of Thought.
Harvard University Press.
Fodor, J.
A.
and Pylyshyn, Z.
W.
(1988).
Connectionism and cognitive architecture: A critical
analysis.
Cognition, 28(1-2):3–71.
27
Foerster, J., Assael, I.
A., de Freitas, N., and Whiteson, S.
(2016).
Learning to communicate with
deep multi-agent reinforcement learning.
In Advances in Neural Information Processing Systems,
pages 2137–2145.
Fukushima, K.
(1980).
Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaﬀected by shift in position.
Biological Cybernetics, 36:193–202.
Garcia, V.
and Bruna, J.
(2018).
Few-shot learning with graph neural networks.
In Proceedings of
the International Conference on Learning Representations (ICLR).
Garc´ıa-Dur´an, A.
and Niepert, M.
(2017).
Learning graph representations with embedding propaga-
tion.
arXiv preprint arXiv:1710.03059.
Garnelo, M., Arulkumaran, K., and Shanahan, M.
(2016).
Towards deep symbolic reinforcement
learning.
arXiv preprint arXiv:1609.05518.
Gaunt, A.
L., Brockschmidt, M., Kushman, N., and Tarlow, D.
(2016).
Diﬀerentiable programs
with neural libraries.
arXiv preprint arXiv:1611.02109.
Geman, S., Bienenstock, E., and Doursat, R.
(1992).
Neural networks and the bias/variance dilemma.
Neural Computation, 4(1):1–58.
Gentner, D.
and Markman, A.
B.
(1997).
Structure mapping in analogy and similarity.
American
Psychologist, 52(1):45.
Getoor, L.
and Taskar, B.
(2007).
Introduction to Statistical Relational Learning.
MIT press.
Ghahramani, Z.
(2015).
Probabilistic machine learning and artiﬁcial intelligence.
Nature,
521(7553):452.
Gilmer, J., Schoenholz, S.
S., Riley, P.
F., Vinyals, O., and Dahl, G.
E.
(2017).
Neural message
passing for quantum chemistry.
arXiv preprint arXiv:1704.01212.
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
(2016).
Deep Learning.
MIT Press.
Goodman, N.
(1955).
The new riddle of induction.
In Fact, Fiction, and Forecast, pages 59–83.
Harvard University Press.
Goodman, N., Mansinghka, V., Roy, D.
M., Bonawitz, K., and Tenenbaum, J.
B.
(2012).
Church: a
language for generative models.
arXiv preprint arXiv:1206.3255.
Goodman, N.
D., Tenenbaum, J.
B., and Gerstenberg, T.
(2015).
Concepts in a probabilistic
language of thought.
In Margolis, E.
and Laurence, S., editors, The Conceptual Mind: New
Directions in the Study of Concepts.
MIT Press.
Goodwin, G.
P.
and Johnson-Laird, P.
(2005).
Reasoning about relations.
Psychological Review,
112(2):468.
Gori, M., Monfardini, G., and Scarselli, F.
(2005).
A new model for learning in graph domains.
In
Proceedings of the International Joint Conference on Neural Networks (IJCNN), volume 2, pages
729–734.
IEEE.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi´nska, A., Colmenarejo,
S.
G., Grefenstette, E., Ramalho, T., Agapiou, J., et al.
(2016).
Hybrid computing using a neural
network with dynamic external memory.
Nature, 538(7626):471.
28
Grefenstette, E., Hermann, K.
M., Suleyman, M., and Blunsom, P.
(2015).
Learning to transduce
with unbounded memory.
In Advances in Neural Information Processing Systems, pages 1828–1836.
Griﬃths, T.
L., Chater, N., Kemp, C., Perfors, A., and Tenenbaum, J.
B.
(2010).
Probabilistic
models of cognition: Exploring representations and inductive biases.
Trends in Cognitive Sciences,
14(8):357–364.
Grover, A.
and Leskovec, J.
(2016).
node2vec: Scalable feature learning for networks.
In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 855–864.
ACM.
Guez, A., Weber, T., Antonoglou, I., Simonyan, K., Vinyals, O., Wierstra, D., Munos, R., and
Silver, D.
(2018).
Learning to search with MCTSnets.
arXiv preprint arXiv:1802.04697.
Gulcehre, C., Denil, M., Malinowski, M., Razavi, A., Pascanu, R., Hermann, K.
M., Battaglia, P.,
Bapst, V., Raposo, D., Santoro, A., and de Freitas, N.
(2018).
Hyperbolic attention networks.
arXiv preprint arXiv:1805.09786.
Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y.
(2017).
Knowledge transfer for out-of-
knowledge-base entities: A graph neural network approach.
In Proceedings of the International
Joint Conference on Artiﬁcial Intelligence (IJCAI).
Hamilton, W., Ying, Z., and Leskovec, J.
(2017).
Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pages 1025–1035.
Hamrick, J., Allen, K., Bapst, V., Zhu, T., McKee, K., Tenenbaum, J., and Battaglia, P.
(2018).
Relational inductive bias for physical construction in humans and machines.
In Proceedings of the
40th Annual Conference of the Cognitive Science Society.
Hamrick, J.
B., Ballard, A.
J., Pascanu, R., Vinyals, O., Heess, N., and Battaglia, P.
W.
(2017).
Metacontrol for adaptive imagination-based optimization.
In Proceedings of the International
Conference on Learning Representations (ICLR).
Hartford, J., Graham, D.
R., Leyton-Brown, K., and Ravanbakhsh, S.
(2018).
Deep models of
interactions across sets.
arXiv preprint arXiv:1803.02879.
Hay, N., Stark, M., Schlegel, A., Wendelken, C., Park, D., Purdy, E., Silver, T., Phoenix, D.
S.,
and George, D.
(2018).
Behavior is everything–towards representing concepts with sensorimotor
contingencies.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI).
Henaﬀ, M., Bruna, J., and LeCun, Y.
(2015).
Deep convolutional networks on graph-structured
data.
arXiv preprint arXiv:1506.05163.
Hinton, G.
E.
(1990).
Mapping part-whole hierarchies into connectionist networks.
Artiﬁcial
Intelligence, 46(1-2):47–75.
Hjort, N.
L., Holmes, C., M¨uller, P., and Walker, S.
G.
(2010).
Bayesian Nonparametrics.
Cambridge
University Press.
Hoshen, Y.
(2017).
Vain: Attentional multi-agent predictive modeling.
In Advances in Neural
Information Processing Systems, pages 2698–2708.
Hu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y.
(2017).
Relation networks for object detection.
arXiv preprint arXiv:1711.11575.
29
Hudson, D.
A.
and Manning, C.
D.
(2018).
Compositional attention networks for machine reasoning.
In Proceedings of the International Conference on Learning Representations (ICLR).
Humboldt, W.
(1999/1836).
On Language: On the diversity of human language construction and its
inﬂuence on the mental development of the human species.
Cambridge University Press.
Hummel, J.
E.
and Holyoak, K.
J.
(2003).
A symbolic-connectionist theory of relational inference
and generalization.
Psychological Review, 110(2):220.
Ioﬀe, S.
and Szegedy, C.
(2015).
Batch normalization: Accelerating deep network training by reducing
internal covariate shift.
In Proceedings of the 32nd International Conference on International
Conference on Machine Learning (ICML).
Johnson, D.
D.
(2017).
Learning graphical state transitions.
Proceedings of the International
Conference on Learning Representations (ICLR).
Joulin, A.
and Mikolov, T.
(2015).
Inferring algorithmic patterns with stack-augmented recurrent
nets.
In Advances in Neural Information Processing Systems, pages 190–198.
Kansky, K., Silver, T., M´ely, D.
A., Eldawy, M., L´azaro-Gredilla, M., Lou, X., Dorfman, N., Sidor,
S., Phoenix, S., and George, D.
(2017).
Schema networks: Zero-shot transfer with a generative
causal model of intuitive physics.
In Proceedings of the International Conference on Machine
Learning (ICML).
Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P.
(2016).
Molecular graph
convolutions: moving beyond ﬁngerprints.
Journal of computer-aided molecular design, 30(8):595–
608.
Kemp, C.
and Tenenbaum, J.
B.
(2008).
The discovery of structural form.
Proceedings of the
National Academy of Sciences, 105(31):10687–10692.
Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R.
(2018).
Neural relational inference
for interacting systems.
In Proceedings of the International Conference on Machine Learning
(ICML).
Kipf, T.
N.
and Welling, M.
(2017).
Semi-supervised classiﬁcation with graph convolutional networks.
In Proceedings of the International Conference on Learning Representations (ICLR).
Koller, D.
and Friedman, N.
(2009).
Probabilistic Graphical Models: Principles and Techniques.
MIT press.
Kondor, R., Son, H.
T., Pan, H., Anderson, B., and Trivedi, S.
(2018).
Covariant compositional
networks for learning graphs.
arXiv preprint arXiv:1801.02144.
Kondor, R.
and Trivedi, S.
(2018).
On the generalization of equivariance and convolution in neural
networks to the action of compact groups.
arXiv preprint arXiv:1802.03690.
Konidaris, G., Kaelbling, L.
P., and Lozano-Perez, T.
(2018).
From skills to symbols: Learning
symbolic representations for abstract high-level planning.
Journal of Artiﬁcial Intelligence
Research, 61:215–289.
Kool, W.
and Welling, M.
(2018).
Attention solves your TSP.
arXiv preprint arXiv:1803.08475.
30
Krizhevsky, A., Sutskever, I., and Hinton, G.
E.
(2012).
ImageNet classiﬁcation with deep convolu-
tional neural networks.
In Advances in Neural Information Processing Systems, pages 1097–1105.
Kurach, K., Andrychowicz, M., and Sutskever, I.
(2016).
Neural random-access machines.
In
Proceedings of the International Conference on Learning Representations (ICLR).
Lake, B.
M.
and Baroni, M.
(2018).
Still not systematic after all these years: On the compositional
skills of sequence-to-sequence recurrent networks.
In Proceedings of the International Conference
on Machine Learning (ICML).
Lake, B.
M., Salakhutdinov, R., and Tenenbaum, J.
B.
(2015).
Human-level concept learning
through probabilistic program induction.
Science, 350(6266):1332–1338.
Lake, B.
M., Ullman, T.
D., Tenenbaum, J.
B., and Gershman, S.
J.
(2017).
Building machines that
learn and think like people.
Behavioral and Brain Sciences, 40.
LeCun, Y., Bengio, Y., and Hinton, G.
(2015).
Deep learning.
Nature, 521(7553):436.
LeCun, Y., Boser, B., Denker, J.
S., Henderson, D., Howard, R.
E., Hubbard, W., and Jackel,
L.
D.
(1989).
Backpropagation applied to handwritten zip code recognition.
Neural computation,
1(4):541–551.
Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R.
(2016).
Gated graph sequence neural networks.
In Proceedings of the International Conference on Learning Representations (ICLR).
Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P.
(2018).
Learning deep generative
models of graphs.
In Workshops at the International Conference on Learning Representations
(ICLR).
Li, Y., Yu, R., Shahabi, C., and Liu, Y.
(2017).
Diﬀusion convolutional recurrent neural network:
Data-driven traﬃc forecasting.
arXiv preprint arXiv:1707.01926.
Lin, Z., Feng, M., Santos, C.
N.
d., Yu, M., Xiang, B., Zhou, B., and Bengio, Y.
(2017).
A structured
self-attentive sentence embedding.
In Proceedings of the International Conference on Learning
Representations (ICLR).
Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K.
(2018).
Hierarchical
representations for eﬃcient architecture search.
In Proceedings of the International Conference on
Learning Representations (ICLR).
Luong, M.-T., Pham, H., and Manning, C.
D.
(2015).
Eﬀective approaches to attention-based neural
machine translation.
arXiv preprint arXiv:1508.04025.
Marcus, G.
(2001).
The algebraic mind.
Marcus, G.
(2018a).
Deep learning: A critical appraisal.
arXiv preprint arXiv:1801.00631.
Marcus,
G.
(2018b).
Innateness,
alphazero,
and artiﬁcial intelligence.
arXiv preprint
arXiv:1801.05667.
McClelland, J.
L.
(1994).
The interaction of nature and nurture in development: A parallel
distributed processing perspective.
International perspectives on psychological science, 1:57–88.
31
McClelland, J.
L.
and Rumelhart, D.
E.
(1981).
An interactive activation model of context eﬀects
in letter perception: I.
an account of basic ﬁndings.
Psychological Review, 88(5):375.
Mikolov, T., Yih, W.-t., and Zweig, G.
(2013).
Linguistic regularities in continuous space word
representations.
In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 746–751.
Mitchell, T.
M.
(1980).
The need for biases in learning generalizations.
Department of Computer
Science, Laboratory for Computer Science Research, Rutgers Univ.
New Jersey.
Mnih, V., Heess, N., Graves, A., et al.
(2014).
Recurrent models of visual attention.
In Advances in
neural information processing systems, pages 2204–2212.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.
A., Veness, J., Bellemare, M.
G., Graves, A.,
Riedmiller, M., Fidjeland, A.
K., Ostrovski, G., et al.
(2015).
Human-level control through deep
reinforcement learning.
Nature, 518(7540):529.
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M.
M.
(2017).
Geometric
deep learning on graphs and manifolds using mixture model cnns.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR).
Moravˇc´ık, M., Schmid, M., Burch, N., Lis`y, V., Morrill, D., Bard, N., Davis, T., Waugh, K.,
Johanson, M., and Bowling, M.
(2017).
Deepstack: Expert-level artiﬁcial intelligence in heads-up
no-limit poker.
Science, 356(6337):508–513.
Narayanan, A., Chandramohan, M., Chen, L., Liu, Y., and Saminathan, S.
(2016).
subgraph2vec:
Learning distributed representations of rooted sub-graphs from large graphs.
In Workshops at the
20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y., and Jaiswal, S.
(2017).
graph2vec: Learning distributed representations of graphs.
arXiv preprint arXiv:1707.05005.
Navon, D.
(1977).
Forest before trees: The precedence of global features in visual perception.
Cognitive Psychology, 9(3):353–383.
Niepert, M., Ahmed, M., and Kutzkov, K.
(2016).
Learning convolutional neural networks for
graphs.
In Proceedings of the International Conference on Machine Learning (ICML), pages
2014–2023.
Nilsson, N.
J.
and Fikes, R.
E.
(1970).
Strips: A new approach to the application of theorem proving
to problem solving.
Technical report, SRI International, Menlo Park, CA Artiﬁcial Intelligence
Center.
Nowak, A., Villar, S., Bandeira, A.
S., and Bruna, J.
(2017).
A note on learning algorithms for
quadratic assignment with graph neural networks.
In Proceedings of the Principled Approaches to
Deep Learning Workshop (PADL) at the International Conference of Machine Learning (ICML).
Nowak, M.
A.
(2006).
Five rules for the evolution of cooperation.
science, 314(5805):1560–1563.
Ohtsuki, H., Hauert, C., Lieberman, E., and Nowak, M.
A.
(2006).
A simple rule for the evolution
of cooperation on graphs and social networks.
Nature, 441(7092):502.
32
O˜noro-Rubio, D., Niepert, M., Garc´ıa-Dur´an, A., Gonz´alez-S´anchez, R., and L´opez-Sastre,
R.
J.
(2017).
Representation learning for visual-relational knowledge graphs.
arXiv preprint
arXiv:1709.02314.
Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P.
(2017).
Neuro-symbolic
program synthesis.
In Proceedings of the International Conference on Learning Representations
(ICLR).
Pascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., Racani`ere, S., Reichert, D., Weber, T.,
Wierstra, D., and Battaglia, P.
(2017).
Learning model-based planning from scratch.
arXiv
preprint arXiv:1707.06170.
Pearl, J.
(1986).
Fusion, propagation, and structuring in belief networks.
Artiﬁcial intelligence,
29(3):241–288.
Pearl, J.
(1988).
Probabilistic reasoning in intelligent systems: Networks of plausible inference.
Morgan Kaufmann.
Pearl, J.
(2009).
Causality: Models, Reasoning and Inference.
Cambridge University Press, New
York, NY, USA, 2nd edition.
Pearl, J.
(2018).
Theoretical impediments to machine learning with seven sparks from the causal
revolution.
arXiv preprint arXiv:1801.04016.
Pennington, J., Socher, R., and Manning, C.
(2014).
Glove: Global vectors for word representation.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 1532–1543.
Perozzi, B., Al-Rfou, R., and Skiena, S.
(2014).
Deepwalk: Online learning of social representations.
In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 701–710.
ACM.
Pevn`y, T.
and Somol, P.
(2017).
Using neural network formalism to solve multiple-instance problems.
In International Symposium on Neural Networks, pages 135–142.
Springer.
Pinker, S.
and Prince, A.
(1988).
On language and connectionism: Analysis of a parallel distributed
processing model of language acquisition.
Cognition, 28(1-2):73–193.
Plate, T.
A.
(1995).
Holographic reduced representations.
IEEE Transactions on Neural Networks,
6(3):623–641.
Plaut, D.
C., McClelland, J.
L., Seidenberg, M.
S., and Patterson, K.
(1996).
Understanding normal
and impaired word reading: computational principles in quasi-regular domains.
Psychological
Review, 103(1):56.
Pollack, J.
B.
(1990).
Recursive distributed representations.
Artiﬁcial Intelligence, 46(1-2):77–105.
Qi, C.
R., Su, H., Mo, K., and Guibas, L.
J.
(2017).
Pointnet: Deep learning on point sets for 3d
classiﬁcation and segmentation.
In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).
Raposo, D., Santoro, A., Barrett, D., Pascanu, R., Lillicrap, T., and Battaglia, P.
(2017).
Discovering
objects and their relations from entangled scene representations.
In Workshops at the International
Conference on Learning Representations (ICLR).
33
Reed, S.
and De Freitas, N.
(2016).
Neural programmer-interpreters.
In Proceedings of the
International Conference on Learning Representations (ICLR).
Ritchie, D., Horsfall, P., and Goodman, N.
D.
(2016).
Deep amortized inference for probabilistic
programs.
arXiv preprint arXiv:1610.05735.
Rosenblatt, F.
(1961).
Principles of neurodynamics.
perceptrons and the theory of brain mechanisms.
Technical report, Cornell Aeronautical Lab Inc., Buﬀalo, NY.
Rumelhart, D.
E., McClelland, J.
L., Group, P.
R., et al.
(1987).
Parallel Distributed Processing,
volume 1.
MIT Press.
Russell, S.
J.
and Norvig, P.
(2009).
Artiﬁcial Intelligence: A Modern Approach (3rd Edition).
Pearson.
Sabour, S., Frosst, N., and Hinton, G.
E.
(2017).
Dynamic routing between capsules.
In Advances
in Neural Information Processing Systems, pages 3859–3869.
Sanchez-Gonzalez, A., Heess, N., Springenberg, J.
T., Merel, J., Riedmiller, M., Hadsell, R., and
Battaglia, P.
(2018).
Graph networks as learnable physics engines for inference and control.
In
Proceedings of the 35th International Conference on Machine Learning (ICLR).
Santoro, A., Raposo, D., Barrett, D.
G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap,
T.
(2017).
A simple neural network module for relational reasoning.
In Advances in Neural
Information Processing Systems.
Scarselli, F., Gori, M., Tsoi, A.
C., Hagenbuchner, M., and Monfardini, G.
(2009a).
Computational
capabilities of graph neural networks.
IEEE Transactions on Neural Networks, 20(1):81–102.
Scarselli, F., Gori, M., Tsoi, A.
C., Hagenbuchner, M., and Monfardini, G.
(2009b).
The graph
neural network model.
IEEE Transactions on Neural Networks, 20(1):61–80.
Scarselli, F., Yong, S.
L., Gori, M., Hagenbuchner, M., Tsoi, A.
C., and Maggini, M.
(2005).
Graph
neural networks for ranking web pages.
In Proceedings of the 2005 IEEE/WIC/ACM International
Conference on Web Intelligence, pages 666–672.
IEEE.
Schmidhuber, J.
(2015).
Deep learning in neural networks: An overview.
Neural Networks, 61:85–117.
Selsam, D., Lamm, M., Bunz, B., Liang, P., de Moura, L., and Dill, D.
L.
(2018).
Learning a sat
solver from single-bit supervision.
arXiv preprint arXiv:1802.03685.
Shalev-Shwartz, S., Shamir, O., and Shammah, S.
(2017).
Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950.
Shaw, P., Uszkoreit, J., and Vaswani, A.
(2018).
Self-attention with relative position representations.
In Proceedings of the 16th Annual Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies.
Shervashidze, N., Schweitzer, P., Leeuwen, E.
J.
v., Mehlhorn, K., and Borgwardt, K.
M.
(2011).
Weisfeiler-lehman graph kernels.
Journal of Machine Learning Research, 12(Sep):2539–2561.
Silver, D., Huang, A., Maddison, C.
J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.
(2016).
Mastering the game of go with
deep neural networks and tree search.
Nature, 529(7587):484–489.
34
Smolensky, P.
(1990).
Tensor product variable binding and the representation of symbolic structures
in connectionist systems.
Artiﬁcial Intelligence, 46(1-2):159–216.
Socher, R., Huval, B., Manning, C.
D., and Ng, A.
Y.
(2012).
Semantic compositionality through
recursive matrix-vector spaces.
In Proceedings of the Joint Conference on Empirical Methods in
Natural Language Processing (EMNLP) and Computational Natural Language Learning (CNLL),
pages 1201–1211.
Association for Computational Linguistics.
Socher, R., Lin, C.
C., Manning, C., and Ng, A.
Y.
(2011a).
Parsing natural scenes and natural
language with recursive neural networks.
In Proceedings of the 28th International Conference on
Machine Learning (ICML), pages 129–136.
Socher, R., Pennington, J., Huang, E.
H., Ng, A.
Y., and Manning, C.
D.
(2011b).
Semi-supervised
recursive autoencoders for predicting sentiment distributions.
In Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 151–161.
Association for
Computational Linguistics.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.
D., Ng, A., and Potts, C.
(2013).
Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of
the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.
Spelke, E.
S., Breinlinger, K., Macomber, J., and Jacobson, K.
(1992).
Origins of knowledge.
Psychological review, 99(4):605.
Spelke, E.
S.
and Kinzler, K.
D.
(2007).
Core knowledge.
Developmental Science, 10(1):89–96.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.
(2014).
Dropout:
A simple way to prevent neural networks from overﬁtting.
The Journal of Machine Learning
Research, 15(1):1929–1958.
Sukhbaatar, S., Fergus, R., et al.
(2016).
Learning multiagent communication with backpropagation.
In Advances in Neural Information Processing Systems, pages 2244–2252.
Sukhbaatar, S., Weston, J., Fergus, R., et al.
(2015).
End-to-end memory networks.
In Advances in
Neural Information Processing Systems, pages 2440–2448.
Sutskever, I., Vinyals, O., and Le, Q.
V.
(2014).
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems, pages 3104–3112.
Szegedy, C., Ioﬀe, S., Vanhoucke, V., and Alemi, A.
A.
(2017).
Inception-v4, inception-resnet
and the impact of residual connections on learning.
In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence (AAAI), volume 4, page 12.
Tai, K.
S., Socher, R., and Manning, C.
D.
(2015).
Improved semantic representations from
tree-structured long short-term memory networks.
In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., and Mei, Q.
(2015).
Line: Large-scale information
network embedding.
In Proceedings of the 24th International Conference on World Wide Web,
pages 1067–1077.
International World Wide Web Conferences Steering Committee.
Tenenbaum, J.
B., Griﬃths, T.
L., and Kemp, C.
(2006).
Theory-based bayesian models of inductive
learning and reasoning.
Trends in Cognitive Sciences, 10(7):309–318.
35
Tenenbaum, J.
B., Kemp, C., Griﬃths, T.
L., and Goodman, N.
D.
(2011).
How to grow a mind:
Statistics, structure, and abstraction.
Science, 331(6022):1279–1285.
Toyer, S., Trevizan, F., Thiebaux, S., and Xie, L.
(2017).
Action schema networks: Generalised
policies with deep learning.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(AAAI).
Ullman, T.
D., Spelke, E., Battaglia, P., and Tenenbaum, J.
B.
(2017).
Mind games: Game engines
as an architecture for intuitive physics.
Trends in Cognitive Sciences, 21(9):649–665.
van Steenkiste, S., Chang, M., Greﬀ, K., and Schmidhuber, J.
(2018).
Relational neural expectation
maximization: Unsupervised discovery of objects and their interactions.
Proceedings of the
International Conference on Learning Representations (ICLR).
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
N., Kaiser, L., and
Polosukhin, I.
(2017).
Attention is all you need.
In Advances in Neural Information Processing
Systems.
Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y.
(2018).
Graph
attention networks.
In Proceedings of the International Conference on Learning Representations
(ICLR).
Wang, J.
X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J.
Z., Hassabis, D.,
and Botvinick, M.
(2018a).
Prefrontal cortex as a meta-reinforcement learning system.
Nature
neuroscience, page 1.
Wang, J.
X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.
Z., Munos, R., Blundell, C.,
Kumaran, D., and Botvinick, M.
(2016).
Learning to reinforcement learn.
arXiv preprint
arXiv:1611.05763.
Wang, T., Liao, R., Ba, J., and Fidler, S.
(2018b).
Nervenet: Learning structured policy with graph
neural networks.
In Proceedings of the International Conference on Learning Representations
(ICLR).
Wang, X., Girshick, R., Gupta, A., and He, K.
(2018c).
Non-local neural networks.
In Proceedings
of the Conference on Computer Vision and Pattern Recognition (CVPR).
Wang, Y., Sun, Y., Liu, Z., Sarma, S.
E., Bronstein, M.
M., and Solomon, J.
M.
(2018d).
Dynamic
graph cnn for learning on point clouds.
arXiv preprint arXiv:1801.07829.
Watters, N., Zoran, D., Weber, T., Battaglia, P., Pascanu, R., and Tacchetti, A.
(2017).
Visual
interaction networks: Learning a physics simulator from video.
In Advances in Neural Information
Processing Systems, pages 4542–4550.
Wu, J., Lu, E., Kohli, P., Freeman, B., and Tenenbaum, J.
(2017).
Learning to see physics via
visual de-animation.
In Advances in Neural Information Processing Systems, pages 152–163.
Yoon, K., Liao, R., Xiong, Y., Zhang, L., Fetaya, E., Urtasun, R., Zemel, R., and Pitkow, X.
(2018).
Inference in probabilistic graphical models by graph neural networks.
In Workshops at
the International Conference on Learning Representations (ICLR).
You, J., Ying, R., Ren, X., Hamilton, W.
L., and Leskovec, J.
(2018).
GraphRNN: A deep generative
model for graphs.
arXiv preprint arXiv:1802.08773.
36
Yuille, A.
L.
and Liu, C.
(2018).
Deep nets: What have they ever done for vision?
arXiv preprint
arXiv:1805.04025.
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.
R., and Smola, A.
J.
(2017).
Deep sets.
In Advances in Neural Information Processing Systems, pages 3394–3404.
Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin, I., Tuyls, K., Reichert, D.,
Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V., Pascanu, R., Botvinick, M., Vinyals, O.,
and Battaglia, P.
(2018).
Relational deep reinforcement learning.
arXiv preprint arXiv.
Zhang, A., Lerer, A., Sukhbaatar, S., Fergus, R., and Szlam, A.
(2018).
Composable planning with
attributes.
arXiv preprint arXiv:1803.00512.
Z¨ugner, D., Akbarnejad, A., and G¨unnemann, S.
(2018).
Adversarial Attacks on Neural Networks
for Graph Data.
arXiv preprint arXiv:1805.07984.
37
Appendix: Formulations of additional models
In this appendix we give more examples of how published networks can ﬁt in the frame deﬁned by
Equation 1.
Interaction networks
Interaction Networks (Battaglia et al., 2016; Watters et al., 2017) and the Neural Physics Engine
Chang et al.
(2017) use a full GN but for the absence of the global to update the edge properties:
φe (ek, vrk, vsk, u) := fe (ek, vrk, vsk) = NNe ([ek, vrk, vsk])
φv  ¯e′
i, vi, u
 := fv  ¯e′
i, vi, u

= NNv
 [¯e′
i, vi, u]

ρe→v  E′
i
 :=
=
X
{k: rk=i}
e′
k
That work also included an extension to the above formulation which output global, rather than
per-node, predictions:
φe (ek, vrk, vsk, u) := fe (ek, vrk, vsk) = NNe ([ek, vrk, vsk])
φv  ¯e′
i, vi, u
 := fv  ¯e′
i, vi, u

= NNv
 [¯e′
i, vi, u]

φu  ¯e′, ¯v′, u
 := fu  ¯v′, u

= NNu
 [¯v′, u]

ρv→g  V ′ :=
=
X
i
v′
i
Non-pairwise interactions
Gated Graph Sequence Neural Networks (GGS-NN) (Li et al., 2016) use a slightly generalized
formulation where each edge has an attached type tk ∈{1, .., T}, and the updates are:
φe ((ek, tk) , vrk, vsk, u) := fe (ek, vsk) = NNe,tk (vsk)
φv  ¯e′
i, vi, u
 := fv  ¯e′
i, vi

= NNv
 [¯e′
i, vi]

ρe→v  E′
i
 :=
=
X
{k: rk=i}
e′
k
These updates are applied recurrently (the NNv is a GRU (Cho et al., 2014)), followed by a global
decoder which computes a weighted sum of embedded ﬁnal node states.
Here each NNe,tk is a neural
network with speciﬁc parameters.
CommNet (Sukhbaatar et al., 2016) (in the slightly more general form described by (Hoshen,
2017)) uses:
φe (ek, vrk, vsk, u) := fe (vsk)
= NNe (vsk)
φv  ¯e′
i, vi, u
 := fv  ¯e′
i, vi

= NNv
 [¯e′
i, NNv′ (vi)]

ρe→v  E′
i
 :=
=
1
|E′
i|
X
{k: rk=i}
e′
k
38
Attention-based approaches
The various attention-based approaches use a φe which is factored into a scalar pairwise-interaction
function which returns the unnormalized attention term, denoted αe (vrk, vsk) = a′
k, and a vector-
valued non-pairwise term, denoted βe (vsk) = b′
k,
φe (ek, vrk, vsk, u) := fe (vrk, vsk) = (αe (vrk, vsk) , βe (vsk)) = (a′
k, b′
k) = e′
k
The single-headed self-attention (SA) in the Transformer architecture (Vaswani et al., 2017),
implements the non-local formulation as:
αe (vrk, vsk) = exp (NNαquery (vrk)⊺· NNαkey (vsk))
βe (vsk)
= NNβ (vsk)
φv  ¯e′
i, vi, u
 := fv  ¯e′
i

= NNv
 ¯e′
i

where NNαquery, NNαkey, and NNβ are again neural network functions with diﬀerent parameters and
possibly diﬀerent architectures.
They also use a multi-headed version which computes Nh parallel
¯e′h
i using diﬀerent NNαquery
h
, NNαkey
h , NNβh, where h indexes the diﬀerent parameters.
These are
passed to fv and concatenated:
fv 
{¯e′h
i }h=1...Nh

= NNv

[¯e′1
i , .
.
.
,¯e′Nh
i
]

Vertex Attention Interaction Networks (Hoshen, 2017) are very similar to single-headed SA,
but use Euclidean distance for the attentional similarity metric, with shared parameters across the
attention inputs’ embeddings, and also use the input node feature in the node update function,
αe (vrk, vsk) = exp
 −∥NNα (vrk) −NNα (vsk) ∥2
βe (vsk)
= NNβ (vsk)
φv  ¯e′
i, vi, u
 := fv  ¯e′
i

= NNv
 [¯e′
i, vi]

Graph Attention Networks (Veliˇckovi´c et al., 2018) are also similar to multi-headed SA, but use
a neural network as the attentional similarity metric, with shared parameters across the attention
inputs’ embeddings:
αe (vrk, vsk)
= exp (NNα′ ([NNα (vrk) , NNα (vsk)))
βe (vsk)
= NNβ (vsk)
φv  ¯e′
i, vi, u
 := fv 
{¯e′h
i }h=1...Nh

= NNv

[¯e′1
i , .
.
.
,¯e′Nh
i
]

Stretching beyond the speciﬁc non-local formulation, Shaw et al.
(2018) extended multi-headed
SA with relative position encodings.
“Relative” refers to an encoding of the spatial distance between
nodes in a sequence or other signal in a metric space.
This can be expressed in GN language as an
edge attribute ek, and replacing the βe (vsk) from multi-headed SA above with:
βe (ek, vsk) = NNe (vsk) + ek
39
Belief Propagation embeddings
Finally, we brieﬂy summarize how the general “structure2vec” algorithm of Dai et al.
(2016) can ﬁt
into our framework.
In order to do so, we need to slightly modify our main Equation 1, i.e.:
¯ϵk = ρ

{el}sl=rk
rl̸=sk

:=
X
rl=sk
sl̸=rk
el
e′
k = φe (¯ϵk)
:= f(¯ϵk) = NN(¯ϵk)
¯e′
i = ρ
 {e′
k}rk=i

:=
X
{k: rk=i}
ek
v′
i = φv  ¯e′
i

:= f(¯e′
i) = NN(¯e′
i)
Edges’ features now takes the meaning of “message” between their receiver and sender; note that
there is only one set of parameters to learn for both the edges and nodes updates.
40
We carefully evaluate a number of design de-
cisions when pretraining BERT models.
We
ﬁnd that performance can be substantially im-
proved by training the model longer, with bigger
batches over more data; removing the next sen-
tence prediction objective; training on longer se-
quences; and dynamically changing the masking
pattern applied to the training data.
Our improved
pretraining procedure, which we call RoBERTa,
achieves state-of-the-art results on GLUE, RACE
and SQuAD, without multi-task ﬁnetuning for
GLUE or additional data for SQuAD.
These re-
sults illustrate the importance of these previ-
ously overlooked design decisions and suggest
that BERT’s pretraining objective remains com-
petitive with recently proposed alternatives.
We
additionally
use
a
novel
dataset,
CC-NEWS,
and
release
our
models
and
code
for
pretraining
and
ﬁnetuning
at:
https://github.com/pytorch/fairseq.
References
Eneko Agirre, Llu’is M‘arquez, and Richard Wicen-
towski, editors.
2007.
Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007).
Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke
Zettlemoyer, and Michael Auli.
2019.
Cloze-
driven pretraining of self-attention networks.
arXiv
preprint arXiv:1903.07785.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor.
2006.
The second PASCAL recognising
textual entailment challenge.
In Proceedings of the
second PASCAL challenges workshop on recognis-
ing textual entailment.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini.
2009.
The
ﬁfth PASCAL recognizing textual entailment chal-
lenge.
Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning.
2015.
A large anno-
tated corpus for learning natural language inference.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
William Chan, Nikita Kitaev, Kelvin Guu, Mitchell
Stern, and Jakob Uszkoreit.
2019.
KERMIT: Gener-
ative insertion-based modeling for sequences.
arXiv
preprint arXiv:1906.01604.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006.
The PASCAL recognising textual entailment
challenge.
In Machine learning challenges.
evalu-
ating predictive uncertainty, visual object classiﬁca-
tion, and recognising tectual entailment.
Andrew M Dai and Quoc V Le.
2015.
Semi-supervised
sequence learning.
In Advances in Neural Informa-
tion Processing Systems (NIPS).
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In North American Association for Com-
putational Linguistics (NAACL).
William B Dolan and Chris Brockett.
2005.
Auto-
matically constructing a corpus of sentential para-
phrases.
In Proceedings of the International Work-
shop on Paraphrasing.
Li Dong, Nan Yang, Wenhui Wang,
Furu Wei,
Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming
Zhou, and Hsiao-Wuen Hon.
2019.
Uniﬁed
language model pre-training for natural language
understanding and generation.
arXiv preprint
arXiv:1905.03197.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan.
2007.
The third PASCAL recog-
nizing textual entailment challenge.
In Proceedings
of the ACL-PASCAL workshop on textual entailment
and paraphrasing.
Aaron Gokaslan and Vanya Cohen.
2019.
Openweb-
text corpus.
http://web.archive.org/
save/http://Skylion007.github.io/
OpenWebTextCorpus.
Felix Hamborg, Norman Meuschke, Corinna Bre-
itinger, and Bela Gipp.
2017.
news-please:
A
generic news crawler and extractor.
In Proceedings
of the 15th International Symposium of Information
Science.
Dan Hendrycks and Kevin Gimpel.
2016.
Gaus-
sian error linear units (gelus).
arXiv preprint
arXiv:1606.08415.
Matthew Honnibal and Ines Montani.
2017.
spaCy 2:
Natural language understanding with Bloom embed-
dings, convolutional neural networks and incremen-
tal parsing.
To appear.
Jeremy Howard and Sebastian Ruder.
2018.
Universal
language model ﬁne-tuning for text classiﬁcation.
arXiv preprint arXiv:1801.06146.
Shankar Iyer, Nikhil Dandekar, and Kornl Cser-
nai.
2016.
First quora dataset release: Question
pairs.
https://data.quora.com/First-
Quora-Dataset-Release-Question-
Pairs.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.
Weld, Luke Zettlemoyer, and Omer Levy.
2019.
SpanBERT:
Improving
pre-training
by
repre-
senting and predicting spans.
arXiv preprint
arXiv:1907.10529.
Diederik Kingma and Jimmy Ba.
2015.
Adam: A
method for stochastic optimization.
In International
Conference on Learning Representations (ICLR).
Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,
Yordan Yordanov, and Thomas Lukasiewicz.
2019.
A surprisingly robust trick for winograd schema
challenge.
arXiv preprint arXiv:1905.06290.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy.
2017.
Race: Large-scale reading
comprehension dataset from examinations.
arXiv
preprint arXiv:1704.04683.
Guillaume Lample and Alexis Conneau.
2019.
Cross-
lingual language model pretraining.
arXiv preprint
arXiv:1901.07291.
Hector J Levesque, Ernest Davis, and Leora Morgen-
stern.
2011.
The Winograd schema challenge.
In
AAAI Spring Symposium: Logical Formalizations of
Commonsense Reasoning.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and
Jianfeng Gao.
2019a.
Improving multi-task deep
neural networks via knowledge distillation for
natural language understanding.
arXiv preprint
arXiv:1904.09482.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-
feng Gao.
2019b.
Multi-task deep neural networks
for natural language understanding.
arXiv preprint
arXiv:1901.11504.
Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher.
2017.
Learned in translation: Con-
textualized word vectors.
In Advances in Neural In-
formation Processing Systems (NIPS), pages 6297–
6308.
Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg,
Michael Houston,
Oleksii Kuchaiev,
Ganesh Venkatesh, and Hao Wu.
2018.
Mixed preci-
sion training.
In International Conference on Learn-
ing Representations.
Sebastian
Nagel.
2016.
Cc-news.
http:
//web.archive.org/save/http:
//commoncrawl.org/2016/10/news-
dataset-available.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli.
2019.
FAIRSEQ:
A fast, exten-
sible toolkit for sequence modeling.
In North
American Association for Computational Linguis-
tics (NAACL): System Demonstrations.
Myle Ott,
Sergey Edunov, David Grangier, and
Michael Auli.
2018.
Scaling neural machine trans-
lation.
In Proceedings of the Third Conference on
Machine Translation (WMT).
Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer.
2017.
Automatic differentiation in PyTorch.
In NIPS Autodiff Workshop.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer.
2018.
Deep contextualized word repre-
sentations.
In North American Association for Com-
putational Linguistics (NAACL).
Alec Radford, Karthik Narasimhan, Time Salimans,
and Ilya Sutskever.
2018.
Improving language un-
derstanding with unsupervised learning.
Technical
report, OpenAI.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever.
2019.
Language
models are unsupervised multitask learners.
Techni-
cal report, OpenAI.
Pranav Rajpurkar, Robin Jia, and Percy Liang.
2018.
Know what you don’t know: Unanswerable ques-
tions for squad.
In Association for Computational
Linguistics (ACL).
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang.
2016.
SQuAD: 100,000+ questions for
machine comprehension of text.
In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016.
Neural machine translation of rare words with
subword units.
In Association for Computational
Linguistics (ACL), pages 1715–1725.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts.
2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Empirical Methods in Natural Language
Processing (EMNLP).
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and
Tie-Yan Liu.
2019.
MASS: Masked sequence
to sequence pre-training for language generation.
In International Conference on Machine Learning
(ICML).
Yu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun
Feng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxi-
ang Zhu, Hao Tian, and Hua Wu.
2019.
ERNIE: En-
hanced representation through knowledge integra-
tion.
arXiv preprint arXiv:1904.09223.
Trieu H Trinh and Quoc V Le.
2018.
A simple
method for commonsense reasoning.
arXiv preprint
arXiv:1806.02847.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin.
2017.
Attention is all
you need.
In Advances in neural information pro-
cessing systems.
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R.
Bowman.
2019a.
SuperGLUE:
A stickier benchmark for general-purpose language
understanding systems.
arXiv preprint 1905.00537.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R.
Bowman.
2019b.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding.
In Inter-
national Conference on Learning Representations
(ICLR).
Alex Warstadt, Amanpreet Singh, and Samuel R.
Bow-
man.
2018.
Neural network acceptability judg-
ments.
arXiv preprint 1805.12471.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018.
A broad-coverage challenge corpus for sen-
tence understanding through inference.
In North
American Association for Computational Linguis-
tics (NAACL).
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019.
Xlnet: Generalized autoregressive pretrain-
ing for language understanding.
arXiv preprint
arXiv:1906.08237.
Yang You, Jing Li, Jonathan Hseu, Xiaodan Song,
James Demmel, and Cho-Jui Hsieh.
2019.
Reduc-
ing bert pre-training time from 3 days to 76 minutes.
arXiv preprint arXiv:1904.00962.
Rowan Zellers,
Ari Holtzman,
Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi.
2019.
Defending against neural fake
news.
arXiv preprint arXiv:1905.12616.
Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler.
2015.
Aligning books and movies:
Towards story-like visual explanations by watch-
ing movies and reading books.
In arXiv preprint
arXiv:1506.06724.
Appendix for “RoBERTa: A Robustly
Optimized BERT Pretraining Approach”
A
Full results on GLUE
In Table 8 we present the full set of development
set results for RoBERTa.
We present results for
a LARGE conﬁguration that follows BERTLARGE,
as well as a BASE conﬁguration that follows
BERTBASE.
B
Pretraining Hyperparameters
Table 9 describes the hyperparameters for pre-
training of RoBERTaLARGE and RoBERTaBASE
C
Finetuning Hyperparameters
Finetuning hyperparameters for RACE, SQuAD
and GLUE are given in Table 10.
We select the
best hyperparameter values based on the median
of 5 random seeds for each task.
MNLI
QNLI
QQP
RTE
SST
MRPC
CoLA
STS
RoBERTaBASE
+ all data + 500k steps
87.6
92.8
91.9
78.7
94.8
90.2
63.6
91.2
RoBERTaLARGE
with BOOKS + WIKI
89.0
93.9
91.9
84.5
95.3
90.2
66.3
91.6
+ additional data (§3.2)
89.3
94.0
92.0
82.7
95.6
91.4
66.1
92.2
+ pretrain longer 300k
90.0
94.5
92.2
83.3
96.1
91.1
67.4
92.3
+ pretrain longer 500k
90.2
94.7
92.2
86.6
96.4
90.9
68.0
92.4
Table 8: Development set results on GLUE tasks for various conﬁgurations of RoBERTa.
Hyperparam
RoBERTaLARGE
RoBERTaBASE
Number of Layers
24
12
Hidden size
1024
768
FFN inner hidden size
4096
3072
Attention heads
16
12
Attention head size
64
64
Dropout
0.1
0.1
Attention Dropout
0.1
0.1
Warmup Steps
30k
24k
Peak Learning Rate
4e-4
6e-4
Batch Size
8k
8k
Weight Decay
0.01
0.01
Max Steps
500k
500k
Learning Rate Decay
Linear
Linear
Adam ǫ
1e-6
1e-6
Adam β1
0.9
0.9
Adam β2
0.98
0.98
Gradient Clipping
0.0
0.0
Table 9: Hyperparameters for pretraining RoBERTaLARGE and RoBERTaBASE.
Hyperparam
RACE
SQuAD
GLUE
Learning Rate
1e-5
1.5e-5
{1e-5, 2e-5, 3e-5}
Batch Size
16
48
{16, 32}
Weight Decay
0.1
0.01
0.1
Max Epochs
4
2
10
Learning Rate Decay
Linear
Linear
Linear
Warmup ratio
0.06
0.06
0.06
Table 10: Hyperparameters for ﬁnetuning RoBERTaLARGE on RACE, SQuAD and GLUE.
We present an investigation into the feasibility
of scoring singletons and pairs according to their
likelihoods of producing summary sentences.
Our
framework is founded on the human process of se-
lecting one or two sentences to merge together and
it has the potential to bridge the gap between com-
pression and fusion studies.
Our method provides
a promising avenue for domain-speciﬁc summa-
rization where content selection and summary
generation are only loosely connected to reduce
the costs of obtaining massive annotated data.
Acknowledgments
We are grateful to the reviewers for their insight-
ful comments that point to interesting future direc-
tions.
The authors also thank students in the UCF
NLP group for useful discussions.
References
Regina Barzilay and Kathleen R.
McKeown.
2005.
Sentence fusion for multidocument news summa-
rization.
Computational Linguistics, 31(3).
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011.
Jointly learning to extract and compress.
In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei
Guo, and Rebecca J.
Passonneau.
2015.
Abstractive
multi-document summarization via phrase selection
and merging.
In Proceedings of ACL.
Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.
2018a.
Retrieve, rerank and rewrite: Soft template
based neural summarization.
In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li.
2018b.
Faithful to the original: Fact aware neural
abstractive summarization.
In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (AAAI).
Jaime Carbonell and Jade Goldstein.
1998.
The use
of MMR, diversity-based reranking for reordering
documents and producing summaries.
In Proceed-
ings of the International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR).
Giuseppe Carenini, Raymond Ng, and Adam Pauls.
2006.
Multi-document summarization of evaluative
text.
In Proceedings of 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi.
2018.
Deep communicating agents
for abstractive summarization.
In Proceedings of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
Danqi Chen, Jason Bolton, and Christopher D.
Man-
ning.
2016a.
A thorough examination of the
cnn/daily mail reading comprehension task.
In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang.
2016b.
Distraction-based neural net-
works for document summarization.
In Proceedings
of the Twenty-Fifth International Joint Conference
on Artiﬁcial Intelligence (IJCAI).
Yen-Chun Chen and Mohit Bansal.
2018.
Fast ab-
stractive summarization with reinforce-selected sen-
tence rewriting.
In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Victor Chenal and Jackie Chi Kit Cheung.
2016.
Pre-
dicting sentential semantic compatibility for aggre-
gation in text-to-text generation.
In Proceedings of
the International Conference on Computational Lin-
guistics (COLING).
Jianpeng Cheng and Mirella Lapata.
2016.
Neural
summarization by extracting sentences and words.
In Proceedings of ACL.
Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, and
Fei Liu.
2019.
Improving the similarity measure of
determinantal point processes for extractive multi-
document summarization.
In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Hal Daum´e III and Daniel Marcu.
2002.
A noisy-
channel model for document compression.
In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Hal Daum´e III and Daniel Marcu.
2004.
Generic sen-
tence fusion is an ill-deﬁned summarization task.
In
Proceedings of ACL Workshop on Text Summariza-
tion Branches Out.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
2018.
BERT: pre-training of
deep bidirectional transformers for language under-
standing.
arXiv:1810.04805.
Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016.
Learning-based single-document summariza-
tion with compression and anaphoricity constraints.
In Proceedings of the Association for Computational
Linguistics (ACL).
G¨unes Erkan and Dragomir R.
Radev.
2004.
LexRank:
Graph-based lexical centrality as salience in text
summarization.
Journal of Artiﬁcial Intelligence
Research.
Katja Filippova.
2010.
Multi-sentence compression:
Finding shortest paths in word graphs.
In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING).
Katja Filippova,
Enrique Alfonseca,
Carlos Col-
menares, Lukasz Kaiser, and Oriol Vinyals.
2015.
Sentence compression by deletion with lstms.
In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Dimitrios Galanis, Gerasimos Lampouras, and Ion An-
droutsopoulos.
2012.
Extractive multi-document
summarization with integer linear programming and
support vector regression.
In Proceedings of the In-
ternational Conference on Computational Linguis-
tics (COLING).
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010.
Opinosis: A graph-based approach to ab-
stractive summarization of highly redundant opin-
ions.
In Proceedings of the International Confer-
ence on Computational Linguistics (COLING).
Sebastian Gehrmann, Yuntian Deng, and Alexander M.
Rush.
2018.
Bottom-up abstractive summariza-
tion.
In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Dan Gillick and Benoit Favre.
2009.
A scalable global
model for summarization.
In Proceedings of the
NAACL Workshop on Integer Linear Programming
for Natural Langauge Processing.
Han Guo, Ramakanth Pasunuru, and Mohit Bansal.
2018.
Soft, layer-speciﬁc multi-task summarization
with entailment and question generation.
In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Aria Haghighi and Lucy Vanderwende.
2009.
Explor-
ing content models for multi-document summariza-
tion.
In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Karl Moritz Hermann,
Tomas Kocisky,
Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom.
2015.
Teaching ma-
chines to read and comprehend.
In Proceedings of
Neural Information Processing Systems (NIPS).
Kai Hong and Ani Nenkova.
2014.
Improving the
estimation of word importance for news multi-
document summarization.
In Proceedings of the
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL).
Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun.
2018.
A uniﬁed
model for extractive and abstractive summarization
using inconsistency loss.
In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Chris Kedzie, Kathleen McKeown, and Hal Daume III.
2018.
Content selection in deep learning models of
summarization.
In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Alex Kulesza and Ben Taskar.
2011.
Learning de-
terminantal point processes.
In Proceedings of the
Conference on Uncertainty in Artiﬁcial Intelligence
(UAI).
Logan Lebanoff, Kaiqiang Song, and Fei Liu.
2018.
Adapting the neural encoder-decoder framework
from single to multi-document summarization.
In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013.
Document summarization via guided sentence com-
pression.
In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang
Weng.
2014.
Improving multi-document summa-
rization by sentence compression based on expanded
constituent parse tree.
In Proceedings of the Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP).
Piji Li, Wai Lam, Lidong Bing, and Zihao Wang.
2017.
Deep recurrent generative decoder for abstractive
text summarization.
In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
Kexin Liao, Logan Lebanoff, and Fei Liu.
2018.
Ab-
stract meaning representation for multi-document
summarization.
In Proceedings of the International
Conference on Computational Linguistics (COL-
ING).
Chin-Yew Lin.
2004.
ROUGE: a package for au-
tomatic evaluation of summaries.
In Proceedings
of ACL Workshop on Text Summarization Branches
Out.
Christopher D.
Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze.
2008.
Introduction to Information
Retrieval.
Cambridge University Press.
Andre F.
T.
Martins and Noah A.
Smith.
2009.
Sum-
marization with a joint model for sentence extrac-
tion and compression.
In Proceedings of the ACL
Workshop on Integer Linear Programming for Natu-
ral Language Processing.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang.
2016.
Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond.
In Proceedings of SIGNLL.
Shashi Narayan, Shay B.
Cohen, and Mirella Lapata.
2018.
Don’t give me the details, just the summary!
Topic-aware convolutional neural networks for ex-
treme summarization.
In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
Ani Nenkova and Kathleen McKeown.
2011.
Auto-
matic summarization.
Foundations and Trends in
Information Retrieval.
Paul Over and James Yen.
2004.
An introduction to
DUC-2004.
National Institute of Standards and
Technology.
Romain Paulus, Caiming Xiong, and Richard Socher.
2017.
A deep reinforced model for abstractive sum-
marization.
In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Alexander M.
Rush, Sumit Chopra, and Jason Weston.
2015.
A neural attention model for sentence sum-
marization.
In Proceedings of EMNLP.
Abigail See, Peter J.
Liu, and Christopher D.
Manning.
2017.
Get to the point: Summarization with pointer-
generator networks.
In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Chao Shen and Tao Li.
2010.
Multi-document summa-
rization via the minimum dominating set.
In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING).
Kaiqiang Song,
Lin Zhao,
and Fei Liu.
2018.
Structure-infused copy mechanisms for abstractive
summarization.
In Proceedings of the International
Conference on Computational Linguistics (COL-
ING).
Jiwei Tan, Xiaojun Wan, and Jianguo Xiao.
2017.
Abstractive document summarization with a graph-
based attentional neural model.
In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Kapil Thadani and Kathleen McKeown.
2013.
Super-
vised sentence fusion with single-stage inference.
In
Proceedings of the International Joint Conference
on Natural Language Processing (IJCNLP).
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova.
2007.
Beyond SumBasic: Task-
focused summarization with sentence simpliﬁcation
and lexical expansion.
Information Processing and
Management, 43(6):1606–1618.
Ashish
Vaswani,
Noam
Shazeer,
Niki
Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez,
Lukasz
Kaiser,
and
Illia
Polosukhin.
2017.
https://arxiv.org/abs/1706.03762.
In
Proceed-
ings of the 31st Conference on Neural Information
Processing Systems (NIPS).
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie.
2013.
A sentence com-
pression based framework to query-focused multi-
document summarization.
In Proceedings of ACL.
Kam-Fai Wong, Mingli Wu, and Wenjie Li.
2008.
Ex-
tractive summarization using supervised and semi-
supervised learning.
In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
David Zajic, Bonnie J.
Dorr, Jimmy Lin, and Richard
Schwartz.
2007.
Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks.
Information Processing and Manage-
ment.
Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017.
Selective encoding for abstractive sentence
summarization.
In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
A
Ground-truth Sets of Instances
We performed a manual inspection over a subset
of our ground-truth sets of singletons and pairs.
Each sentence from a human-written summary is
matched with one or two source sentences based
on average ROUGE similarity (details in Section
4 of the paper).
Tables 4, 5, and 6 present ran-
domly selected examples from CNN/Daily Mail,
XSum, and DUC-04, respectively.
Colored text
represents overlapping tokens between sentences.
Darker colors represent content from primary sen-
tences, while lighter colors represent content from
secondary sentences.
Best viewed in color.
B
Example Summaries
Table 7 presents example system summaries and
human-written abstracts from CNN/Daily Mail.
Each Human Abstract sentence is matched with
a sentence singleton or pair from the source doc-
ument; these singletons/pairs make up the GT-
SingPairMix summary.
Similarly, each sentence
from BERT-Abs is created by compressing a sin-
gleton or merging a pair selected by BERT-Extr.
Selected Source Sentence(s)
Human Summary Sentence
an
inmate housed on the “ forgotten ﬂoor , ” where many
mentally ill inmates are
housed in miami before trial .
mentally ill inmates in miami are housed on the “
forgotten ﬂoor ”
most often , they face drug charges or charges of assaulting an ofﬁcer – charges that
judge steven leifman says are usually “ avoidable felonies .
”
judge steven leifman says most are there as a result
of “ avoidable felonies ”
“ i am the son of the president .
miami , ﬂorida -lrb-
cnn -rrb- – the ninth ﬂoor of the miami-dade pretrial detention
facility is dubbed the “ forgotten ﬂoor .
”
while cnn tours facility , patient shouts :
“ i am
the son of the president ”
it ’s brutally unjust , in his mind , and he has become a strong advocate for changing
things in miami .
so , he
says , the sheer volume is overwhelming
the system , and the result is what
we see on the ninth ﬂoor .
leifman says the system is
unjust and he ’s ﬁght-
ing for change .
Selected Source Sentence(s)
Human Summary Sentence
the average surface temperature has warmed one degree fahrenheit -lrb- 0.6 degrees
celsius -rrb- during the last century , according to the national research council .
earth has warmed one degree in past 100 years .
the reason most cited –
by scientists and scientiﬁc organizations – for the current
warming trend is an increase in the concentrations
of greenhouse gases , which are
in the atmosphere naturally and help keep the planet ’s temperature at a comfortable
level .
in the worst-case scenario , experts
say oceans could
rise to overwhelming and
catastrophic levels , ﬂooding cities and altering seashores .
majority
of scientists
say
greenhouse gases are
causing temperatures
to rise .
a change in the earth ’s orbit or the intensity of the sun ’s radiation could change ,
triggering warming or cooling .
other scientists and observers , a minority compared to those who believe the warming
trend is something ominous , say it is simply the latest shift in the cyclical patterns of
a planet ’s life .
some critics say planets often in periods of warm-
ing or cooling .
Table 4: Sample of our ground-truth labels for singleton/pair instances from CNN/Daily Mail.
Large chunks of text are copied
straight out of the source sentences.
Selected Source Sentence(s)
Human Summary Sentence
the premises , used by east belfast mp naomi long , have been targeted a number of
times .
army explosives experts were called out to deal with a suspect package at the ofﬁces
on the newtownards road on friday night .
a suspicious package left outside an alliance party
ofﬁce in east belfast has been declared a hoax .
Selected Source Sentence(s)
Human Summary Sentence
nev edwards scored an early try for sale , before castres ’ ﬂorian vialelle went over
, but julien dumora ’s penalty put the hosts 10-7 ahead at the break .
a late penalty try gave sale victory over castres
at stade pierre-antoine in their european challenge
cup clash .
Selected Source Sentence(s)
Human Summary Sentence
speaking
in the dil , sinn fin leader gerry adams also
called for
a commission of
investigation and said his party had “ little conﬁdence the government is protecting
the public interest ” .
last year ,
nama sold its entire 850-property
loan portfolio in
northern ireland to
the new york investment ﬁrm cerberus for more than # 1bn .
the irish
government has rejected
calls to set
up
a commission of investigation into the sale of
nama ’s portfolio of loans in northern ireland .
Table 5: Sample of our ground-truth labels for singleton/pair instances from XSum.
Each article has only one summary
sentences, and thus only one singleton or pair matched with it.
Selected Source Sentence(s)
Human Summary Sentence
hun sen ’s
cambodian people ’s
party won 64
of the 122 parliamentary seats in
july ’s
elections , short
of the two-thirds
majority needed
to form a government
on its own .
cambodian elections , fraudulent according to op-
position parties , gave the cpp of hun sen a scant
majority but not enough
to form its own govern-
ment .
opposition leaders prince norodom ranariddh and sam rainsy , citing hun sen ’s threats
to arrest opposition ﬁgures after two alleged attempts on his life , said they could not
negotiate freely in cambodia and called for talks at sihanouk ’s residence in beijing .
cambodian leader hun sen has guaranteed the safety and political freedom of all politi-
cians , trying to ease
the fears of his rivals that they will be arrested or killed if they
return to the country .
opposition leaders
fearing
arrest , or worse , ﬂed
and asked for talks outside the country .
the cambodian people ’s party criticized a non-binding resolution passed earlier this
month
by the u.s.
house of representatives
calling for an investigation into
violations of international humanitarian law allegedly committed by hun sen .
the un found evidence of rights
violations by hun
sen prompting the us house to call for an inves-
tigation .
cambodian politicians expressed hope monday that a new partnership between the
parties of strongman hun sen and his rival , prince norodom ranariddh , in a coalition
government would not end in more violence .
the three-month governmental deadlock
ended
with han sen and his chief rival , prince norodom
ranariddh sharing power .
citing hun sen ’s threats to arrest opposition politicians following two alleged attempts
on his life , ranariddh and
sam rainsy have said they do not feel
safe negotiating
inside the country and asked the king to chair the summit at gis residence in beijing .
after a meeting between hun sen and the new french ambassador to
cambodia , hun
sen aide prak sokhonn said the cambodian leader had repeated calls for the opposition
to return , but expressed concern that the international community may be asked for
security guarantees .
han sen
guaranteed
safe
return to cambodia
for all opponents but his strongest
critic
, sam
rainsy ,
remained wary .
diplomatic efforts to revive the stalled talks appeared to bear fruit monday as japanese
foreign affairs secretary
of state nobutaka machimura said
king norodom sihanouk
has called on ranariddh and sam rainsy to return to cambodia .
king norodom sihanouk on tuesday praised agreements by cambodia ’s top two polit-
ical parties – previously bitter rivals – to form a coalition government led by strongman
hun sen .
chief of state king norodom sihanouk
praised the
agreement .
Table 6: Sample of our ground-truth labels for singleton/pair instances from DUC-04, a multi-document dataset.
Ground-truth
sentences are widely dispersed among all ten documents.
Extractive Upper Bound
• She’s a high school freshman with Down syndrome.
• Trey – a
star on Eastern High School’s basketball team in Louisville, Kentucky,
who’s headed to play college ball next year at Ball State – was origi-
nally going to take his girlfriend to Eastern’s prom.
• Trina Helson, a teacher at Eastern, alerted the school’s newspaper
staff to the prom-posal and posted photos of Trey and Ellie on Twitter
that have gone viral.
BERT-Extractive
• But all that changed Thursday when Trey asked Ellie to be his prom
date.
• Trey – a star on Eastern High School’s basketball team in
Louisville, Kentucky, who’s headed to play college ball next year at
Ball State – was originally going to take his girlfriend to Eastern’s
prom.
• Trina Helson, a teacher at Eastern, alerted the school’s newspaper
staff to the prom-posal and posted photos of Trey and Ellie on Twitter
that have gone viral.
• (CNN) He’s a blue chip college basketball recruit.
• She’s a high
school freshman with Down syndrome.
Human Abstract
• College-bound basketball star asks girl with Down syndrome to
high school prom.
• Pictures of the two during the ”prom-posal” have gone viral.
BERT-Abstractive
• Trey asked Ellie to be his prom date.
• Trina Helson, a teacher at Eastern, alerted the school’s newspaper
staff.
• He’s a high school student with Down syndrome.
Extractive Upper Bound
• Marseille prosecutor Brice Robin told CNN that ”so far no videos
were used in the crash investigation.”
• Reichelt told ”Erin Burnett: outfront” that he had watched the video
and stood by the report, saying Bild and Paris Match are ”very conﬁ-
dent” that the clip is real.
• Lubitz told his Lufthansa ﬂight training school in 2009 that he had a
”previous episode of severe depression,” the airline said Tuesday.
BERT-Extractive
• Marseille, France (CNN) - the French prosecutor leading an investi-
gation into the crash of Germanwings ﬂight 9525 insisted Wednesday
that he was not aware of any video footage from on board the plane.
•
Marseille prosecutor Brice Robin told CNN that ”so far no videos were
used in the crash investigation.”
• Robin’s comments follow claims by two magazines, German Daily
Bild and French Paris Match, of a cell phone video showing the har-
rowing ﬁnal seconds from on board Germanwings ﬂight 9525 as it
crashed into the French Alps.
• The two publications described the
supposed video, but did not post it on their websites.
Human Abstract
• Marseille prosecutor says ”so far no videos were used in the crash
investigation” despite media reports.
• Journalists at Bild and Paris Match are ”very conﬁdent” the video
clip is real, an editor says.
• Andreas Lubitz had informed his Lufthansa training school of an
episode of severe depression, airline says.
BERT-Abstractive
• New : French prosecutor says he was not aware of video footage
from on board the plane.
• Two magazines, including German Daily Bild, have been described
as the video.
Table 7: Example system summaries and human-written abstracts.
Each Human Abstract sentence is lined up horizontally
with its corresponding ground-truth instance, which is found in Extractive Upper Bound summary.
Similarly, each sentence
from BERT-Abstractive is lined up horizontally with its corresponding instance selected by BERT-Extractive.
The sentences
are manually de-tokenized for readability.
We presented SegNet, a deep convolutional network architecture
for semantic segmentation.
The main motivation behind SegNet
was the need to design an efﬁcient architecture for road and indoor
scene understanding which is efﬁcient both in terms of memory
and computational time.
We analysed SegNet and compared it
with other important variants to reveal the practical trade-offs
involved in designing architectures for segmentation, particularly
training time, memory versus accuracy.
Those architectures which
13
Wall
Floor
Cabinet
Bed
Chair
Sofa
Table
Door
Window Bookshelf Picture Counter Blinds
83.42
93.43
63.37
73.18
75.92
59.57
64.18
52.50
57.51
42.05
56.17
37.66
40.29
Desk
Shelves
Curtain
Dresser
Pillow
Mirror
Floor mat Clothes
Ceiling
Books
Fridge
TV
Paper
11.92
11.45
66.56
52.73
43.80
26.30
0.00
34.31
74.11
53.77
29.85
33.76
22.73
Towel Shower curtain
Box
Whiteboard Person Night stand
Toilet
Sink
Lamp
Bathtub
Bag
19.83
0.03
23.14
60.25
27.27
29.88
76.00
58.10
35.27
48.86
16.76
TABLE 5
Class average accuracies of SegNet predictions for the 37 indoor scene classes in the SUN RGB-D benchmark dataset.
The performance
correlates well with size of the classes in indoor scenes.
Note that class average accuracy has a strong correlation with mIoU metric.
Network
Forward pass(ms) Backward pass(ms) GPU training memory (MB) GPU inference memory (MB) Model size (MB)
SegNet
422.50
488.71
6803
1052
117
DeepLab-LargeFOV [3]
110.06
160.73
5618
1993
83
FCN (learnt deconv) [2]
317.09
484.11
9735
1806
539
DeconvNet [4]
474.65
602.15
9731
1872
877
TABLE 6
A comparison of computational time and hardware resources required for various deep architectures.
The caffe time command was used to
compute time requirement averaged over 10 iterations with mini batch size 1 and an image of 360 × 480 resolution We used nvidia-smi unix
command to compute memory consumption.
For training memory computation we used a mini-batch of size 4 and for inference memory the batch
size was 1.
Model size was the size of the caffe models on disk.
SegNet is most memory efﬁcient during inference model.
store the encoder network feature maps in full perform best but
consume more memory during inference time.
SegNet on the
other hand is more efﬁcient since it only stores the max-pooling
indices of the feature maps and uses them in its decoder network
to achieve good performance.
On large and well known datasets
SegNet performs competitively, achieving high scores for road
scene understanding.
End-to-end learning of deep segmentation
architectures is a harder challenge and we hope to see more
attention paid to this important problem.
REFERENCES
[1]
K.
Simonyan and A.
Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[2]
J.
Long, E.
Shelhamer, and T.
Darrell, “Fully convolutional networks for
semantic segmentation,” in CVPR, pp.
3431–3440, 2015.
[3]
C.
Liang-Chieh, G.
Papandreou, I.
Kokkinos, K.
Murphy, and A.
Yuille,
“Semantic image segmentation with deep convolutional nets and fully
connected crfs,” in ICLR, 2015.
[4]
H.
Noh, S.
Hong, and B.
Han, “Learning deconvolution network for
semantic segmentation,” in ICCV, pp.
1520–1528, 2015.
[5]
C.
Szegedy, W.
Liu, Y.
Jia, P.
Sermanet, S.
Reed, D.
Anguelov, D.
Erhan,
V.
Vanhoucke, and A.
Rabinovich, “Going deeper with convolutions,” in
CVPR, pp.
1–9, 2015.
[6]
K.
Simonyan and A.
Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol.
abs/1409.1556, 2014.
[7]
C.
Farabet, C.
Couprie, L.
Najman, and Y.
LeCun, “Learning hierarchical
features for scene labeling,” IEEE PAMI, vol.
35, no.
8, pp.
1915–1929,
2013.
[8]
N.
Hft, H.
Schulz, and S.
Behnke, “Fast semantic segmentation of rgb-d
scenes with gpu-accelerated deep neural networks,” in KI 2014: Advances
in Artiﬁcial Intelligence (C.
Lutz and M.
Thielscher, eds.), vol.
8736 of
Lecture Notes in Computer Science, pp.
80–85, Springer International
Publishing, 2014.
[9]
R.
Socher, C.
C.
Lin, C.
Manning, and A.
Y.
Ng, “Parsing natural scenes
and natural language with recursive neural networks,” in ICML, pp.
129–
136, 2011.
[10] S.
Zheng, S.
Jayasumana, B.
Romera-Paredes, V.
Vineet, Z.
Su, D.
Du,
C.
Huang, and P.
H.
Torr, “Conditional random ﬁelds as recurrent neural
networks,” in Proceedings of the IEEE International Conference on
Computer Vision, pp.
1529–1537, 2015.
[11] W.
Liu, A.
Rabinovich, and A.
C.
Berg, “Parsenet: Looking wider to see
better,” CoRR, vol.
abs/1506.04579, 2015.
[12] V.
Badrinarayanan, A.
Handa, and R.
Cipolla, “Segnet: A deep con-
volutional encoder-decoder architecture for robust semantic pixel-wise
labelling,” CoRR, vol.
abs/1505.07293, 2015.
[13] D.
Eigen and R.
Fergus, “Predicting depth, surface normals and semantic
labels with a common multi-scale convolutional architecture,” in ICCV,
pp.
2650–2658, 2015.
[14] G.
Papandreou, L.-C.
Chen, K.
Murphy, and A.
L.
Yuille, “Weakly-and
semi-supervised learning of a dcnn for semantic image segmentation,”
arXiv preprint arXiv:1502.02734, 2015.
[15] F.
Yu and V.
Koltun, “Multi-scale context aggregation by dilated convo-
lutions,” arXiv preprint arXiv:1511.07122, 2015.
[16] O.
Ronneberger, P.
Fischer, and T.
Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in MICCAI, pp.
234–241, Springer,
2015.
[17] L.
Bottou, “Large-scale machine learning with stochastic gradient de-
scent,” in Proceedings of COMPSTAT’2010, pp.
177–186, Springer,
2010.
[18] S.
Hong, H.
Noh, and B.
Han, “Decoupled deep neural network for semi-
supervised semantic segmentation,” in NIPS, pp.
1495–1503, 2015.
[19] M.
Ranzato, F.
J.
Huang, Y.
Boureau, and Y.
LeCun, “Unsupervised
learning of invariant feature hierarchies with applications to object
recognition,” in CVPR, 2007.
[20] R.
Mottaghi, X.
Chen, X.
Liu, N.-G.
Cho, S.-W.
Lee, S.
Fidler, R.
Ur-
tasun, et al., “The role of context for object detection and semantic
segmentation in the wild,” in Computer Vision and Pattern Recognition
(CVPR), 2014 IEEE Conference on, pp.
891–898, IEEE, 2014.
[21] M.
Everingham, S.
A.
Eslami, L.
Van Gool, C.
K.
Williams, J.
Winn,
and A.
Zisserman, “The pascal visual object classes challenge: A ret-
rospective,” International Journal of Computer Vision, vol.
111, no.
1,
pp.
98–136.
[22] G.
Brostow, J.
Fauqueur, and R.
Cipolla, “Semantic object classes in
video: A high-deﬁnition ground truth database,” PRL, vol.
30(2), pp.
88–
97, 2009.
[23] S.
Song, S.
P.
Lichtenberg, and J.
Xiao, “Sun rgb-d: A rgb-d scene
understanding benchmark suite,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp.
567–576, 2015.
[24] C.
L.
Zitnick and P.
Doll´ar, “Edge boxes: Locating object proposals from
edges,” in Computer Vision–ECCV 2014, pp.
391–405, Springer, 2014.
[25] N.
Silberman, D.
Hoiem, P.
Kohli, and R.
Fergus, “Indoor segmentation
and support inference from rgbd images,” in ECCV, pp.
746–760,
Springer, 2012.
[26] A.
Geiger, P.
Lenz, and R.
Urtasun, “Are we ready for autonomous
driving?
the KITTI vision benchmark suite,” in CVPR, pp.
3354–3361,
2012.
[27] J.
Shotton, M.
Johnson, and R.
Cipolla, “Semantic texton forests for
image categorization and segmentation,” in CVPR, 2008.
[28] G.
Brostow, J.
Shotton, J., and R.
Cipolla, “Segmentation and recognition
using structure from motion point clouds,” in ECCV, Marseille, 2008.
[29] P.
Sturgess, K.
Alahari, L.
Ladicky, and P.
H.S.Torr, “Combining appear-
ance and structure from motion features for road scene understanding,”
in BMVC, 2009.
[30] L.
Ladicky, P.
Sturgess, K.
Alahari, C.
Russell, and P.
H.
S.
Torr, “What,
where and how many?
combining object detectors and crfs,” in ECCV,
pp.
424–437, 2010.
[31] P.
Kontschieder, S.
R.
Bulo, H.
Bischof, and M.
Pelillo, “Structured
class-labels in random forests for semantic image labelling,” in ICCV,
pp.
2190–2197, IEEE, 2011.
14
[32] C.
Zhang, L.
Wang, and R.
Yang, “Semantic segmentation of urban
scenes using dense depth maps,” in ECCV, pp.
708–721, Springer, 2010.
[33] J.
Tighe and S.
Lazebnik, “Superparsing,” IJCV, vol.
101, no.
2, pp.
329–
349, 2013.
[34] X.
Ren, L.
Bo, and D.
Fox, “Rgb-(d) scene labeling: Features and
algorithms,” in CVPR, pp.
2759–2766, IEEE, 2012.
[35] A.
Hermans, G.
Floros, and B.
Leibe, “Dense 3D Semantic Mapping of
Indoor Scenes from RGB-D Images,” in ICRA, 2014.
[36] S.
Gupta, P.
Arbelaez, and J.
Malik, “Perceptual organization and
recognition of indoor scenes from rgb-d images,” in CVPR, pp.
564–571,
IEEE, 2013.
[37] C.
Farabet, C.
Couprie, L.
Najman, and Y.
LeCun, “Scene parsing with
multiscale feature learning, purity trees, and optimal covers,” in ICML,
2012.
[38] D.
Grangier, L.
Bottou, and R.
Collobert, “Deep convolutional networks
for scene parsing,” in ICML Workshop on Deep Learning, 2009.
[39] C.
Gatta, A.
Romero, and J.
van de Weijer, “Unrolling loopy top-down
semantic feedback in convolutional deep networks,” in CVPR Workshop
on Deep Vision, 2014.
[40] P.
Pinheiro and R.
Collobert, “Recurrent convolutional neural networks
for scene labeling,” in ICML, pp.
82–90, 2014.
[41] O.
Russakovsky, J.
Deng, H.
Su, J.
Krause, S.
Satheesh, S.
Ma, Z.
Huang,
A.
Karpathy, A.
Khosla, M.
Bernstein, A.
C.
Berg, and L.
Fei-Fei,
“ImageNet Large Scale Visual Recognition Challenge,” International
Journal of Computer Vision (IJCV), pp.
1–42, April 2015.
[42] T.-Y.
Lin, M.
Maire, S.
Belongie, J.
Hays, P.
Perona, D.
Ramanan,
P.
Doll´ar, and C.
L.
Zitnick, “Microsoft coco: Common objects in
context,” in Computer Vision–ECCV 2014, pp.
740–755, Springer, 2014.
[43] A.
G.
Schwing and R.
Urtasun, “Fully connected deep structured net-
works,” arXiv preprint arXiv:1503.02351, 2015.
[44] G.
Lin, C.
Shen, I.
Reid, et al., “Efﬁcient piecewise training of
deep structured models for semantic segmentation,” arXiv preprint
arXiv:1504.01013, 2015.
[45] B.
Hariharan, P.
Arbel´aez, R.
Girshick, and J.
Malik, “Hypercolumns for
object segmentation and ﬁne-grained localization,” in CVPR, pp.
447–
456, 2015.
[46] M.
Mostajabi, P.
Yadollahpour, and G.
Shakhnarovich, “Feedforward se-
mantic segmentation with zoom-out features,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp.
3376–3385,
2015.
[47] M.
D.
Zeiler, D.
Krishnan, G.
W.
Taylor, and R.
Fergus, “Deconvolutional
networks,” in CVPR, pp.
2528–2535, IEEE, 2010.
[48] K.
Kavukcuoglu, P.
Sermanet, Y.
Boureau, K.
Gregor, M.
Mathieu,
and Y.
LeCun, “Learning convolutional feature hierarchies for visual
recognition,” in NIPS, pp.
1090–1098, 2010.
[49] C.
Dong, C.
C.
Loy, K.
He, and X.
Tang, “Learning a deep convolutional
network for image super-resolution,” in ECCV, pp.
184–199, Springer,
2014.
[50] D.
Eigen, C.
Puhrsch, and R.
Fergus, “Depth map prediction from a single
image using a multi-scale deep network,” in NIPS, pp.
2366–2374, 2014.
[51] S.
Ioffe and C.
Szegedy, “Batch normalization: Accelerating deep
network
training
by
reducing
internal
covariate
shift,”
CoRR,
vol.
abs/1502.03167, 2015.
[52] V.
Badrinarayanan, B.
Mishra, and R.
Cipolla, “Understanding symme-
tries in deep networks,”
[53] H.
Noh, S.
Hong, and B.
Han, “Learning deconvolution network for
semantic segmentation,” CoRR, vol.
abs/1505.04366, 2015.
[54] K.
Jarrett, K.
Kavukcuoglu, M.
Ranzato, and Y.
LeCun, “What is the
best multi-stage architecture for object recognition?,” in ICCV, pp.
2146–
2153, 2009.
[55] K.
He, X.
Zhang, S.
Ren, and J.
Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,” in
ICCV, pp.
1026–1034, 2015.
[56] Y.
Jia, E.
Shelhamer, J.
Donahue, S.
Karayev, J.
Long, R.
Girshick,
S.
Guadarrama, and T.
Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the 22nd ACM international
conference on Multimedia, pp.
675–678, ACM, 2014.
[57] G.
Csurka, D.
Larlus, F.
Perronnin, and F.
Meylan, “What is a good
evaluation measure for semantic segmentation?.,” in BMVC, 2013.
[58] J.
Long, E.
Shelhamer, and T.
Darrell, “Fully convolutional networks for
semantic segmentation,” in https://arxiv.org/pdf/1605.06211v1.pdf, 2016.
[59] D.
R.
Martin, C.
C.
Fowlkes, and J.
Malik, “Learning to detect natural
image boundaries using local brightness, color, and texture cues,” IEEE
transactions on pattern analysis and machine intelligence, vol.
26, no.
5,
pp.
530–549, 2004.
[60] S.
Gould, R.
Fulton, and D.
Koller, “Decomposing a scene into geometric
and semantically consistent regions,” in ICCV, pp.
1–8, IEEE, 2009.
[61] B.
C.
Russell, A.
Torralba, K.
P.
Murphy, and W.
T.
Freeman, “Labelme:
a database and web-based tool for image annotation,” IJCV, vol.
77,
no.
1-3, pp.
157–173, 2008.
[62] M.
Cordts, M.
Omran, S.
Ramos, T.
Rehfeld, M.
Enzweiler, R.
Benenson,
U.
Franke, S.
Roth, and B.
Schiele, “The cityscapes dataset for semantic
urban scene understanding,” arXiv preprint arXiv:1604.01685, 2016.
[63] V.
Koltun, “Efﬁcient inference in fully connected crfs with gaussian edge
potentials,” in In: NIPS (2011, 2011.
[64] Bulo, S.
Rota, and P.
Kontschieder, “Neural decision forests for semantic
image labelling.,” in CVPR, 2014.
[65] Y.
Yang, Z.
Li, L.
Zhang, C.
Murphy, J.
Ver Hoeve, and H.
Jiang, “Local
label descriptor for example based semantic image labeling,” in ECCV,
pp.
361–375, Springer, 2012.
[66] Z.
Liu, X.
Li, P.
Luo, C.-C.
Loy, and X.
Tang, “Semantic image
segmentation via deep parsing network,” in Proceedings of the IEEE
International Conference on Computer Vision, pp.
1377–1385, 2015.
[67] D.
Eigen and R.
Fergus, “Predicting depth, surface normals and semantic
labels with a common multi-scale convolutional architecture,” arXiv
preprint arXiv:1411.4734, 2014.
[68] A.
Handa, V.
Patraucean, V.
Badrinarayanan, S.
Stent, and R.
Cipolla,
“Scenenet: Understanding real world indoor scenes with synthetic data,”
in CVPR, 2016.
[69] Y.
Gal and Z.
Ghahramani, “Dropout as a bayesian approximation:
Insights and applications,” in Deep Learning Workshop, ICML, 2015.
[70] A.
Kendall, V.
Badrinarayanan, and R.
Cipolla, “Bayesian segnet: Model
uncertainty in deep convolutional encoder-decoder architectures for scene
understanding,” arXiv preprint arXiv:1511.02680, 2015.
Vijay Badrinarayanan obtained his Ph.D from
INRIA Rennes, France in 2009.
He was a se-
nior post-doctoral research associate at the Ma-
chine Intelligence Laboratory, Department of En-
gineering, University of Cambridge, U.K.
He
currently works as a Principal Engineer, Deep
Learning at Magic Leap, Inc.
in Mountain View,
CA.
His research interests are in probabilistic
graphical models, deep learning applied to im-
age and video based perception problems.
Alex Kendall graduated with a Bachelor of En-
gineering with First Class Honours in 2013 from
the University of Auckland, New Zealand.
In
2014 he was awarded a Woolf Fisher Scholar-
ship to study towards a Ph.D at the University
of Cambridge, U.K.
He is a member of the Ma-
chine Intelligence Laboratory and is interested in
applications of deep learning for mobile robotics.
Roberto Cipolla obtained a B.A.
(Engineer-
ing) degree from the University of Cambridge
in 1984, an M.S.E.
(Electrical Engineering) from
the University of Pennsylvania in 1985 and a
D.Phil.
(Computer Vision) from the University of
Oxford in 1991.
from 1991-92 was a Toshiba
Fellow and engineer at the Toshiba Corporation
Research and Development Centre in Kawasaki,
Japan.
He joined the Department of Engineer-
ing, University of Cambridge in 1992 as a Lec-
turer and a Fellow of Jesus College.
He became
a Reader in Information Engineering in 1997 and a Professor in 2000.
He became a Fellow of the Royal Academy of Engineering (FREng) in
2010.
His research interests are in computer vision and robotics.
He
has authored 3 books, edited 9 volumes and co-authored more than
300 papers.
Our ﬁndings provide evidence that deep neural networks can be made resistant to adversarial
attacks.
As our theory and experiments indicate, we can design reliable adversarial training
Memorization and generalization are both important for
recommender systems.
Wide linear models can eﬀectively
memorize sparse feature interactions using cross-product fea-
ture transformations, while deep neural networks can gener-
alize to previously unseen feature interactions through low-
dimensional embeddings.
We presented the Wide & Deep
learning framework to combine the strengths of both types
of model.
We productionized and evaluated the framework
on the recommender system of Google Play, a massive-scale
commercial app store.
Online experiment results showed
that the Wide & Deep model led to signiﬁcant improvement
on app acquisitions over wide-only and deep-only models.
8.
REFERENCES
[1] J.
Duchi, E.
Hazan, and Y.
Singer.
Adaptive
subgradient methods for online learning and stochastic
optimization.
Journal of Machine Learning Research,
12:2121–2159, July 2011.
[2] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Deep residual
learning for image recognition.
Proc.
IEEE Conference
on Computer Vision and Pattern Recognition, 2016.
[3] H.
B.
McMahan.
Follow-the-regularized-leader and
mirror descent: Equivalence theorems and l1
regularization.
In Proc.
AISTATS, 2011.
[4] T.
Mikolov, A.
Deoras, D.
Povey, L.
Burget, and J.
H.
Cernocky.
Strategies for training large scale neural
network language models.
In IEEE Automatic Speech
Recognition & Understanding Workshop, 2011.
[5] S.
Rendle.
Factorization machines with libFM.
ACM
Trans.
Intell.
Syst.
Technol., 3(3):57:1–57:22, May 2012.
[6] J.
J.
Tompson, A.
Jain, Y.
LeCun, and C.
Bregler.
Joint
training of a convolutional network and a graphical
model for human pose estimation.
In Z.
Ghahramani,
M.
Welling, C.
Cortes, N.
D.
Lawrence, and K.
Q.
Weinberger, editors, NIPS, pages 1799–1807.
2014.
[7] H.
Wang, N.
Wang, and D.-Y.
Yeung.
Collaborative
deep learning for recommender systems.
In Proc.
KDD,
pages 1235–1244, 2015.
[8] B.
Yan and G.
Chen.
AppJoy: Personalized mobile
application discovery.
In MobiSys, pages 113–126, 2011.
We showed how convolutions and depthwise separable
convolutions lie at both extremes of a discrete spectrum,
with Inception modules being an intermediate point in be-
tween.
This observation has led to us to propose replacing
Inception modules with depthwise separable convolutions in
neural computer vision architectures.
We presented a novel
architecture based on this idea, named Xception, which has
a similar parameter count as Inception V3.
Compared to
Inception V3, Xception shows small gains in classiﬁcation
performance on the ImageNet dataset and large gains on the
JFT dataset.
We expect depthwise separable convolutions
to become a cornerstone of convolutional neural network
architecture design in the future, since they offer similar
properties as Inception modules, yet are as easy to use as
regular convolution layers.
References
[1] M.
Abadi, A.
Agarwal, P.
Barham, E.
Brevdo, Z.
Chen,
C.
Citro, G.
S.
Corrado, A.
Davis, J.
Dean, M.
Devin, S.
Ghe-
mawat, I.
Goodfellow, A.
Harp, G.
Irving, M.
Isard, Y.
Jia,
R.
Jozefowicz, L.
Kaiser, M.
Kudlur, J.
Levenberg, D.
Man´e,
R.
Monga, S.
Moore, D.
Murray, C.
Olah, M.
Schuster,
J.
Shlens, B.
Steiner, I.
Sutskever, K.
Talwar, P.
Tucker, V.
Van-
houcke, V.
Vasudevan, F.
Vi´egas, O.
Vinyals, P.
Warden,
M.
Wattenberg, M.
Wicke, Y.
Yu, and X.
Zheng.
Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015.
Software available from tensorﬂow.org.
[2] F.
Chollet.
Keras.
https://github.com/fchollet/keras, 2015.
[3] D.-A.
Clevert, T.
Unterthiner, and S.
Hochreiter.
Fast and
accurate deep network learning by exponential linear units
(elus).
arXiv preprint arXiv:1511.07289, 2015.
[4] K.
He, X.
Zhang, S.
Ren, and J.
Sun.
Deep residual learning
for image recognition.
arXiv preprint arXiv:1512.03385,
2015.
[5] G.
Hinton, O.
Vinyals, and J.
Dean.
Distilling the knowledge
in a neural network, 2015.
[6] A.
Howard.
Mobilenets: Efﬁcient convolutional neural net-
works for mobile vision applications.
Forthcoming.
[7] S.
Ioffe and C.
Szegedy.
Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In Proceedings of The 32nd International Conference on
Machine Learning, pages 448–456, 2015.
[8] J.
Jin, A.
Dundar, and E.
Culurciello.
Flattened convolutional
neural networks for feedforward acceleration.
arXiv preprint
arXiv:1412.5474, 2014.
[9] A.
Krizhevsky, I.
Sutskever, and G.
E.
Hinton.
Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages
1097–1105, 2012.
[10] Y.
LeCun, L.
Jackel, L.
Bottou, C.
Cortes, J.
S.
Denker,
H.
Drucker, I.
Guyon, U.
Muller, E.
Sackinger, P.
Simard,
et al.
Learning algorithms for classiﬁcation: A comparison on
handwritten digit recognition.
Neural networks: the statistical
mechanics perspective, 261:276, 1995.
[11] M.
Lin, Q.
Chen, and S.
Yan.
Network in network.
arXiv
preprint arXiv:1312.4400, 2013.
[12] F.
Mamalet and C.
Garcia.
Simplifying ConvNets for Fast
Learning.
In International Conference on Artiﬁcial Neural
Networks (ICANN 2012), pages 58–65.
Springer, 2012.
[13] B.
T.
Polyak and A.
B.
Juditsky.
Acceleration of stochas-
tic approximation by averaging.
SIAM J.
Control Optim.,
30(4):838–855, July 1992.
[14] O.
Russakovsky, J.
Deng, H.
Su, J.
Krause, S.
Satheesh, S.
Ma,
Z.
Huang, A.
Karpathy, A.
Khosla, M.
Bernstein, et al.
Ima-
genet large scale visual recognition challenge.
2014.
[15] L.
Sifre.
Rigid-motion scattering for image classiﬁcation,
2014.
Ph.D.
thesis.
[16] L.
Sifre and S.
Mallat.
Rotation, scaling and deformation
invariant scattering for texture discrimination.
In 2013 IEEE
Conference on Computer Vision and Pattern Recognition,
Portland, OR, USA, June 23-28, 2013, pages 1233–1240,
2013.
[17] N.
Silberman and S.
Guadarrama.
Tf-slim, 2016.
[18] K.
Simonyan and A.
Zisserman.
Very deep convolutional
networks for large-scale image recognition.
arXiv preprint
arXiv:1409.1556, 2014.
[19] C.
Szegedy, S.
Ioffe, and V.
Vanhoucke.
Inception-v4,
inception-resnet and the impact of residual connections on
learning.
arXiv preprint arXiv:1602.07261, 2016.
[20] C.
Szegedy, W.
Liu, Y.
Jia, P.
Sermanet, S.
Reed, D.
Anguelov,
D.
Erhan, V.
Vanhoucke, and A.
Rabinovich.
Going deeper
with convolutions.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1–9, 2015.
[21] C.
Szegedy, V.
Vanhoucke, S.
Ioffe, J.
Shlens, and Z.
Wojna.
Rethinking the inception architecture for computer vision.
arXiv preprint arXiv:1512.00567, 2015.
[22] T.
Tieleman and G.
Hinton.
Divide the gradient by a run-
ning average of its recent magnitude.
COURSERA: Neural
Networks for Machine Learning, 4, 2012.
Accessed: 2015-
11-05.
[23] V.
Vanhoucke.
Learning visual representations at scale.
ICLR,
2014.
[24] M.
Wang, B.
Liu, and H.
Foroosh.
Factorized convolutional
neural networks.
arXiv preprint arXiv:1608.04337, 2016.
[25] M.
D.
Zeiler and R.
Fergus.
Visualizing and understanding
convolutional networks.
In Computer Vision–ECCV 2014,
pages 818–833.
Springer, 2014.