Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5].
Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].
Recurrent models typically factor computation along the symbol positions of the input and output
sequences.
Aligning the positions to steps in computation time, they generate a sequence of hidden
states ht, as a function of the previous hidden state ht−1 and the input for position t.
This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples.
Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter.
The fundamental
constraint of sequential computation, however, remains.
Attention mechanisms have become an integral part of compelling sequence modeling and transduc-
tion models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19].
In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions.
In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.
This makes
it more difficult to learn dependencies between distant positions [12].
In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence.
Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution.
In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].
3
Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence
of continuous representations z = (z1, ..., zn).
Given z, the decoder then generates an output
sequence (y1, ..., ym) of symbols one element at a time.
At each step the model is auto-regressive
[10], consuming the previously generated symbols as additional input when generating the next.
2
Figure 1: The Transformer - model architecture.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1
Encoder and Decoder Stacks
Encoder:
The encoder is composed of a stack of N = 6 identical layers.
Each layer has two
sub-layers.
The first is a multi-head self-attention mechanism, and the second is a simple, position-
wise fully connected feed-forward network.
We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1].
That is, the output of each sub-layer is
LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
itself.
To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512.
Decoder:
The decoder is also composed of a stack of N = 6 identical layers.
In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack.
Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization.
We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions.
This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position i can depend only on the known outputs at positions less than i.
3.2
Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors.
The output is computed as a weighted sum
3
Scaled Dot-Product Attention
Multi-Head Attention
Figure 2: (left) Scaled Dot-Product Attention.
(right) Multi-Head Attention consists of several
attention layers running in parallel.
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
3.2.1
Scaled Dot-Product Attention
We call our particular attention "Scaled Dot-Product Attention" (Figure 2).
The input consists of
queries and keys of dimension dk, and values of dimension dv.
We compute the dot products of the
query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the
values.
In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix Q.
The keys and values are also packed together into matrices K and V .
We compute
the matrix of outputs as:
Attention(Q, K, V ) = softmax(QKT
√dk
)V
(1)
The two most commonly used attention functions are additive attention [2], and dot-product (multi-
plicative) attention.
Dot-product attention is identical to our algorithm, except for the scaling factor
of
1
√dk .
Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer.
While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.
While for small values of dk the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of dk [3].
We suspect that for large values of
dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients 4.
To counteract this effect, we scale the dot products by
1
√dk .
3.2.2
Multi-Head Attention
Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values h times with different, learned
linear projections to dk, dk and dv dimensions, respectively.
On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
4To illustrate why the dot products get large, assume that the components of q and k are independent random
variables with mean 0 and variance 1.
Then their dot product, q · k = Pdk
i=1 qiki, has mean 0 and variance dk.
4
output values.
These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.
Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions.
With a single attention head, averaging inhibits this.
MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O
where headi = Attention(QW Q
i , KW K
i , V W V
i )
Where the projections are parameter matrices W Q
i
∈Rdmodel×dk, W K
i
∈Rdmodel×dk, W V
i
∈Rdmodel×dv
and W O ∈Rhdv×dmodel.
In this work we employ h = 8 parallel attention layers, or heads.
For each of these we use
dk = dv = dmodel/h = 64.
Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.
3.2.3
Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:
• In "encoder-decoder attention" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder.
This allows every
position in the decoder to attend over all positions in the input sequence.
This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[38, 2, 9].
• The encoder contains self-attention layers.
In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder.
Each position in the encoder can attend to all positions in the previous layer of the
encoder.
• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position.
We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property.
We implement this
inside of scaled dot-product attention by masking out (setting to −∞) all values in the input
of the softmax which correspond to illegal connections.
See Figure 2.
3.3
Position-wise Feed-Forward Networks
In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically.
This
consists of two linear transformations with a ReLU activation in between.
FFN(x) = max(0, xW1 + b1)W2 + b2
(2)
While the linear transformations are the same across different positions, they use different parameters
from layer to layer.
Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality
dff = 2048.
3.4
Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension dmodel.
We also use the usual learned linear transfor-
mation and softmax function to convert the decoder output to predicted next-token probabilities.
In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30].
In the embedding layers, we multiply those weights by √dmodel.
5
Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types.
n is the sequence length, d is the representation dimension, k is the kernel
size of convolutions and r the size of the neighborhood in restricted self-attention.
Layer Type
Complexity per Layer
Sequential
Maximum Path Length
Operations
Self-Attention
O(n2 · d)
O(1)
O(1)
Recurrent
O(n · d2)
O(n)
O(n)
Convolutional
O(k · n · d2)
O(1)
O(logk(n))
Self-Attention (restricted)
O(r · n · d)
O(1)
O(n/r)
3.5
Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence.
To this end, we add "positional encodings" to the input embeddings at the
bottoms of the encoder and decoder stacks.
The positional encodings have the same dimension dmodel
as the embeddings, so that the two can be summed.
There are many choices of positional encodings,
learned and fixed [9].
In this work, we use sine and cosine functions of different frequencies:
PE(pos,2i) = sin(pos/100002i/dmodel)
PE(pos,2i+1) = cos(pos/100002i/dmodel)
where pos is the position and i is the dimension.
That is, each dimension of the positional encoding
corresponds to a sinusoid.
The wavelengths form a geometric progression from 2π to 10000 · 2π.
We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of
PEpos.
We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)).
We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.
4
Why Self-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolu-
tional layers commonly used for mapping one variable-length sequence of symbol representations
(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden
layer in a typical sequence transduction encoder or decoder.
Motivating our use of self-attention we
consider three desiderata.
One is the total computational complexity per layer.
Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.
The third is the path length between long-range dependencies in the network.
Learning long-range
dependencies is a key challenge in many sequence transduction tasks.
One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network.
The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12].
Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.
As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires O(n) sequential operations.
In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence
6
length n is smaller than the representation dimensionality d, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece
[38] and byte-pair [31] representations.
To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size r in
the input sequence centered around the respective output position.
This would increase the maximum
path length to O(n/r).
We plan to investigate this approach further in future work.
A single convolutional layer with kernel width k < n does not connect all pairs of input and output
positions.
Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,
or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network.
Convolutional layers are generally more expensive than
recurrent layers, by a factor of k.
Separable convolutions [6], however, decrease the complexity
considerably, to O(k · n · d + n · d2).
Even with k = n, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.
As side benefit, self-attention could yield more interpretable models.
We inspect attention distributions
from our models and present and discuss examples in the appendix.
Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.
5
Training
This section describes the training regime for our models.
5.1
Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs.
Sentences were encoded using byte-pair encoding [3], which has a shared source-
target vocabulary of about 37000 tokens.
For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [38].
Sentence pairs were batched together by approximate sequence length.
Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.
5.2
Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs.
For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds.
We
trained the base models for a total of 100,000 steps or 12 hours.
For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds.
The big models were trained for 300,000 steps
(3.5 days).
5.3
Optimizer
We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.
We varied the learning
rate over the course of training, according to the formula:
lrate = d−0.5
model · min(step_num−0.5, step_num · warmup_steps−1.5)
(3)
This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number.
We used
warmup_steps = 4000.
5.4
Regularization
We employ three types of regularization during training:
7
Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
Model
BLEU
Training Cost (FLOPs)
EN-DE
EN-FR
EN-DE
EN-FR
ByteNet [18]
23.75
Deep-Att + PosUnk [39]
39.2
1.0 · 1020
GNMT + RL [38]
24.6
39.92
2.3 · 1019
1.4 · 1020
ConvS2S [9]
25.16
40.46
9.6 · 1018
1.5 · 1020
MoE [32]
26.03
40.56
2.0 · 1019
1.2 · 1020
Deep-Att + PosUnk Ensemble [39]
40.4
8.0 · 1020
GNMT + RL Ensemble [38]
26.30
41.16
1.8 · 1020
1.1 · 1021
ConvS2S Ensemble [9]
26.36
41.29
7.7 · 1019
1.2 · 1021
Transformer (base model)
27.3
38.1
3.3 · 1018
Transformer (big)
28.4
41.8
2.3 · 1019
Residual Dropout
We apply dropout [33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized.
In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks.
For the base model, we use a rate of
Pdrop = 0.1.
Label Smoothing
During training, we employed label smoothing of value ϵls = 0.1 [36].
This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
Text classiﬁcation is an important task in Natural
Language Processing with many applications, such
as web search, information retrieval, ranking and
document
classiﬁcation
(Deerwester et al., 1990;
Pang and Lee, 2008).
Recently,
models based
on neural networks have become increasingly
popular
(Kim, 2014;
Zhang and LeCun, 2015;
Conneau et al., 2016).
While these models achieve
very good performance in practice, they tend to be
relatively slow both at train and test time, limiting
their use on very large datasets.
Meanwhile,
linear
classiﬁers
are
of-
ten
considered
as
strong
baselines
for
text
classiﬁcation
problems
(Joachims, 1998;
McCallum and Nigam, 1998;
Fan et al., 2008).
Despite their simplicity, they often obtain state-
of-the-art performances if the right features are
used
(Wang and Manning, 2012).
They
also
have the potential to scale to very large cor-
pus (Agarwal et al., 2014).
In this work, we explore ways to scale these
baselines to very large corpus with a large output
space, in the context of text classiﬁcation.
Inspired
by the recent work in efﬁcient word representation
learning (Mikolov et al., 2013;
Levy et al., 2015),
we show that linear models with a rank constraint
and a fast loss approximation can train on a billion
words within ten minutes, while achieving perfor-
mance on par with the state-of-the-art.
We evalu-
ate the quality of our approach fastText1 on two
different tasks, namely tag prediction and sentiment
analysis.
2
Model architecture
A
simple
and
efﬁcient
baseline
for
sentence
classiﬁcation is to represent sentences as bag of
words (BoW) and train a linear classiﬁer, e.g., a
logistic regression or an SVM (Joachims, 1998;
Fan et al., 2008).
However, linear classiﬁers do
not share parameters among features and classes.
This possibly limits their generalization in the
context of large output space where some classes
have very few examples.
Common solutions
to this problem are to factorize the linear clas-
siﬁer
into
low
rank
matrices
(Schutze, 1992;
Mikolov et al., 2013)
or
to
use
multilayer
neural
networks
(Collobert and Weston, 2008;
Zhang et al., 2015).
Figure 1 shows a simple linear model with rank
constraint.
The ﬁrst weight matrix A is a look-up
table over the words.
The word representations are
then averaged into a text representation, which is in
turn fed to a linear classiﬁer.
The text representa-
1https://github.com/facebookresearch/fastText
x1
x2
.
.
.
xN−1
xN
hidden
output
Figure 1: Model architecture of fastText for a sentence with
N ngram features x1, .
.
.
, xN.
The features are embedded and
averaged to form the hidden variable.
tion is an hidden variable which can be potentially
be reused.
This architecture is similar to the cbow
model of Mikolov et al.
(2013), where the middle
word is replaced by a label.
We use the softmax
function f to compute the probability distribution
over the predeﬁned classes.
For a set of N doc-
uments, this leads to minimizing the negative log-
likelihood over the classes:
−1
N
N
X
n=1
yn log(f(BAxn)),
where xn is the normalized bag of features of the n-
th document, yn the label, A and B the weight matri-
ces.
This model is trained asynchronously on mul-
tiple CPUs using stochastic gradient descent and a
linearly decaying learning rate.
2.1
Hierarchical softmax
When the number of classes is large, computing the
linear classiﬁer is computationally expensive.
More
precisely, the computational complexity is O(kh)
where k is the number of classes and h the di-
mension of the text representation.
In order to im-
prove our running time, we use a hierarchical soft-
max (Goodman, 2001) based on the Huffman cod-
ing tree (Mikolov et al., 2013).
During training, the
computational complexity drops to O(h log2(k)).
The hierarchical softmax is also advantageous at
test time when searching for the most likely class.
Each node is associated with a probability that is the
probability of the path from the root to that node.
If
the node is at depth l + 1 with parents n1, .
.
.
, nl, its
probability is
P(nl+1) =
lY
i=1
P(ni).
This means that the probability of a node is always
lower than the one of its parent.
Exploring the tree
with a depth ﬁrst search and tracking the maximum
probability among the leaves allows us to discard
any branch associated with a small probability.
In
practice, we observe a reduction of the complexity
to O(h log2(k)) at test time.
This approach is fur-
ther extended to compute the T-top targets at the
cost of O(log(T)), using a binary heap.
2.2
N-gram features
Bag of words is invariant to word order but taking
explicitly this order into account is often computa-
tionally very expensive.
Instead, we use a bag of
n-grams as additional features to capture some par-
tial information about the local word order.
This
is very efﬁcient in practice while achieving compa-
rable results to methods that explicitly use the or-
der (Wang and Manning, 2012).
We
maintain
a
fast
and
memory
efﬁcient
mapping of the n-grams by using the hashing
trick (Weinberger et al., 2009) with the same hash-
ing function as in Mikolov et al.
(2011) and 10M
bins if we only used bigrams, and 100M otherwise.
3
Experiments
We evaluate fastText on two different tasks.
First, we compare it to existing text classifers on the
problem of sentiment analysis.
Then, we evaluate
its capacity to scale to large output space on a tag
prediction dataset.
Note that our model could be im-
plemented with the Vowpal Wabbit library,2 but we
observe in practice, that our tailored implementation
is at least 2-5× faster.
3.1
Sentiment analysis
Datasets
and
baselines.
We
employ
the
same
8
datasets
and
evaluation
protocol
of Zhang et al.
(2015).
We report the n-grams
and TFIDF baselines from Zhang et al.
(2015),
as
well
as
the
character
level
convolutional
model (char-CNN) of Zhang and LeCun (2015),
the character based convolution recurrent net-
work (char-CRNN) of (Xiao and Cho, 2016) and
the very deep convolutional network (VDCNN)
of
Conneau et al.
(2016).
We
also
compare
2Using the options --nn, --ngrams and --log multi
Model
AG
Sogou
DBP
Yelp P.
Yelp F.
Yah.
A.
Amz.
F.
Amz.
P.
BoW (Zhang et al., 2015)
88.8
92.9
96.6
92.2
58.0
68.9
54.6
90.4
ngrams (Zhang et al., 2015)
92.0
97.1
98.6
95.6
56.3
68.5
54.3
92.0
ngrams TFIDF (Zhang et al., 2015)
92.4
97.2
98.7
95.4
54.8
68.5
52.4
91.5
char-CNN (Zhang and LeCun, 2015)
87.2
95.1
98.3
94.7
62.0
71.2
59.5
94.5
char-CRNN (Xiao and Cho, 2016)
91.4
95.2
98.6
94.5
61.8
71.7
59.2
94.1
VDCNN (Conneau et al., 2016)
91.3
96.8
98.7
95.7
64.7
73.4
63.0
95.7
fastText, h = 10
91.5
93.9
98.1
93.8
60.4
72.0
55.8
91.2
fastText, h = 10, bigram
92.5
96.8
98.6
95.7
63.9
72.3
60.2
94.6
Table 1: Test accuracy [%] on sentiment datasets.
FastText has been run with the same parameters for all the datasets.
It has
10 hidden units and we evaluate it with and without bigrams.
For char-CNN, we show the best reported numbers without data
augmentation.
Zhang and LeCun (2015)
Conneau et al.
(2016)
fastText
small char-CNN
big char-CNN
depth=9
depth=17
depth=29
h = 10, bigram
AG
1h
3h
24m
37m
51m
1s
Sogou
-
-
25m
41m
56m
7s
DBpedia
2h
5h
27m
44m
1h
2s
Yelp P.
-
-
28m
43m
1h09
3s
Yelp F.
-
-
29m
45m
1h12
4s
Yah.
A.
8h
1d
1h
1h33
2h
5s
Amz.
F.
2d
5d
2h45
4h20
7h
9s
Amz.
P.
2d
5d
2h45
4h25
7h
10s
Table 2: Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN.
to Tang et al.
(2015) following their
evaluation
protocol.
We report their main baselines as
well as their two approaches based on recurrent
networks (Conv-GRNN and LSTM-GRNN).
Self-supervised methods have achieved remarkable
success in a wide range of NLP tasks (Mikolov et al.,
2013; Peters et al., 2018; Devlin et al., 2019; Joshi
et al., 2019; Yang et al., 2019; Liu et al., 2019).
The most successful approaches have been variants of
masked language models, which are denoising autoen-
coders that are trained to reconstruct text where a ran-
dom subset of the words has been masked out.
Recent
work has shown gains by improving the distribution of
masked tokens (Joshi et al., 2019), the order in which
masked tokens are predicted (Yang et al., 2019), and the
available context for replacing masked tokens (Dong
et al., 2019).
However, these methods typically focus
on particular types of end tasks (e.g.
span prediction,
generation, etc.), limiting their applicability.
In this paper, we present BART, which pre-trains
a model combining Bidirectional and Auto-Regressive
Transformers.
BART is a denoising autoencoder built
with a sequence-to-sequence model that is applicable
to a very wide range of end tasks.
Pretraining has
two stages (1) text is corrupted with an arbitrary nois-
ing function, and (2) a sequence-to-sequence model is
learned to reconstruct the original text.
BART uses a
standard Tranformer-based neural machine translation
architecture which, despite its simplicity, can be seen as
generalizing BERT (due to the bidirectional encoder),
GPT (with the left-to-right decoder), and many other
more recent pretraining schemes (see Figure 1).
A key advantage of this setup is the noising ﬂexibil-
ity; arbitrary transformations can be applied to the orig-
inal text, including changing its length.
We evaluate
a number of noising approaches, ﬁnding the best per-
formance by both randomly shufﬂing the order of the
original sentences and using a novel in-ﬁlling scheme,
where arbitrary length spans of text (including zero
length) are replaced with a single mask token.
This ap-
proach generalizes the original word masking and next
sentence prediction objectives in BERT by forcing the
model to reason more about overall sentence length and
make longer range transformations to the input.
BART is particularly effective when ﬁne tuned for
text generation but also works well for comprehen-
sion tasks.
It matches the performance of RoBERTa
(Liu et al., 2019) with comparable training resources
on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar
et al., 2016), and achieves new state-of-the-art results
on a range of abstractive dialogue, question answer-
ing, and summarization tasks.
For example, it im-
proves performance by 6 ROUGE over previous work
on XSum (Narayan et al., 2018).
BART also opens up new ways of thinking about ﬁne
tuning.
We present a new scheme for machine transla-
tion where a BART model is stacked above a few ad-
ditional transformer layers.
These layers are trained
to essentially translate the foreign language to noised
arXiv:1910.13461v1  [cs.CL]  29 Oct 2019
Bidirectional 
Encoder
A  _  C  _  E 
B       D    
(a) BERT: Random tokens are replaced with masks, and
the document is encoded bidirectionally.
Missing tokens
are predicted independently, so BERT cannot easily be
used for generation.
Autoregressive 
Decoder
A  B  C  D  E
<s> A  B  C  D  
(b) GPT: Tokens are predicted auto-regressively, meaning
GPT can be used for generation.
However words can only
condition on leftward context, so it cannot learn bidirec-
tional interactions.
Autoregressive 
Decoder
Bidirectional 
Encoder
A  B  C  D  E
A  _  B  _  E      
<s> A  B  C  D  
(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.
Here, a
document has been corrupted by replacing spans of text with mask symbols.
The corrupted document (left) is encoded with
a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.
For ﬁne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the ﬁnal
hidden state of the decoder.
Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).
English, by propagation through BART, thereby us-
ing BART as a pre-trained target-side language model.
This approach improves performance over a strong
back-translation MT baseline by 1.1 BLEU on the
WMT Romanian-English benchmark.
To better understand these effects, we also report
an ablation analysis that replicates other recently pro-
posed training objectives.
This study allows us to care-
fully control for a number of factors, including data
and optimization parameters, which have been shown
to be as important for overall performance as the se-
lection of training objectives (Liu et al., 2019).
We ﬁnd
that BART exhibits the most consistently strong perfor-
mance across the full range of tasks we consider.
2
Model
BART is a denoising autoencoder that maps a corrupted
document to the original document it was derived from.
It is implemented as a sequence-to-sequence model
with a bidirectional encoder over corrupted text and a
left-to-right autoregressive decoder.
For pre-training,
we optimize the negative log likelihood of the original
document.
2.1
Architecture
BART uses the standard sequence-to-sequence Trans-
former architecture from (Vaswani et al., 2017), ex-
cept, following GPT, that we modify ReLU activa-
tion functions to GeLUs (Hendrycks & Gimpel, 2016)
and initialise parameters from N(0, 0.02).
For our
base model, we use 6 layers in the encoder and de-
coder, and for our large model we use 12 layers in
each.
The architecture is closely related to that used in
BERT, with the following differences: (1) each layer of
the decoder additionally performs cross-attention over
the ﬁnal hidden layer of the encoder (as in the trans-
former sequence-to-sequence model); and (2) BERT
uses an additional feed-forward network before word-
prediction, which BART does not.
In total, BART con-
tains roughly 10% more parameters than the equiva-
lently sized BERT model.
2.2
Pre-training BART
BART is trained by corrupting documents and then op-
timizing a reconstruction loss—the cross-entropy be-
tween the decoder’s output and the original document.
Unlike existing denoising autoencoders, which are tai-
lored to speciﬁc noising schemes, BART allows us to
apply any type of document corruption.
In the extreme
case, where all information about the source is lost,
BART is equivalent to a language model.
We experiment with several previously proposed and
novel transformations, but we believe there is a sig-
niﬁcant potential for development of other new alter-
natives.
The transformations we used are summarized
below, and examples are shown in Figure 2.
Token Masking
Following BERT (Devlin et al.,
2019), random tokens are sampled and replaced with
[MASK] elements.
Token Deletion
Random tokens are deleted from the
input.
In contrast to token masking, the model must
decide which positions are missing inputs.
A B C .
D E .
A .
C .
E .
A _ .
D _ E .
A _C .
_ E .
C .
D E .
A B
Document Rotation
Token Masking
Token Deletion
Text Inﬁlling
D E .
A B C .
Sentence Permutation
Figure 2: Transformations for noising the input that we experiment with.
These transformations can be composed.
Text Inﬁlling
A number of text spans are sampled,
with span lengths drawn from a Poisson distribution
(λ = 3).
Each span is replaced with a single [MASK]
token.
0-length spans correspond to the insertion of
[MASK] tokens.
Text inﬁlling is inspired by Span-
BERT (Joshi et al., 2019), but SpanBERT samples
span lengths from a different (clamped geometric) dis-
tribution, and replaces each span with a sequence of
[MASK] tokens of exactly the same length.
Text inﬁll-
ing teaches the model to predict how many tokens are
missing from a span.
Sentence Permutation
A document is divided into
sentences based on full stops, and these sentences are
shufﬂed in a random order.
Document Rotation
A token is chosen uniformly at
random, and the document is rotated so that it begins
with that token.
This task trains the model to identify
the start of the document.
3
Fine-tuning BART
The representations produced by BART can be used in
several ways for downstream applications.
3.1
Sequence Classiﬁcation Tasks
For sequence classiﬁcation tasks, the same input is fed
into the encoder and decoder, and the ﬁnal hidden state
of the ﬁnal decoder token is fed into new multi-class
linear classiﬁer.
This approach is related to the CLS
token in BERT; however we add the additional token
to the end so that representation for the token in the
decoder can attend to decoder states from the complete
input (Figure 3a).
3.2
Token Classiﬁcation Tasks
For token classiﬁcation tasks, such as answer endpoint
classiﬁcation for SQuAD, we feed the complete doc-
ument into the encoder and decoder, and use the top
hidden state of the decoder as a representation for each
word.
This representation is used to classify the token.
3.3
Sequence Generation Tasks
Because BART has an autoregressive decoder, it can be
directly ﬁne tuned for sequence generation tasks such
as abstractive question answering and summarization.
In both of these tasks, information is copied from the
input but manipulated, which is closely related to the
denoising pre-training objective.
Here, the encoder in-
put is the input sequence, and the decoder generates
outputs autoregressively.
3.4
Machine Translation
We also explore using BART to improve machine trans-
lation decoders for translating into English.
Previous
work Edunov et al.
(2019) has shown that models can
be improved by incorporating pre-trained encoders, but
gains from using pre-trained language models in de-
coders have been limited.
We show that it is possible
to use the entire BART model (both encoder and de-
coder) as a single pretrained decoder for machine trans-
lation, by adding a new set of encoder parameters that
are learned from bitext (see Figure 3b).
More precisely, we replace BART’s encoder embed-
ding layer with a new randomly initialized encoder.
The model is trained end-to-end, which trains the new
encoder to map foreign words into an input that BART
can de-noise to English.
The new encoder can use a
separate vocabulary from the original BART model.
We train the source encoder in two steps, in both
cases backpropagating the cross-entropy loss from the
output of the BART model.
In the ﬁrst step, we freeze
most of BART parameters and only update the ran-
domly initialized source encoder, the BART positional
embeddings, and the self-attention input projection ma-
trix of BART’s encoder ﬁrst layer.
In the second step,
we train all model parameters for a small number of
iterations.
4
Comparing Pre-training Objectives
BART supports a much wider range of noising schemes
during pre-training than previous work.
We compare a
range of options using base-size models (6 encoder and
6 decoder layers, with a hidden size of 768), evaluated
on a representative subset of the tasks we will consider
for the full large scale experiments in §5.
4.1
Comparison Objectives
While many pre-training objectives have been pro-
posed, fair comparisons between these have been dif-
ﬁcult to perform, at least in part due to differences in
training data, training resources, architectural differ-
ences between models, and ﬁne-tuning procedures.
We
Pre-trained 
Decoder
Pre-trained 
Encoder
label
A  B  C  D  E 
<s> A  B  C  D  E
(a) To use BART for classiﬁcation problems, the same
input is fed into the encoder and decoder, and the repre-
sentation from the ﬁnal output is used.
Randomly 
Initialized Encoder
    α   β   γ   δ   ε
Pre-trained  
Decoder
Pre-trained 
Encoder
A  B  C  D  E
<s> A  B  C  D  
(b) For machine translation, we learn a small additional
encoder that replaces the word embeddings in BART.
The
new encoder can use a disjoint vocabulary.
Figure 3: Fine tuning BART for classiﬁcation and translation.
re-implement strong pre-training approaches recently
proposed for discriminative and generation tasks.
We
aim, as much as possible, to control for differences un-
related to the pre-training objective.
However, we do
make minor changes to the learning rate and usage of
layer normalisation in order to improve performance
(tuning these separately for each objective).
For refer-
ence, we compare our implementations with published
numbers from BERT, which was also trained for 1M
steps on a combination of books and Wikipedia data.
We compare the following approaches:
Language Model
Similarly to GPT (Radford et al.,
2018), we train a left-to-right Transformer language
model.
This model is equivalent to the BART decoder,
without cross-attention.
Permuted Language Model
Based on XLNet (Yang
et al., 2019), we sample 1/6 of the tokens, and gener-
ate them in a random order autoregressively.
For con-
sistency with other models, we do not implement the
relative positional embeddings or attention across seg-
ments from XLNet.
Masked Language Model
Following BERT (Devlin
et al., 2019), we replace 15% of tokens with [MASK]
symbols, and train the model to independently predict
the original tokens.
Multitask Masked Language Model
As in UniLM
(Dong et al., 2019), we train a Masked Language
Model with additional self-attention masks.
Self at-
tention masks are chosen randomly in with the follow
proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 un-
masked, and 1/3 with the ﬁrst 50% of tokens unmasked
and a left-to-right mask for the remainder.
Masked Seq-to-Seq
Inspired by MASS (Song et al.,
2019), we mask a span containing 50% of tokens,
and train a sequence to sequence model to predict the
masked tokens.
For the Permuted LM, Masked LM and Multitask
Masked LM, we use two-stream attention (Yang et al.,
2019) to efﬁciently compute likelihoods of the output
part of the sequence (using a diagonal self-attention
mask on the output to predict words left-to-right).
We experiment with (1) treating the task as a stan-
dard sequence-to-sequence problem, where the source
input to the encoder and the target is the decoder out-
put, or (2) adding the source as preﬁx to the target in
the decoder, with a loss only on the target part of the
sequence.
We ﬁnd the former works better for BART
models, and the latter for other models.
To most directly compare our models on their ability
to model their ﬁne-tuning objective (the log likelihood
of the human text), we report perplexity in Table 1.
4.2
Tasks
SQuAD
(Rajpurkar et al., 2016)a an extractive ques-
tion answering task on Wikipedia paragraphs.
Answers
are text spans extracted from a given document context.
Similar to BERT (Devlin et al., 2019), we use concate-
nated question and context as input to the encoder of
BART, and additionally pass them to the decoder.
The
model includes classiﬁers to predict the start and end
indices of each token.
MNLI
(Williams et al., 2017), a bitext classiﬁcation
task to predict whether one sentence entails another.
The ﬁne-tuned model concatenates the two sentences
with appended an EOS token, and passes them to both
the BART encoder and decoder.
In contrast to BERT,
the representation of the EOS token is used to classify
the sentences relations.
ELI5
(Fan et al., 2019), a long-form abstractive ques-
tion answering dataset.
Models generate answers con-
ditioned on the concatenation of a question and sup-
porting documents.
XSum
(Narayan et al., 2018), a news summarization
dataset with highly abstractive summaries.
ConvAI2
(Dinan et al., 2019), a dialogue response
generation task, conditioned on context and a persona.
CNN/DM
(Hermann et al., 2015), a news summa-
rization dataset.
Summaries here are typically closely
related to source sentences.
4.3
Language model pre-training has been shown to
be effective for improving many natural language
processing tasks (Dai and Le, 2015; Peters et al.,
2018a; Radford et al., 2018; Howard and Ruder,
2018).
These include sentence-level tasks such as
natural language inference (Bowman et al., 2015;
Williams et al., 2018) and paraphrasing (Dolan
and Brockett, 2005), which aim to predict the re-
lationships between sentences by analyzing them
holistically, as well as token-level tasks such as
named entity recognition and question answering,
where models are required to produce ﬁne-grained
output at the token level (Tjong Kim Sang and
De Meulder, 2003; Rajpurkar et al., 2016).
There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and ﬁne-tuning.
The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-speciﬁc architectures that
include the pre-trained representations as addi-
tional features.
The ﬁne-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-speciﬁc parameters, and is trained on the
downstream tasks by simply ﬁne-tuning all pre-
trained parameters.
The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the ﬁne-tuning approaches.
The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training.
For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017).
Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying ﬁne-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
In this paper, we improve the ﬁne-tuning based
approaches by proposing BERT: Bidirectional
Encoder
Representations
from
Transformers.
BERT alleviates the previously mentioned unidi-
rectionality constraint by using a “masked lan-
guage model” (MLM) pre-training objective, in-
spired by the Cloze task (Taylor, 1953).
The
masked language model randomly masks some of
the tokens from the input, and the objective is to
predict the original vocabulary id of the masked
arXiv:1810.04805v2  [cs.CL]  24 May 2019
word based only on its context.
Unlike left-to-
right language model pre-training, the MLM ob-
jective enables the representation to fuse the left
and the right context, which allows us to pre-
train a deep bidirectional Transformer.
In addi-
tion to the masked language model, we also use
a “next sentence prediction” task that jointly pre-
trains text-pair representations.
The contributions
of our paper are as follows:
• We demonstrate the importance of bidirectional
pre-training for language representations.
Un-
like Radford et al.
(2018), which uses unidirec-
tional language models for pre-training, BERT
uses masked language models to enable pre-
trained deep bidirectional representations.
This
is also in contrast to Peters et al.
(2018a), which
uses a shallow concatenation of independently
trained left-to-right and right-to-left LMs.
• We show that pre-trained representations reduce
the need for many heavily-engineered task-
speciﬁc architectures.
BERT is the ﬁrst ﬁne-
tuning based representation model that achieves
state-of-the-art performance on a large suite
of sentence-level and token-level tasks, outper-
forming many task-speciﬁc architectures.
• BERT advances the state of the art for eleven
NLP tasks.
The code and pre-trained mod-
els are available at https://github.com/
google-research/bert.
2
Related Work
There is a long history of pre-training general lan-
guage representations, and we brieﬂy review the
most widely-used approaches in this section.
2.1
Unsupervised Feature-based Approaches
Learning widely applicable representations of
words has been an active area of research for
decades, including non-neural (Brown et al., 1992;
Ando and Zhang, 2005; Blitzer et al., 2006) and
neural (Mikolov et al., 2013; Pennington et al.,
2014) methods.
Pre-trained word embeddings
are an integral part of modern NLP systems, of-
fering signiﬁcant improvements over embeddings
learned from scratch (Turian et al., 2010).
To pre-
train word embedding vectors, left-to-right lan-
guage modeling objectives have been used (Mnih
and Hinton, 2009), as well as objectives to dis-
criminate correct from incorrect words in left and
right context (Mikolov et al., 2013).
These approaches have been generalized to
coarser granularities, such as sentence embed-
dings (Kiros et al., 2015; Logeswaran and Lee,
2018) or paragraph embeddings (Le and Mikolov,
2014).
To train sentence representations, prior
work has used objectives to rank candidate next
sentences (Jernite et al., 2017; Logeswaran and
Lee, 2018), left-to-right generation of next sen-
tence words given a representation of the previous
sentence (Kiros et al., 2015), or denoising auto-
encoder derived objectives (Hill et al., 2016).
ELMo and its predecessor (Peters et al., 2017,
2018a) generalize traditional word embedding re-
search along a different dimension.
They extract
context-sensitive features from a left-to-right and a
right-to-left language model.
The contextual rep-
resentation of each token is the concatenation of
the left-to-right and right-to-left representations.
When integrating contextual word embeddings
with existing task-speciﬁc architectures, ELMo
advances the state of the art for several major NLP
benchmarks (Peters et al., 2018a) including ques-
tion answering (Rajpurkar et al., 2016), sentiment
analysis (Socher et al., 2013), and named entity
recognition (Tjong Kim Sang and De Meulder,
2003).
Melamud et al.
(2016) proposed learning
contextual representations through a task to pre-
dict a single word from both left and right context
using LSTMs.
Similar to ELMo, their model is
feature-based and not deeply bidirectional.
Fedus
et al.
(2018) shows that the cloze task can be used
to improve the robustness of text generation mod-
els.
2.2
Unsupervised Fine-tuning Approaches
As with the feature-based approaches, the ﬁrst
works in this direction only pre-trained word em-
bedding parameters from unlabeled text
(Col-
lobert and Weston, 2008).
More recently, sentence or document encoders
which produce contextual token representations
have been pre-trained from unlabeled text and
ﬁne-tuned for a supervised downstream task (Dai
and Le, 2015; Howard and Ruder, 2018; Radford
et al., 2018).
The advantage of these approaches
is that few parameters need to be learned from
scratch.
At least partly due to this advantage,
OpenAI GPT (Radford et al., 2018) achieved pre-
viously state-of-the-art results on many sentence-
level tasks from the GLUE benchmark (Wang
et al., 2018a).
Left-to-right language model-
BERT
BERT
E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
C
T1
T[SEP]
...
TN
T1’
...
TM’
[CLS]
Tok 1
 [SEP]
...
Tok N
Tok 1
...
TokM
Question
Paragraph
Start/End Span
BERT
E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
C
T1
T[SEP]
...
TN
T1’
...
TM’
[CLS]
Tok 1
 [SEP]
...
Tok N
Tok 1
...
TokM
Masked Sentence A
Masked Sentence B
Pre-training
Fine-Tuning
NSP
Mask LM
Mask LM
Unlabeled Sentence A and B Pair 
SQuAD
Question Answer Pair
NER
MNLI
Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT.
Apart from output layers, the same architec-
tures are used in both pre-training and ﬁne-tuning.
The same pre-trained model parameters are used to initialize
models for different down-stream tasks.
During ﬁne-tuning, all parameters are ﬁne-tuned.
[CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g.
separating ques-
tions/answers).
ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
2.3
Transfer Learning from Supervised Data
There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017).
Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to ﬁne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
3
BERT
We introduce BERT and its detailed implementa-
tion in this section.
There are two steps in our
framework: pre-training and ﬁne-tuning.
Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks.
For ﬁne-
tuning, the BERT model is ﬁrst initialized with
the pre-trained parameters, and all of the param-
eters are ﬁne-tuned using labeled data from the
downstream tasks.
Each downstream task has sep-
arate ﬁne-tuned models, even though they are ini-
tialized with the same pre-trained parameters.
The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniﬁed ar-
chitecture across different tasks.
There is mini-
mal difference between the pre-trained architec-
ture and the ﬁnal downstream architecture.
Model Architecture
BERT’s model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al.
(2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al.
(2017) as well as excellent guides
such as “The Annotated Transformer.”2
In this work, we denote the number of layers
(i.e., Transformer blocks) as L, the hidden size as
H, and the number of self-attention heads as A.3
We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
1https://github.com/tensorﬂow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/ﬁlter size to be 4H,
i.e., 3072 for the H = 768 and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-
Input/Output Representations
To make BERT
handle a variety of down-stream tasks, our input
representation is able to unambiguously represent
both a single sentence and a pair of sentences
(e.g., ⟨Question, Answer ⟩) in one token sequence.
Throughout this work, a “sentence” can be an arbi-
trary span of contiguous text, rather than an actual
linguistic sentence.
A “sequence” refers to the in-
put token sequence to BERT, which may be a sin-
gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al.,
2016) with a 30,000 token vocabulary.
The ﬁrst
token of every sequence is always a special clas-
siﬁcation token ([CLS]).
The ﬁnal hidden state
corresponding to this token is used as the ag-
gregate sequence representation for classiﬁcation
tasks.
Sentence pairs are packed together into a
single sequence.
We differentiate the sentences in
two ways.
First, we separate them with a special
token ([SEP]).
Second, we add a learned embed-
ding to every token indicating whether it belongs
to sentence A or sentence B.
As shown in Figure 1,
we denote input embedding as E, the ﬁnal hidden
vector of the special [CLS] token as C ∈RH,
and the ﬁnal hidden vector for the ith input token
as Ti ∈RH.
For a given token, its input representation is
constructed by summing the corresponding token,
segment, and position embeddings.
A visualiza-
tion of this construction can be seen in Figure 2.
3.1
Pre-training BERT
Unlike Peters et al.
(2018a) and Radford et al.
(2018), we do not use traditional left-to-right or
right-to-left language models to pre-train BERT.
Instead, we pre-train BERT using two unsuper-
vised tasks, described in this section.
This step
is presented in the left part of Figure 1.
Task #1: Masked LM
Intuitively, it is reason-
able to believe that a deep bidirectional model is
strictly more powerful than either a left-to-right
model or the shallow concatenation of a left-to-
right and a right-to-left model.
Unfortunately,
standard conditional language models can only be
trained left-to-right or right-to-left, since bidirec-
tional conditioning would allow each word to in-
directly “see itself”, and the model could trivially
predict the target word in a multi-layered context.
former is often referred to as a “Transformer encoder” while
the left-context-only version is referred to as a “Transformer
decoder” since it can be used for text generation.
In order to train a deep bidirectional representa-
tion, we simply mask some percentage of the input
tokens at random, and then predict those masked
tokens.
We refer to this procedure as a “masked
LM” (MLM), although it is often referred to as a
Cloze task in the literature (Taylor, 1953).
In this
case, the ﬁnal hidden vectors corresponding to the
mask tokens are fed into an output softmax over
the vocabulary, as in a standard LM.
In all of our
experiments, we mask 15% of all WordPiece to-
kens in each sequence at random.
In contrast to
denoising auto-encoders (Vincent et al., 2008), we
only predict the masked words rather than recon-
structing the entire input.
Although this allows us to obtain a bidirec-
tional pre-trained model, a downside is that we
are creating a mismatch between pre-training and
ﬁne-tuning, since the [MASK] token does not ap-
pear during ﬁne-tuning.
To mitigate this, we do
not always replace “masked” words with the ac-
tual [MASK] token.
The training data generator
chooses 15% of the token positions at random for
prediction.
If the i-th token is chosen, we replace
the i-th token with (1) the [MASK] token 80% of
the time (2) a random token 10% of the time (3)
the unchanged i-th token 10% of the time.
Then,
Ti will be used to predict the original token with
cross entropy loss.
We compare variations of this
procedure in Appendix C.2.
Task #2:
Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques-
tion Answering (QA) and Natural Language Infer-
ence (NLI) are based on understanding the rela-
tionship between two sentences, which is not di-
rectly captured by language modeling.
In order
to train a model that understands sentence rela-
tionships, we pre-train for a binarized next sen-
tence prediction task that can be trivially gener-
ated from any monolingual corpus.
Speciﬁcally,
when choosing the sentences A and B for each pre-
training example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext),
and 50% of the time it is a random sentence from
the corpus (labeled as NotNext).
As we show
in Figure 1, C is used for next sentence predic-
tion (NSP).5 Despite its simplicity, we demon-
strate in Section 5.1 that pre-training towards this
task is very beneﬁcial to both QA and NLI.
6
5The ﬁnal model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation
without ﬁne-tuning, since it was trained with NSP.
[CLS]
he
likes
play
##ing
[SEP]
my
dog
is
cute
[SEP]
Input
E[CLS]
Ehe
Elikes
Eplay
E##ing
E[SEP]
Emy
Edog
Eis
Ecute
E[SEP]
Token
Embeddings
EA
EB
EB
EB
EB
EB
EA
EA
EA
EA
EA
Segment
Embeddings
E0
E6
E7
E8
E9
E10
E1
E2
E3
E4
E5
Position
Embeddings
Figure 2: BERT input representation.
The input embeddings are the sum of the token embeddings, the segmenta-
tion embeddings and the position embeddings.
The NSP task is closely related to representation-
learning objectives used in Jernite et al.
(2017) and
Logeswaran and Lee (2018).
However, in prior
work, only sentence embeddings are transferred to
down-stream tasks, where BERT transfers all pa-
rameters to initialize end-task model parameters.
Pre-training data The pre-training procedure
largely follows the existing literature on language
model pre-training.
For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al.,
2015) and English Wikipedia (2,500M words).
For Wikipedia we extract only the text passages
and ignore lists, tables, and headers.
It is criti-
cal to use a document-level corpus rather than a
shufﬂed sentence-level corpus such as the Billion
Word Benchmark (Chelba et al., 2013) in order to
extract long contiguous sequences.
3.2
Fine-tuning BERT
Fine-tuning is straightforward since the self-
attention mechanism in the Transformer al-
lows BERT to model many downstream tasks—
whether they involve single text or text pairs—by
swapping out the appropriate inputs and outputs.
For applications involving text pairs, a common
pattern is to independently encode text pairs be-
fore applying bidirectional cross attention, such
as Parikh et al.
(2016); Seo et al.
(2017).
BERT
instead uses the self-attention mechanism to unify
these two stages, as encoding a concatenated text
pair with self-attention effectively includes bidi-
rectional cross attention between two sentences.
For each task, we simply plug in the task-
speciﬁc inputs and outputs into BERT and ﬁne-
tune all the parameters end-to-end.
At the in-
put, sentence A and sentence B from pre-training
are analogous to (1) sentence pairs in paraphras-
ing, (2) hypothesis-premise pairs in entailment, (3)
question-passage pairs in question answering, and
(4) a degenerate text-∅pair in text classiﬁcation
or sequence tagging.
At the output, the token rep-
resentations are fed into an output layer for token-
level tasks, such as sequence tagging or question
answering, and the [CLS] representation is fed
into an output layer for classiﬁcation, such as en-
tailment or sentiment analysis.
Compared to pre-training, ﬁne-tuning is rela-
tively inexpensive.
All of the results in the pa-
per can be replicated in at most 1 hour on a sin-
gle Cloud TPU, or a few hours on a GPU, starting
from the exact same pre-trained model.7 We de-
scribe the task-speciﬁc details in the correspond-
ing subsections of Section 4.
More details can be
found in Appendix A.5.
4
Experiments
In this section, we present BERT ﬁne-tuning re-
sults on 11 NLP tasks.
4.1
GLUE
The General Language Understanding Evaluation
(GLUE) benchmark (Wang et al., 2018a) is a col-
lection of diverse natural language understanding
tasks.
Detailed descriptions of GLUE datasets are
included in Appendix B.1.
To ﬁne-tune on GLUE, we represent the input
sequence (for single sentence or sentence pairs)
as described in Section 3, and use the ﬁnal hid-
den vector C ∈RH corresponding to the ﬁrst
input token ([CLS]) as the aggregate representa-
tion.
The only new parameters introduced during
ﬁne-tuning are classiﬁcation layer weights W ∈
RK×H, where K is the number of labels.
We com-
pute a standard classiﬁcation loss with C and W,
i.e., log(softmax(CW T )).
7For example, the BERT SQuAD model can be trained in
around 30 minutes on a single Cloud TPU to achieve a Dev
F1 score of 91.0%.
8See (10) in https://gluebenchmark.com/faq.
System
MNLI-(m/mm)
QQP
QNLI
SST-2
CoLA
STS-B
MRPC
RTE
Average
392k
363k
108k
67k
8.5k
5.7k
3.5k
2.5k
-
Pre-OpenAI SOTA
80.6/80.1
66.1
82.3
93.2
35.0
81.0
86.0
61.7
74.0
BiLSTM+ELMo+Attn
76.4/76.1
64.8
79.8
90.4
36.0
73.3
84.9
56.8
71.0
OpenAI GPT
82.1/81.4
70.3
87.4
91.3
45.4
80.0
82.3
56.0
75.1
BERTBASE
84.6/83.4
71.2
90.5
93.5
52.1
85.8
88.9
66.4
79.6
BERTLARGE
86.7/85.9
72.1
92.7
94.9
60.5
86.5
89.3
70.1
82.1
Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).
The number below each task denotes the number of training examples.
The “Average” column is slightly different
than the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-
model, single task.
F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and
accuracy scores are reported for the other tasks.
We exclude entries that use BERT as one of their components.
We use a batch size of 32 and ﬁne-tune for 3
epochs over the data for all GLUE tasks.
For each
task, we selected the best ﬁne-tuning learning rate
(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.
Additionally, for BERTLARGE we found that ﬁne-
tuning was sometimes unstable on small datasets,
so we ran several random restarts and selected the
best model on the Dev set.
With random restarts,
we use the same pre-trained checkpoint but per-
form different ﬁne-tuning data shufﬂing and clas-
siﬁer layer initialization.9
Results are presented in Table 1.
Both
BERTBASE and BERTLARGE outperform all sys-
tems on all tasks by a substantial margin, obtaining
4.5% and 7.0% respective average accuracy im-
provement over the prior state of the art.
Note that
BERTBASE and OpenAI GPT are nearly identical
in terms of model architecture apart from the at-
tention masking.
For the largest and most widely
reported GLUE task, MNLI, BERT obtains a 4.6%
absolute accuracy improvement.
On the ofﬁcial
GLUE leaderboard10, BERTLARGE obtains a score
of 80.5, compared to OpenAI GPT, which obtains
72.8 as of the date of writing.
We ﬁnd that BERTLARGE signiﬁcantly outper-
forms BERTBASE across all tasks, especially those
with very little training data.
The effect of model
size is explored more thoroughly in Section 5.2.
4.2
SQuAD v1.1
The
Stanford
Question
Answering
Dataset
(SQuAD v1.1) is a collection of 100k crowd-
sourced question/answer pairs (Rajpurkar et al.,
2016).
Given a question and a passage from
9The GLUE data set distribution does not include the Test
labels, and we only made a single GLUE evaluation server
submission for each of BERTBASE and BERTLARGE.
10https://gluebenchmark.com/leaderboard
Wikipedia containing the answer, the task is to
predict the answer text span in the passage.
As shown in Figure 1, in the question answer-
ing task, we represent the input question and pas-
sage as a single packed sequence, with the ques-
tion using the A embedding and the passage using
the B embedding.
We only introduce a start vec-
tor S ∈RH and an end vector E ∈RH during
ﬁne-tuning.
The probability of word i being the
start of the answer span is computed as a dot prod-
uct between Ti and S followed by a softmax over
all of the words in the paragraph: Pi =
eS·Ti
P
j eS·Tj .
The analogous formula is used for the end of the
answer span.
The score of a candidate span from
position i to position j is deﬁned as S·Ti + E·Tj,
and the maximum scoring span where j ≥i is
used as a prediction.
The training objective is the
sum of the log-likelihoods of the correct start and
end positions.
We ﬁne-tune for 3 epochs with a
learning rate of 5e-5 and a batch size of 32.
Table 2 shows top leaderboard entries as well
as results from top published systems (Seo et al.,
2017; Clark and Gardner, 2018; Peters et al.,
2018a; Hu et al., 2018).
The top results from the
SQuAD leaderboard do not have up-to-date public
system descriptions available,11 and are allowed to
use any public data when training their systems.
We therefore use modest data augmentation in
our system by ﬁrst ﬁne-tuning on TriviaQA (Joshi
et al., 2017) befor ﬁne-tuning on SQuAD.
Our best performing system outperforms the top
leaderboard system by +1.5 F1 in ensembling and
+1.3 F1 as a single system.
In fact, our single
BERT model outperforms the top ensemble sys-
tem in terms of F1 score.
Without TriviaQA ﬁne-
11QANet is described in Yu et al.
(2018), but the system
has improved substantially after publication.
System
Dev
Test
EM
F1
EM
F1
Top Leaderboard Systems (Dec 10th, 2018)
Human
-
-
82.3 91.2
#1 Ensemble - nlnet
-
-
86.0 91.7
#2 Ensemble - QANet
-
-
84.5 90.5
Published
BiDAF+ELMo (Single)
-
85.6
-
85.8
R.M.
Reader (Ensemble)
81.2 87.9 82.3 88.5
Ours
BERTBASE (Single)
80.8 88.5
-
-
BERTLARGE (Single)
84.1 90.9
-
-
BERTLARGE (Ensemble)
85.8 91.8
-
-
BERTLARGE (Sgl.+TriviaQA)
84.2 91.1 85.1 91.8
BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2
Table 2:
SQuAD 1.1 results.
The BERT ensemble
is 7x systems which use different pre-training check-
points and ﬁne-tuning seeds.
System
Dev
Test
EM
F1
EM
F1
Top Leaderboard Systems (Dec 10th, 2018)
Human
86.3 89.0 86.9 89.5
#1 Single - MIR-MRC (F-Net)
-
-
74.8 78.0
#2 Single - nlnet
-
-
74.2 77.1
Published
unet (Ensemble)
-
-
71.4 74.9
SLQA+ (Single)
-
71.4 74.4
Ours
BERTLARGE (Single)
78.7 81.9 80.0 83.1
Table 3: SQuAD 2.0 results.
We exclude entries that
use BERT as one of their components.
tuning data, we only lose 0.1-0.4 F1, still outper-
forming all existing systems by a wide margin.12
4.3
SQuAD v2.0
The SQuAD 2.0 task extends the SQuAD 1.1
problem deﬁnition by allowing for the possibility
that no short answer exists in the provided para-
graph, making the problem more realistic.
We use a simple approach to extend the SQuAD
v1.1 BERT model for this task.
We treat ques-
tions that do not have an answer as having an an-
swer span with start and end at the [CLS] to-
ken.
The probability space for the start and end
answer span positions is extended to include the
position of the [CLS] token.
For prediction, we
compare the score of the no-answer span: snull =
S·C + E·C to the score of the best non-null span
12The TriviaQA data we used consists of paragraphs from
TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,
that contain at least one of the provided possible answers.
System
Dev
Test
ESIM+GloVe
51.9 52.7
ESIM+ELMo
59.1 59.2
OpenAI GPT
-
78.0
BERTBASE
81.6
-
BERTLARGE
86.6 86.3
Human (expert)†
-
85.0
Human (5 annotations)†
-
88.0
Table 4: SWAG Dev and Test accuracies.
†Human per-
formance is measured with 100 samples, as reported in
the SWAG paper.
ˆ
si,j = maxj≥iS·Ti + E·Tj.
We predict a non-null
answer when ˆ
si,j > snull + τ, where the thresh-
old τ is selected on the dev set to maximize F1.
We did not use TriviaQA data for this model.
We
ﬁne-tuned for 2 epochs with a learning rate of 5e-5
and a batch size of 48.
The results compared to prior leaderboard en-
tries and top published work (Sun et al., 2018;
Wang et al., 2018b) are shown in Table 3, exclud-
ing systems that use BERT as one of their com-
ponents.
We observe a +5.1 F1 improvement over
the previous best system.
4.4
SWAG
The Situations With Adversarial Generations
(SWAG) dataset contains 113k sentence-pair com-
pletion examples that evaluate grounded common-
sense inference (Zellers et al., 2018).
Given a sen-
tence, the task is to choose the most plausible con-
tinuation among four choices.
When ﬁne-tuning on the SWAG dataset, we
construct four input sequences, each containing
the concatenation of the given sentence (sentence
A) and a possible continuation (sentence B).
The
only task-speciﬁc parameters introduced is a vec-
tor whose dot product with the [CLS] token rep-
resentation C denotes a score for each choice
which is normalized with a softmax layer.
We ﬁne-tune the model for 3 epochs with a
learning rate of 2e-5 and a batch size of 16.
Re-
sults are presented in Table 4.
BERTLARGE out-
performs the authors’ baseline ESIM+ELMo sys-
tem by +27.1% and OpenAI GPT by 8.3%.
5
Ablation Studies
In this section, we perform ablation experiments
over a number of facets of BERT in order to better
understand their relative importance.
Additional
Dev Set
Tasks
MNLI-m QNLI MRPC SST-2 SQuAD
(Acc)
(Acc)
(Acc)
(Acc)
(F1)
BERTBASE
84.4
88.4
86.7
92.7
88.5
No NSP
83.9
84.9
86.5
92.6
87.9
LTR & No NSP
82.1
84.3
77.5
92.1
77.8
+ BiLSTM
82.1
84.1
75.7
91.6
84.9
Table 5: Ablation over the pre-training tasks using the
BERTBASE architecture.
“No NSP” is trained without
the next sentence prediction task.
“LTR & No NSP” is
trained as a left-to-right LM without the next sentence
prediction, like OpenAI GPT.
“+ BiLSTM” adds a ran-
domly initialized BiLSTM on top of the “LTR + No
NSP” model during ﬁne-tuning.
ablation studies can be found in Appendix C.
5.1
Effect of Pre-training Tasks
We demonstrate the importance of the deep bidi-
rectionality of BERT by evaluating two pre-
training objectives using exactly the same pre-
training data, ﬁne-tuning scheme, and hyperpa-
rameters as BERTBASE:
No NSP: A bidirectional model which is trained
using the “masked LM” (MLM) but without the
“next sentence prediction” (NSP) task.
LTR & No NSP: A left-context-only model which
is trained using a standard Left-to-Right (LTR)
LM, rather than an MLM.
The left-only constraint
was also applied at ﬁne-tuning, because removing
it introduced a pre-train/ﬁne-tune mismatch that
degraded downstream performance.
Additionally,
this model was pre-trained without the NSP task.
This is directly comparable to OpenAI GPT, but
using our larger training dataset, our input repre-
sentation, and our ﬁne-tuning scheme.
We ﬁrst examine the impact brought by the NSP
task.
In Table 5, we show that removing NSP
hurts performance signiﬁcantly on QNLI, MNLI,
and SQuAD 1.1.
Next, we evaluate the impact
of training bidirectional representations by com-
paring “No NSP” to “LTR & No NSP”.
The LTR
model performs worse than the MLM model on all
tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR
model will perform poorly at token predictions,
since the token-level hidden states have no right-
side context.
In order to make a good faith at-
tempt at strengthening the LTR system, we added
a randomly initialized BiLSTM on top.
This does
signiﬁcantly improve results on SQuAD, but the
results are still far worse than those of the pre-
trained bidirectional models.
The BiLSTM hurts
performance on the GLUE tasks.
We recognize that it would also be possible to
train separate LTR and RTL models and represent
each token as the concatenation of the two mod-
els, as ELMo does.
However: (a) this is twice as
expensive as a single bidirectional model; (b) this
is non-intuitive for tasks like QA, since the RTL
model would not be able to condition the answer
on the question; (c) this it is strictly less powerful
than a deep bidirectional model, since it can use
both left and right context at every layer.
5.2
Effect of Model Size
In this section, we explore the effect of model size
on ﬁne-tuning task accuracy.
We trained a number
of BERT models with a differing number of layers,
hidden units, and attention heads, while otherwise
using the same hyperparameters and training pro-
cedure as described previously.
Results on selected GLUE tasks are shown in
Table 6.
In this table, we report the average Dev
Set accuracy from 5 random restarts of ﬁne-tuning.
We can see that larger models lead to a strict ac-
curacy improvement across all four datasets, even
for MRPC which only has 3,600 labeled train-
ing examples, and is substantially different from
the pre-training tasks.
It is also perhaps surpris-
ing that we are able to achieve such signiﬁcant
improvements on top of models which are al-
ready quite large relative to the existing literature.
For example, the largest Transformer explored in
Vaswani et al.
(2017) is (L=6, H=1024, A=16)
with 100M parameters for the encoder, and the
largest Transformer we have found in the literature
is (L=64, H=512, A=2) with 235M parameters
(Al-Rfou et al., 2018).
By contrast, BERTBASE
contains 110M parameters and BERTLARGE con-
tains 340M parameters.
It has long been known that increasing the
model size will lead to continual improvements
on large-scale tasks such as machine translation
and language modeling, which is demonstrated
by the LM perplexity of held-out training data
shown in Table 6.
However, we believe that
this is the ﬁrst work to demonstrate convinc-
ingly that scaling to extreme model sizes also
leads to large improvements on very small scale
tasks, provided that the model has been sufﬁ-
ciently pre-trained.
Peters et al.
(2018b) presented
mixed results on the downstream task impact of
increasing the pre-trained bi-LM size from two
to four layers and Melamud et al.
(2016) men-
tioned in passing that increasing hidden dimen-
sion size from 200 to 600 helped, but increasing
further to 1,000 did not bring further improve-
ments.
Both of these prior works used a feature-
based approach — we hypothesize that when the
model is ﬁne-tuned directly on the downstream
tasks and uses only a very small number of ran-
domly initialized additional parameters, the task-
speciﬁc models can beneﬁt from the larger, more
expressive pre-trained representations even when
downstream task data is very small.
5.3
Feature-based Approach with BERT
All of the BERT results presented so far have used
the ﬁne-tuning approach, where a simple classiﬁ-
cation layer is added to the pre-trained model, and
all parameters are jointly ﬁne-tuned on a down-
stream task.
However, the feature-based approach,
where ﬁxed features are extracted from the pre-
trained model, has certain advantages.
First, not
all tasks can be easily represented by a Trans-
former encoder architecture, and therefore require
a task-speciﬁc model architecture to be added.
Second, there are major computational beneﬁts
to pre-compute an expensive representation of the
training data once and then run many experiments
with cheaper models on top of this representation.
In this section, we compare the two approaches
by applying BERT to the CoNLL-2003 Named
Entity Recognition (NER) task (Tjong Kim Sang
and De Meulder, 2003).
In the input to BERT, we
use a case-preserving WordPiece model, and we
include the maximal document context provided
by the data.
Following standard practice, we for-
mulate this as a tagging task but do not use a CRF
Hyperparams
Dev Set Accuracy
#L
#H #A LM (ppl) MNLI-m MRPC SST-2
3
768
12
5.84
77.9
79.8
88.4
6
768
3
5.24
80.6
82.2
90.7
6
768
12
4.68
81.9
84.8
91.3
12
768
12
3.99
84.4
86.7
92.9
12 1024
16
3.54
85.7
86.9
93.3
24 1024
16
3.23
86.6
87.8
93.7
Table 6:
Ablation over BERT model size.
#L = the
number of layers; #H = hidden size; #A = number of at-
tention heads.
“LM (ppl)” is the masked LM perplexity
of held-out training data.
System
Dev F1 Test F1
ELMo (Peters et al., 2018a)
95.7
92.2
CVT (Clark et al., 2018)
-
92.6
CSE (Akbik et al., 2018)
-
93.1
Fine-tuning approach
BERTLARGE
96.6
92.8
BERTBASE
96.4
92.4
Feature-based approach (BERTBASE)
Embeddings
91.0
-
Second-to-Last Hidden
95.6
-
Last Hidden
94.9
-
Weighted Sum Last Four Hidden
95.9
-
Concat Last Four Hidden
96.1
-
Weighted Sum All 12 Layers
95.5
-
Table 7: CoNLL-2003 Named Entity Recognition re-
sults.
Hyperparameters were selected using the Dev
set.
The reported Dev and Test scores are averaged over
5 random restarts using those hyperparameters.
layer in the output.
We use the representation of
the ﬁrst sub-token as the input to the token-level
classiﬁer over the NER label set.
To ablate the ﬁne-tuning approach, we apply the
feature-based approach by extracting the activa-
tions from one or more layers without ﬁne-tuning
any parameters of BERT.
These contextual em-
beddings are used as input to a randomly initial-
ized two-layer 768-dimensional BiLSTM before
the classiﬁcation layer.
Results are presented in Table 7.
BERTLARGE
performs competitively with state-of-the-art meth-
ods.
The best performing method concatenates the
token representations from the top four hidden lay-
ers of the pre-trained Transformer, which is only
0.3 F1 behind ﬁne-tuning the entire model.
This
demonstrates that BERT is effective for both ﬁne-
tuning and feature-based approaches.
Text summarization systems aim to generate nat-
ural language summaries that compress the infor-
mation in a longer text.
Approaches using neu-
ral networks have shown promising results on this
task with end-to-end models that encode a source
document and then decode it into an abstrac-
tive summary.
Current state-of-the-art neural ab-
stractive summarization models combine extrac-
tive and abstractive techniques by using pointer-
generator style models which can copy words
from the source document (Gu et al., 2016; See
et al., 2017).
These end-to-end models produce
ﬂuent abstractive summaries but have had mixed
success in content selection, i.e.
deciding what to
summarize, compared to fully extractive models.
There is an appeal to end-to-end models from a
modeling perspective; however, there is evidence
that when summarizing people follow a two-step
Source Document
german chancellor angela merkel [did] not [look] too
pleased about the weather during her [annual] easter
holiday [in italy.] as britain [basks] in [sunshine] and
temperatures of up to 21c, mrs merkel and her husband[,
chemistry professor joachim sauer,] had to settle for a
measly 12 degrees.
the chancellor and her [spouse] have
been spending easter on the small island of ischia, near
naples in the mediterranean for over a [decade.]
[not so sunny:]
angela merkel [and] her husband[,
chemistry professor joachim sauer,] are spotted on their
[annual] easter trip to the island of ischia[,] near naples[.
the] couple [traditionally] spend their holiday at the ﬁve-
star miramare spa hotel on the south of the island [,
which comes] with its own private beach [, and bal-
conies overlooking the] ocean [.]...
Reference
• angela merkel and husband spotted while on italian
island holiday.
.
.
.
Baseline Approach
• angela merkel and her husband, chemistry professor
joachim sauer, are spotted on their annual easter trip
to the island of ischia, near naples.
.
.
.
Bottom-Up Summarization
• angela merkel and her husband are spotted on their
easter trip to the island of ischia, near naples.
.
.
.
Figure 1: Example of two sentence summaries with and
without bottom-up attention.
The model does not al-
low copying of words in [gray], although it can gen-
erate words.
With bottom-up attention, we see more
explicit sentence compression, while without it whole
sentences are copied verbatim.
approach of ﬁrst selecting important phrases and
then paraphrasing them (Anderson and Hidi, 1988;
Jing and McKeown, 1999).
A similar argument
has been made for image captioning.
Ander-
son et al.
(2017) develop a state-of-the-art model
with a two-step approach that ﬁrst pre-computes
bounding boxes of segmented objects and then ap-
plies attention to these regions.
This so-called
bottom-up attention is inspired by neuroscience re-
search describing attention based on properties in-
arXiv:1808.10792v2  [cs.CL]  9 Oct 2018
herent to a stimulus (Buschman and Miller, 2007).
Motivated by this approach,
we consider
bottom-up attention for neural abstractive sum-
marization.
Our approach ﬁrst selects a selection
mask for the source document and then constrains
a standard neural model by this mask.
This
approach can better decide which phrases a model
should include in a summary, without sacriﬁcing
the ﬂuency advantages of neural abstractive sum-
marizers.
Furthermore, it requires much fewer
data to train, which makes it more adaptable to
new domains.
Our full model incorporates a separate content
selection system to decide on relevant aspects of
the source document.
We frame this selection task
as a sequence-tagging problem, with the objec-
tive of identifying tokens from a document that
are part of its summary.
We show that a con-
tent selection model that builds on contextual word
embeddings (Peters et al., 2018) can identify cor-
rect tokens with a recall of over 60%, and a pre-
cision of over 50%.
To incorporate bottom-up
attention into abstractive summarization models,
we employ masking to constrain copying words
to the selected parts of the text, which produces
grammatical outputs.
We additionally experiment
with multiple methods to incorporate similar con-
straints into the training process of more com-
plex end-to-end abstractive summarization mod-
els, either through multi-task learning or through
directly incorporating a fully differentiable mask.
Our experiments compare bottom-up attention
with several other state-of-the-art abstractive sys-
tems.
Compared to our baseline models of See
et al.
(2017) bottom-up attention leads to an im-
provement in ROUGE-L score on the CNN-Daily
Mail (CNN-DM) corpus from 36.4 to 38.3 while
being simpler to train.
We also see comparable or
better results than recent reinforcement-learning
based methods with our MLE trained system.
Fur-
thermore, we ﬁnd that the content selection model
is very data-efﬁcient and can be trained with less
than 1% of the original training data.
This pro-
vides opportunities for domain-transfer and low-
resource summarization.
We show that a summa-
rization model trained on CNN-DM and evalu-
ated on the NYT corpus can be improved by over 5
points in ROUGE-L with a content selector trained
on only 1,000 in-domain sentences.
2
Related Work
There is a tension in document summarization be-
tween staying close to the source document and
allowing compressive or abstractive modiﬁcation.
Many non-neural systems take a select and com-
press approach.
For example, Dorr et al.
(2003)
introduced a system that ﬁrst extracts noun and
verb phrases from the ﬁrst sentence of a news ar-
ticle and uses an iterative shortening algorithm to
compress it.
Recent systems such as Durrett et al.
(2016) also learn a model to select sentences and
then compress them.
In contrast, recent work in neural network based
data-driven extractive summarization has focused
on extracting and ordering full sentences (Cheng
and Lapata, 2016; Dlikman and Last, 2016).
Nal-
lapati et al.
(2016b) use a classiﬁer to determine
whether to include a sentence and a selector that
ranks the positively classiﬁed ones.
These meth-
ods often over-extract, but extraction at a word
level requires maintaining grammatically correct
output (Cheng and Lapata, 2016), which is difﬁ-
cult.
Interestingly, key phrase extraction while un-
grammatical often matches closely in content with
human-generated summaries (Bui et al., 2016).
A third approach is neural abstractive sum-
marization with sequence-to-sequence models
(Sutskever et al., 2014; Bahdanau et al., 2014).
These methods have been applied to tasks such as
headline generation (Rush et al., 2015) and article
summarization (Nallapati et al., 2016a).
Chopra
et al.
(2016) show that attention approaches that
are more speciﬁc to summarization can further im-
prove the performance of models.
Gu et al.
(2016)
were the ﬁrst to show that a copy mechanism, in-
troduced by Vinyals et al.
(2015), can combine
the advantages of both extractive and abstractive
summarization by copying words from the source.
See et al.
(2017) reﬁne this pointer-generator ap-
proach and use an additional coverage mechanism
(Tu et al., 2016) that makes a model aware of its
attention history to prevent repeated attention.
Most recently, reinforcement learning (RL) ap-
proaches that optimize objectives for summariza-
tion other than maximum likelihood have been
shown to further improve performance on these
tasks (Paulus et al., 2017; Li et al., 2018b; Celiky-
ilmaz et al., 2018).
Paulus et al.
(2017) approach
the coverage problem with an intra-attention in
which a decoder has an attention over previously
generated words.
However RL-based training can
be difﬁcult to tune and slow to train.
Our method
does not utilize RL training, although in theory
this approach can be adapted to RL methods.
Several
papers
also
explore
multi-pass
extractive-abstractive
summarization.
Nalla-
pati et al.
(2017) create a new source document
comprised of the important sentences from the
source and then train an abstractive system.
Liu
et al.
(2018) describe an extractive phase that
extracts full paragraphs and an abstractive one
that determines their order.
Finally Zeng et al.
(2016) introduce a mechanism that reads a source
document in two passes and uses the information
from the ﬁrst pass to bias the second.
Our method
differs in that we utilize a completely abstractive
model, biased with a powerful content selector.
Other recent work explores alternative ap-
proaches to content selection.
For example, Cohan
et al.
(2018) use a hierarchical attention to detect
relevant sections in a document, Li et al.
(2018a)
generate a set of keywords that is used to guide the
summarization process, and Pasunuru and Bansal
(2018) develop a loss-function based on whether
salient keywords are included in a summary.
Other
approaches investigate the content-selection at the
sentence-level.
Tan et al.
(2017) describe a graph-
based attention to attend to one sentence at a time,
Chen and Bansal (2018) ﬁrst extract full sentences
from a document and then compress them, and
Hsu et al.
(2018) modulate the attention based on
how likely a sentence is included in a summary.
Neural Summarization
Throughout this paper, we consider a set of pairs
of texts (X, Y) where x ∈X corresponds to
source tokens x1, .
.
.
, xn and y ∈Y to a summary
y1, .
.
.
, ym with m ≪n.
Abstractive summaries are generated one word
at a time.
At every time-step, a model is aware of
the previously generated words.
The problem is to
learn a function f(x) parametrized by θ that max-
imizes the probability of generating the correct
sequences.
Following previous work, we model
the abstractive summarization with an attentional
sequence-to-sequence model.
The attention distri-
bution p(aj|x, y1:j−1) for a decoding step j, cal-
culated within the neural network, represents an
embedded soft distribution over all of the source
tokens and can be interpreted as the current focus
of the model.
The model additionally has a copy mecha-
Source
Masked Source
Discovering state-of-the-art neural network architectures requires substantial effort of human experts.
Recently, there has been a growing interest in developing algorithmic solutions to automate the
manual process of architecture design.
The automatically searched architectures have achieved highly
competitive performance in tasks such as image classiﬁcation (Zoph & Le, 2017; Zoph et al., 2018;
Liu et al., 2018b;a; Real et al., 2018) and object detection (Zoph et al., 2018).
The best existing architecture search algorithms are computationally demanding despite their remark-
able performance.
For example, obtaining a state-of-the-art architecture for CIFAR-10 and ImageNet
required 2000 GPU days of reinforcement learning (RL) (Zoph et al., 2018) or 3150 GPU days of
evolution (Real et al., 2018).
Several approaches for speeding up have been proposed, such as impos-
ing a particular structure of the search space (Liu et al., 2018b;a), weights or performance prediction
for each individual architecture (Brock et al., 2018; Baker et al., 2018) and weight sharing/inheritance
across multiple architectures (Elsken et al., 2017; Pham et al., 2018b; Cai et al., 2018; Bender et al.,
2018), but the fundamental challenge of scalability remains.
An inherent cause of inefﬁciency for the
dominant approaches, e.g.
based on RL, evolution, MCTS (Negrinho & Gordon, 2017), SMBO (Liu
et al., 2018a) or Bayesian optimization (Kandasamy et al., 2018), is the fact that architecture search
is treated as a black-box optimization problem over a discrete domain, which leads to a large number
of architecture evaluations required.
In this work, we approach the problem from a different angle, and propose a method for efﬁcient
architecture search called DARTS (Differentiable ARchiTecture Search).
Instead of searching over
a discrete set of candidate architectures, we relax the search space to be continuous, so that the
architecture can be optimized with respect to its validation set performance by gradient descent.
The
data efﬁciency of gradient-based optimization, as opposed to inefﬁcient black-box search, allows
DARTS to achieve competitive performance with the state of the art using orders of magnitude
less computation resources.
It also outperforms another recent efﬁcient architecture search method,
ENAS (Pham et al., 2018b).
Notably, DARTS is simpler than many existing approaches as it does
not involve controllers (Zoph & Le, 2017; Baker et al., 2017; Zoph et al., 2018; Pham et al., 2018b;
Zhong et al., 2018), hypernetworks (Brock et al., 2018) or performance predictors (Liu et al., 2018a),
yet it is generic enough handle both convolutional and recurrent architectures.
The idea of searching architectures within a continuous domain is not new (Saxena & Verbeek, 2016;
Ahmed & Torresani, 2017; Veniat & Denoyer, 2017; Shin et al., 2018), but there are several major
∗Current afﬁliation: Google Brain.
1
arXiv:1806.09055v2  [cs.LG]  23 Apr 2019
Published as a conference paper at ICLR 2019
distinctions.
While prior works seek to ﬁne-tune a speciﬁc aspect of an architecture, such as ﬁlter
shapes or branching patterns in a convolutional network, DARTS is able to learn high-performance
architecture building blocks with complex graph topologies within a rich search space.
Moreover,
DARTS is not restricted to any speciﬁc architecture family, and is applicable to both convolutional
and recurrent networks.
In our experiments (Sect.
3) we show that DARTS is able to design a convolutional cell that achieves
2.76 ± 0.09% test error on CIFAR-10 for image classiﬁcation using 3.3M parameters, which is
competitive with the state-of-the-art result by regularized evolution (Real et al., 2018) obtained using
three orders of magnitude more computation resources.
The same convolutional cell also achieves
26.7% top-1 error when transferred to ImageNet (mobile setting), which is comparable to the best RL
method (Zoph et al., 2018).
On the language modeling task, DARTS efﬁciently discovers a recurrent
cell that achieves 55.7 test perplexity on Penn Treebank (PTB), outperforming both extensively tuned
LSTM (Melis et al., 2018) and all the existing automatically searched cells based on NAS (Zoph &
Le, 2017) and ENAS (Pham et al., 2018b).
Our contributions can be summarized as follows:
• We introduce a novel algorithm for differentiable network architecture search based on
bilevel optimization, which is applicable to both convolutional and recurrent architectures.
• Through extensive experiments on image classiﬁcation and language modeling tasks we show
that gradient-based architecture search achieves highly competitive results on CIFAR-10
and outperforms the state of the art on PTB.
This is a very interesting result, considering
that so far the best architecture search methods used non-differentiable search techniques,
e.g.
based on RL (Zoph et al., 2018) or evolution (Real et al., 2018; Liu et al., 2018b).
• We achieve remarkable efﬁciency improvement (reducing the cost of architecture discovery
to a few GPU days), which we attribute to the use of gradient-based optimization as opposed
to non-differentiable search techniques.
• We show that the architectures learned by DARTS on CIFAR-10 and PTB are transferable
to ImageNet and WikiText-2, respectively.
The implementation of DARTS is available at https://github.com/quark0/darts
2
DIFFERENTIABLE ARCHITECTURE SEARCH
We describe our search space in general form in Sect.
2.1, where the computation procedure for an
architecture (or a cell in it) is represented as a directed acyclic graph.
We then introduce a simple
continuous relaxation scheme for our search space which leads to a differentiable learning objective
for the joint optimization of the architecture and its weights (Sect.
2.2).
Finally, we propose an
approximation technique to make the algorithm computationally feasible and efﬁcient (Sect.
2.3).
2.1
SEARCH SPACE
Following Zoph et al.
(2018); Real et al.
(2018); Liu et al.
(2018a;b), we search for a computation
cell as the building block of the ﬁnal architecture.
The learned cell could either be stacked to form a
convolutional network or recursively connected to form a recurrent network.
A cell is a directed acyclic graph consisting of an ordered sequence of N nodes.
Each node x(i) is
a latent representation (e.g.
a feature map in convolutional networks) and each directed edge (i, j)
is associated with some operation o(i,j) that transforms x(i).
We assume the cell to have two input
nodes and a single output node.
For convolutional cells, the input nodes are deﬁned as the cell outputs
in the previous two layers (Zoph et al., 2018).
For recurrent cells, these are deﬁned as the input at
the current step and the state carried from the previous step.
The output of the cell is obtained by
applying a reduction operation (e.g.
concatenation) to all the intermediate nodes.
Each intermediate node is computed based on all of its predecessors:
x(j) =
X
i<j
o(i,j)(x(i))
(1)
2
Published as a conference paper at ICLR 2019
Figure 1: An overview of DARTS: (a) Operations on the edges are initially unknown.
(b) Continuous
relaxation of the search space by placing a mixture of candidate operations on each edge.
(c) Joint
optimization of the mixing probabilities and the network weights by solving a bilevel optimization
problem.
(d) Inducing the ﬁnal architecture from the learned mixing probabilities.
A special zero operation is also included to indicate a lack of connection between two nodes.
The
task of learning the cell therefore reduces to learning the operations on its edges.
2.2
CONTINUOUS RELAXATION AND OPTIMIZATION
Let O be a set of candidate operations (e.g., convolution, max pooling, zero) where each operation
represents some function o(·) to be applied to x(i).
To make the search space continuous, we relax
the categorical choice of a particular operation to a softmax over all possible operations:
¯o(i,j)(x) =
X
o∈O
exp(α(i,j)
o
)
P
o′∈O exp(α(i,j)
o′
)
o(x)
(2)
where the operation mixing weights for a pair of nodes (i, j) are parameterized by a vector α(i,j) of
dimension |O|.
The task of architecture search then reduces to learning a set of continuous variables
α =

α(i,j)	
, as illustrated in Fig.
1.
At the end of search, a discrete architecture can be obtained by
replacing each mixed operation ¯o(i,j) with the most likely operation, i.e., o(i,j) = argmaxo∈O α(i,j)
o
.
In the following, we refer to α as the (encoding of the) architecture.
After relaxation, our goal is to jointly learn the architecture α and the weights w within all the mixed
operations (e.g.
weights of the convolution ﬁlters).
Analogous to architecture search using RL (Zoph
& Le, 2017; Zoph et al., 2018; Pham et al., 2018b) or evolution (Liu et al., 2018b; Real et al., 2018)
where the validation set performance is treated as the reward or ﬁtness, DARTS aims to optimize the
validation loss, but using gradient descent.
Denote by Ltrain and Lval the training and the validation loss, respectively.
Both losses are deter-
mined not only by the architecture α, but also the weights w in the network.
The goal for architecture
search is to ﬁnd α∗that minimizes the validation loss Lval(w∗, α∗), where the weights w∗associated
with the architecture are obtained by minimizing the training loss w∗= argminw Ltrain(w, α∗).
This implies a bilevel optimization problem (Anandalingam & Friesz, 1992; Colson et al., 2007) with
α as the upper-level variable and w as the lower-level variable:
min
α
Lval(w∗(α), α)
(3)
s.t.
w∗(α) = argminw Ltrain(w, α)
(4)
The nested formulation also arises in gradient-based hyperparameter optimization (Maclaurin et al.,
2015; Pedregosa, 2016; Franceschi et al., 2018), which is related in a sense that the architecture α
could be viewed as a special type of hyperparameter, although its dimension is substantially higher
than scalar-valued hyperparameters such as the learning rate, and it is harder to optimize.
3
Published as a conference paper at ICLR 2019
Algorithm 1: DARTS – Differentiable Architecture Search
Create a mixed operation ¯o(i,j) parametrized by α(i,j) for each edge (i, j)
while not converged do
1.
Update architecture α by descending ∇αLval(w −ξ∇wLtrain(w, α), α)
(ξ = 0 if using ﬁrst-order approximation)
2.
Update weights w by descending ∇wLtrain(w, α)
Derive the ﬁnal architecture based on the learned α.
2.3
APPROXIMATE ARCHITECTURE GRADIENT
Evaluating the architecture gradient exactly can be prohibitive due to the expensive inner optimization.
We therefore propose a simple approximation scheme as follows:
∇αLval(w∗(α), α)
(5)
≈∇αLval(w −ξ∇wLtrain(w, α), α)
(6)
where w denotes the current weights maintained by the algorithm, and ξ is the learning rate for a step
of inner optimization.
The idea is to approximate w∗(α) by adapting w using only a single training
step, without solving the inner optimization (equation 4) completely by training until convergence.
Related techniques have been used in meta-learning for model transfer (Finn et al., 2017), gradient-
based hyperparameter tuning (Luketina et al., 2016) and unrolled generative adversarial networks
(Metz et al., 2017).
Note equation 6 will reduce to ∇αLval(w, α) if w is already a local optimum for
the inner optimization and thus ∇wLtrain(w, α) = 0.
The iterative procedure is outlined in Alg.
1.
While we are not currently aware of the convergence
guarantees for our optimization algorithm, in practice it is able to reach a ﬁxed point with a suitable
choice of ξ1.
We also note that when momentum is enabled for weight optimisation, the one-step
unrolled learning objective in equation 6 is modiﬁed accordingly and all of our analysis still applies.
Applying chain rule to the approximate architecture gradient (equation 6) yields
∇αLval(w′, α) −ξ∇2
α,wLtrain(w, α)∇w′Lval(w′, α)
(7)
where w′ = w−ξ∇wLtrain(w, α) denotes the weights for a one-step forward model.
The expression
above contains an expensive matrix-vector product in its second term.
Fortunately, the complexity
can be substantially reduced using the ﬁnite difference approximation.
Let ϵ be a small scalar2 and
w± = w ± ϵ∇w′Lval(w′, α).
Then:
∇2
α,wLtrain(w, α)∇w′Lval(w′, α) ≈∇αLtrain(w+, α) −∇αLtrain(w−, α)
2ϵ
(8)
Evaluating the ﬁnite difference requires only two forward passes for the weights and two backward
passes for α, and the complexity is reduced from O(|α||w|) to O(|α| + |w|).
First-order Approximation
When ξ = 0, the second-order derivative in equation 7 will disappear.
In this case, the architecture gradient is given by ∇αLval(w, α), corresponding to the simple heuristic
of optimizing the validation loss by assuming the current w is the same as w∗(α).
This leads to some
speed-up but empirically worse performance, according to our experimental results in Table 1 and
Table 2.
In the following, we refer to the case of ξ = 0 as the ﬁrst-order approximation, and refer to
the gradient formulation with ξ > 0 as the second-order approximation.
2.4
DERIVING DISCRETE ARCHITECTURES
To form each node in the discrete architecture, we retain the top-k strongest operations (from distinct
nodes) among all non-zero candidate operations collected from all the previous nodes.
The strength
of an operation is deﬁned as
exp(α(i,j)
o
)
P
o′∈O exp(α(i,j)
o′
).
To make our derived architecture comparable with
1A simple working strategy is to set ξ equal to the learning rate for w’s optimizer.
2We found ϵ = 0.01/∥∇w′Lval(w′, α)∥2 to be sufﬁciently accurate in all of our experiments.
4
Published as a conference paper at ICLR 2019
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Architecture ( )
2
1
0
1
2
3
Weights (w)
= 0.7
= 0.5
= 0.3
= 0
Figure 2: Learning dynamics of our iterative algorithm
when Lval(w, α) = αw −2α + 1 and Ltrain(w, α) =
w2−2αw+α2, starting from (α(0), w(0)) = (2, −2).
The
analytical solution for the corresponding bilevel optimiza-
tion problem is (α∗, w∗) = (1, 1), which is highlighted
in the red circle.
The dashed red line indicates the fea-
sible set where constraint equation 4 is satisﬁed exactly
(namely, weights in w are optimal for the given architec-
ture α).
The example shows that a suitable choice of ξ
helps to converge to a better local optimum.
those in the existing works, we use k = 2 for convolutional cells (Zoph et al., 2018; Liu et al., 2018a;
Real et al., 2018) and k = 1 for recurrent cells (Pham et al., 2018b).
The zero operations are excluded in the above for two reasons.
First, we need exactly k non-zero
incoming edges per node for fair comparison with the existing models.
Second, the strength of the
zero operations is underdetermined, as increasing the logits of zero operations only affects the scale
of the resulting node representations, and does not affect the ﬁnal classiﬁcation outcome due to the
presence of batch normalization (Ioffe & Szegedy, 2015).
3
EXPERIMENTS AND RESULTS
Our experiments on CIFAR-10 and PTB consist of two stages, architecture search (Sect.
3.1) and
architecture evaluation (Sect.
3.2).
In the ﬁrst stage, we search for the cell architectures using DARTS,
and determine the best cells based on their validation performance.
In the second stage, we use these
cells to construct larger architectures, which we train from scratch and report their performance on
the test set.
We also investigate the transferability of the best cells learned on CIFAR-10 and PTB by
evaluating them on ImageNet and WikiText-2 (WT2) respectively.
3.1
ARCHITECTURE SEARCH
3.1.1
SEARCHING FOR CONVOLUTIONAL CELLS ON CIFAR-10
We include the following operations in O: 3 × 3 and 5 × 5 separable convolutions, 3 × 3 and 5 × 5
dilated separable convolutions, 3 × 3 max pooling, 3 × 3 average pooling, identity, and zero.
All
operations are of stride one (if applicable) and the convolved feature maps are padded to preserve
their spatial resolution.
We use the ReLU-Conv-BN order for convolutional operations, and each
separable convolution is always applied twice (Zoph et al., 2018; Real et al., 2018; Liu et al., 2018a).
Our convolutional cell consists of N = 7 nodes, among which the output node is deﬁned as the
depthwise concatenation of all the intermediate nodes (input nodes excluded).
The rest of the setup
follows Zoph et al.
(2018); Liu et al.
(2018a); Real et al.
(2018), where a network is then formed by
stacking multiple cells together.
The ﬁrst and second nodes of cell k are set equal to the outputs of cell
k−2 and cell k−1, respectively, and 1×1 convolutions are inserted as necessary.
Cells located at the
1/3 and 2/3 of the total depth of the network are reduction cells, in which all the operations adjacent
to the input nodes are of stride two.
The architecture encoding therefore is (αnormal, αreduce), where
αnormal is shared by all the normal cells and αreduce is shared by all the reduction cells.
Detailed experimental setup for this section can be found in Sect.
A.1.1.
3.1.2
SEARCHING FOR RECURRENT CELLS ON PENN TREEBANK
Our set of available operations includes linear transformations followed by one of tanh, relu, sigmoid
activations, as well as the identity mapping and the zero operation.
The choice of these candidate
operations follows Zoph & Le (2017); Pham et al.
(2018b).
Our recurrent cell consists of N = 12 nodes.
The very ﬁrst intermediate node is obtained by linearly
transforming the two input nodes, adding up the results and then passing through a tanh activation
5
Published as a conference paper at ICLR 2019
0
10
20
GPU Hours
8
9
10
11
12
13
14
15
Best Valid Error So Far (%)
CIFAR-10
DARTS (run 1)
DARTS (run 2)
DARTS (run 3)
DARTS (run 4)
AmoebaNet-A
NASNet-A
0
5
10
15
20
25
GPU Hours
8
9
10
11
12
13
14
15
Valid Error (%)
CIFAR-10
DARTS
AmoebaNet-A
NASNet-A
0
1
2
3
4
GPU Hours
63
64
65
66
67
68
69
70
71
Best Valid Perplexity So Far
Penn Treebank
DARTS (run 1)
DARTS (run 2)
DARTS (run 3)
DARTS (run 4)
ENAS
0
1
2
3
4
GPU Hours
63
64
65
66
67
68
69
70
71
Valid Perplexity
Penn Treebank
DARTS
ENAS
Figure 3: Search progress of DARTS for convolutional cells on CIFAR-10 and recurrent cells on
Penn Treebank.
We keep track of the most recent architectures over time.
Each architecture snapshot
is re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs on
PTB) and then evaluated on the validation set.
For each task, we repeat the experiments for 4 times
with different random seeds, and report the median and the best (per run) validation performance of
the architectures over time.
As references, we also report the results (under the same evaluation setup;
with comparable number of parameters) of the best existing cells discovered using RL or evolution,
including NASNet-A (Zoph et al., 2018) (2000 GPU days), AmoebaNet-A (3150 GPU days) (Real
et al., 2018) and ENAS (0.5 GPU day) (Pham et al., 2018b).
function, as done in the ENAS cell (Pham et al., 2018b).
The rest of the cell is learned.
Other settings
are similar to ENAS, where each operation is enhanced with a highway bypass (Zilly et al., 2016) and
the cell output is deﬁned as the average of all the intermediate nodes.
As in ENAS, we enable batch
normalization in each node to prevent gradient explosion during architecture search, and disable it
during architecture evaluation.
Our recurrent network consists of only a single cell, i.e.
we do not
assume any repetitive patterns within the recurrent architecture.
Detailed experimental setup for this section can be found in Sect.
A.1.2.
c_{k-2}
0
sep_conv_3x3
1
sep_conv_3x3
2
skip_connect
3
skip_connect
c_{k-1}
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
dil_conv_3x3
c_{k}
Figure 4: Normal cell learned on CIFAR-10.
c_{k-2}
0
max_pool_3x3
2
max_pool_3x3
c_{k-1}
max_pool_3x3
1
max_pool_3x3
3
max_pool_3x3
skip_connect
skip_connect
skip_connect
c_{k}
Figure 5: Reduction cell learned on CIFAR-10.
x_{t}
0
h_{t-1}
1
sigmoid
2
relu
3
relu
4
identity
h_{t}
5
tanh
7
tanh
6
sigmoid
8
relu
Figure 6: Recurrent cell learned on PTB.
3.2
ARCHITECTURE EVALUATION
To determine the architecture for ﬁnal evaluation, we run DARTS four times with different random
seeds and pick the best cell based on its validation performance obtained by training from scratch for
6
Published as a conference paper at ICLR 2019
a short period (100 epochs on CIFAR-10 and 300 epochs on PTB).
This is particularly important for
recurrent cells, as the optimization outcomes can be initialization-sensitive (Fig.
3).
To evaluate the selected architecture, we randomly initialize its weights (weights learned during the
search process are discarded), train it from scratch, and report its performance on the test set.
We
note the test set is never used for architecture search or architecture selection.
Detailed experimental setup for architecture evaluation on CIFAR-10 and PTB can be found in
Sect.
A.2.1 and Sect.
A.2.2, respectively.
Besides CIFAR-10 and PTB, we further investigated the
transferability of our best convolutional cell (searched on CIFAR-10) and recurrent cell (searched on
PTB) by evaluating them on ImageNet (mobile setting) and WikiText-2, respectively.
More details of
the transfer learning experiments can be found in Sect.
A.2.3 and Sect.
A.2.4.
Table 1: Comparison with state-of-the-art image classiﬁers on CIFAR-10 (lower error rate is better).
Note the search cost for DARTS does not include the selection cost (1 GPU day) or the ﬁnal evaluation
cost by training the selected architecture from scratch (1.5 GPU days).
Architecture
Test Error
Params
Search Cost
#ops
Search
(%)
(M)
(GPU days)
Method
DenseNet-BC (Huang et al., 2017)
3.46
25.6
–
–
manual
NASNet-A + cutout (Zoph et al., 2018)
2.65
3.3
2000
13
RL
NASNet-A + cutout (Zoph et al., 2018)†
2.83
3.1
2000
13
RL
BlockQNN (Zhong et al., 2018)
3.54
39.8
96
8
RL
AmoebaNet-A (Real et al., 2018)
3.34 ± 0.06
3.2
3150
19
evolution
AmoebaNet-A + cutout (Real et al., 2018)†
3.12
3.1
3150
19
evolution
AmoebaNet-B + cutout (Real et al., 2018)
2.55 ± 0.05
2.8
3150
19
evolution
Hierarchical evolution (Liu et al., 2018b)
3.75 ± 0.12
15.7
300
6
evolution
PNAS (Liu et al., 2018a)
3.41 ± 0.09
3.2
225
8
SMBO
ENAS + cutout (Pham et al., 2018b)
2.89
4.6
0.5
6
RL
ENAS + cutout (Pham et al., 2018b)*
2.91
4.2
4
6
RL
Random search baseline‡ + cutout
3.29 ± 0.15
3.2
4
7
random
DARTS (ﬁrst order) + cutout
3.00 ± 0.14
3.3
1.5
7
gradient-based
DARTS (second order) + cutout
2.76 ± 0.09
3.3
4
7
gradient-based
evaluation is chosen according to the same selection protocol as for DARTS.
Table 2: Comparison with state-of-the-art language models on PTB (lower perplexity is better).
Note
the search cost for DARTS does not include the selection cost (1 GPU day) or the ﬁnal evaluation
cost by training the selected architecture from scratch (3 GPU days).
Architecture
Perplexity
Params
Search Cost
#ops
Search
valid
test
(M)
(GPU days)
Method
Variational RHN (Zilly et al., 2016)
67.9
65.4
23
–
–
manual
LSTM (Merity et al., 2018)
60.7
58.8
24
–
–
manual
LSTM + skip connections (Melis et al., 2018)
60.9
58.3
24
–
–
manual
LSTM + 15 softmax experts (Yang et al., 2018)
58.1
56.0
22
–
–
manual
NAS (Zoph & Le, 2017)
–
64.0
25
1e4 CPU days
4
RL
ENAS (Pham et al., 2018b)*
68.3
63.1
24
0.5
4
RL
ENAS (Pham et al., 2018b)†
60.8
58.6
24
0.5
4
RL
Random search baseline‡
61.8
59.4
23
2
4
random
DARTS (ﬁrst order)
60.2
57.6
23
0.5
4
gradient-based
DARTS (second order)
58.1
55.7
23
1
4
gradient-based
7
Published as a conference paper at ICLR 2019
Table 3: Comparison with state-of-the-art image classiﬁers on ImageNet in the mobile setting.
Architecture
Test Error (%)
Params
+×
Search Cost
Search
top-1
top-5
(M)
(M)
(GPU days)
Method
Inception-v1 (Szegedy et al., 2015)
30.2
10.1
6.6
1448
–
manual
MobileNet (Howard et al., 2017)
29.4
10.5
4.2
569
–
manual
ShufﬂeNet 2× (g = 3) (Zhang et al., 2017)
26.3
–
∼5
524
–
manual
NASNet-A (Zoph et al., 2018)
26.0
8.4
5.3
564
2000
RL
NASNet-B (Zoph et al., 2018)
27.2
8.7
5.3
488
2000
RL
NASNet-C (Zoph et al., 2018)
27.5
9.0
4.9
558
2000
RL
AmoebaNet-A (Real et al., 2018)
25.5
8.0
5.1
555
3150
evolution
AmoebaNet-B (Real et al., 2018)
26.0
8.5
5.3
555
3150
evolution
AmoebaNet-C (Real et al., 2018)
24.3
7.6
6.4
570
3150
evolution
PNAS (Liu et al., 2018a)
25.8
8.1
5.1
588
∼225
SMBO
DARTS (searched on CIFAR-10)
26.7
8.7
4.7
574
4
gradient-based
3.3
RESULTS ANALYSIS
The CIFAR-10 results for convolutional architectures are presented in Table 1.
Notably, DARTS
achieved comparable results with the state of the art (Zoph et al., 2018; Real et al., 2018) while using
three orders of magnitude less computation resources (i.e.
1.5 or 4 GPU days vs 2000 GPU days for
NASNet and 3150 GPU days for AmoebaNet).
Moreover, with slightly longer search time, DARTS
outperformed ENAS (Pham et al., 2018b) by discovering cells with comparable error rates but less
parameters.
The longer search time is due to the fact that we have repeated the search process four
times for cell selection.
This practice is less important for convolutional cells however, because the
performance of discovered architectures does not strongly depend on initialization (Fig.
3).
Alternative Optimization Strategies
To better understand the necessity of bilevel optimization,
we investigated a simplistic search strategy, where α and w are jointly optimized over the union of
the training and validation sets using coordinate descent.
The resulting best convolutional cell (out of
4 runs) yielded 4.16 ± 0.16% test error using 3.1M parameters, which is worse than random search.
In the second experiment, we optimized α simultaneously with w (without alteration) using SGD,
again over all the data available (training + validation).
The resulting best cell yielded 3.56 ± 0.10%
test error using 3.0M parameters.
We hypothesize that these heuristics would cause α (analogous
to hyperparameters) to overﬁt the training data, leading to poor generalization.
Note that α is not
directly optimized on the training set in DARTS.
Table 2 presents the results for recurrent architectures on PTB, where a cell discovered by DARTS
achieved the test perplexity of 55.7.
This is on par with the state-of-the-art model enhanced by a
mixture of softmaxes (Yang et al., 2018), and better than all the rest of the architectures that are either
manually or automatically discovered.
Note that our automatically searched cell outperforms the
extensively tuned LSTM (Melis et al., 2018), demonstrating the importance of architecture search in
addition to hyperparameter search.
In terms of efﬁciency, the overall cost (4 runs in total) is within 1
GPU day, which is comparable to ENAS and signiﬁcantly faster than NAS (Zoph & Le, 2017).
It is also interesting to note that random search is competitive for both convolutional and recurrent
models, which reﬂects the importance of the search space design.
Nevertheless, with comparable or
less search cost, DARTS is able to signiﬁcantly improve upon random search in both cases (2.76 ±
0.09 vs 3.29 ± 0.15 on CIFAR-10; 55.7 vs 59.4 on PTB).
Results in Table 3 show that the cell learned on CIFAR-10 is indeed transferable to ImageNet.
It is
worth noticing that DARTS achieves competitive performance with the state-of-the-art RL method
(Zoph et al., 2018) while using three orders of magnitude less computation resources.
Table 4 shows that the cell identiﬁed by DARTS transfers to WT2 better than ENAS, although the
overall results are less strong than those presented in Table 2 for PTB.
The weaker transferability
between PTB and WT2 (as compared to that between CIFAR-10 and ImageNet) could be explained by
the relatively small size of the source dataset (PTB) for architecture search.
The issue of transferability
could potentially be circumvented by directly optimizing the architecture on the task of interest.
8
Published as a conference paper at ICLR 2019
Table 4: Comparison with state-of-the-art language models on WT2.
Architecture
Perplexity
Params
Search Cost
Search
valid
test
(M)
(GPU days)
Method
LSTM + augmented loss (Inan et al., 2017)
91.5
87.0
28
–
manual
LSTM + continuous cache pointer (Grave et al., 2016)
–
68.9
–
–
manual
LSTM (Merity et al., 2018)
69.1
66.0
33
–
manual
LSTM + skip connections (Melis et al., 2018)
69.1
65.9
24
–
manual
LSTM + 15 softmax experts (Yang et al., 2018)
66.0
63.3
33
–
manual
ENAS (Pham et al., 2018b)† (searched on PTB)
72.4
70.4
33
0.5
RL
DARTS (searched on PTB)
71.2
69.6
33
1
gradient-based
Semantic segmentation with the goal to assign semantic labels to every pixel in an
image [1,2,3,4,5] is one of the fundamental topics in computer vision.
Deep con-
volutional neural networks [6,7,8,9,10] based on the Fully Convolutional Neural
Network [8,11] show striking improvement over systems relying on hand-crafted
features [12,13,14,15,16,17] on benchmark tasks.
In this work, we consider two
types of neural networks that use spatial pyramid pooling module [18,19,20] or
encoder-decoder structure [21,22] for semantic segmentation, where the former
one captures rich contextual information by pooling features at diﬀerent resolu-
tion while the latter one is able to obtain sharp object boundaries.
In order to capture the contextual information at multiple scales, DeepLabv3
[23] applies several parallel atrous convolution with diﬀerent rates (called Atrous
arXiv:1802.02611v3  [cs.CV]  22 Aug 2018
2
L.-C Chen, Y.
Zhu, G.
Papandreou, F.
Schroﬀ, and H.
Adam
Image
Spatial Pyramid Pooling
0.5x
0.5x
0.5x
Prediction
8x
Image
2x
0.5x
0.5x
0.5x
0.5x
0.5x
2x
2x
2x
2x
Prediction
Image
Spatial Pyramid Pooling
0.5x
0.5x
0.5x
0.5x
4x
Prediction
4x
(a) Spatial Pyramid Pooling
(b) Encoder-Decoder
(c) Encoder-Decoder with Atrous Conv
Fig.
1.
We improve DeepLabv3, which employs the spatial pyramid pooling module (a),
with the encoder-decoder structure (b).
The proposed model, DeepLabv3+, contains
rich semantic information from the encoder module, while the detailed object bound-
aries are recovered by the simple yet eﬀective decoder module.
The encoder module
allows us to extract features at an arbitrary resolution by applying atrous convolution.
Spatial Pyramid Pooling, or ASPP), while PSPNet [24] performs pooling opera-
tions at diﬀerent grid scales.
Even though rich semantic information is encoded in
the last feature map, detailed information related to object boundaries is missing
due to the pooling or convolutions with striding operations within the network
backbone.
This could be alleviated by applying the atrous convolution to extract
denser feature maps.
However, given the design of state-of-art neural networks
[7,9,10,25,26] and limited GPU memory, it is computationally prohibitive to ex-
tract output feature maps that are 8, or even 4 times smaller than the input
resolution.
Taking ResNet-101 [25] for example, when applying atrous convolu-
tion to extract output features that are 16 times smaller than input resolution,
features within the last 3 residual blocks (9 layers) have to be dilated.
Even
worse, 26 residual blocks (78 layers!) will be aﬀected if output features that are
8 times smaller than input are desired.
Thus, it is computationally intensive if
denser output features are extracted for this type of models.
On the other hand,
encoder-decoder models [21,22] lend themselves to faster computation (since no
features are dilated) in the encoder path and gradually recover sharp object
boundaries in the decoder path.
Attempting to combine the advantages from
both methods, we propose to enrich the encoder module in the encoder-decoder
networks by incorporating the multi-scale contextual information.
In particular, our proposed model, called DeepLabv3+, extends DeepLabv3
[23] by adding a simple yet eﬀective decoder module to recover the object bound-
aries, as illustrated in Fig.
1.
The rich semantic information is encoded in the
output of DeepLabv3, with atrous convolution allowing one to control the den-
sity of the encoder features, depending on the budget of computation resources.
Furthermore, the decoder module allows detailed object boundary recovery.
Motivated by the recent success of depthwise separable convolution [27,28,26,29,30],
we also explore this operation and show improvement in terms of both speed and
accuracy by adapting the Xception model [26], similar to [31], for the task of
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
3
semantic segmentation, and applying the atrous separable convolution to both
the ASPP and decoder modules.
Finally, we demonstrate the eﬀectiveness of the
proposed model on PASCAL VOC 2012 and Cityscapes datasts and attain the
test set performance of 89.0% and 82.1% without any post-processing, setting a
new state-of-the-art.
In summary, our contributions are:
– We propose a novel encoder-decoder structure which employs DeepLabv3 as
a powerful encoder module and a simple yet eﬀective decoder module.
– In our structure, one can arbitrarily control the resolution of extracted en-
coder features by atrous convolution to trade-oﬀprecision and runtime,
which is not possible with existing encoder-decoder models.
– We adapt the Xception model for the segmentation task and apply depthwise
separable convolution to both ASPP module and decoder module, resulting
in a faster and stronger encoder-decoder network.
– Our proposed model attains a new state-of-art performance on PASCAL
VOC 2012 and Cityscapes datasets.
We also provide detailed analysis of
design choices and model variants.
– We make our Tensorﬂow-based implementation of the proposed model pub-
licly available at https://github.com/tensorflow/models/tree/master/
research/deeplab.
2
Related Work
Models based on Fully Convolutional Networks (FCNs) [8,11] have demonstrated
signiﬁcant improvement on several segmentation benchmarks [1,2,3,4,5].
There
are several model variants proposed to exploit the contextual information for
segmentation [12,13,14,15,16,17,32,33], including those that employ multi-scale
inputs (i.e., image pyramid) [34,35,36,37,38,39] or those that adopt probabilistic
graphical models (such as DenseCRF [40] with eﬃcient inference algorithm [41])
[42,43,44,37,45,46,47,48,49,50,51,39].
In this work, we mainly discuss about the
models that use spatial pyramid pooling and encoder-decoder structure.
Spatial pyramid pooling: Models, such as PSPNet [24] or DeepLab [39,23],
perform spatial pyramid pooling [18,19] at several grid scales (including image-
level pooling [52]) or apply several parallel atrous convolution with diﬀerent
rates (called Atrous Spatial Pyramid Pooling, or ASPP).
These models have
shown promising results on several segmentation benchmarks by exploiting the
multi-scale information.
Encoder-decoder: The encoder-decoder networks have been successfully
applied to many computer vision tasks, including human pose estimation [53], ob-
ject detection [54,55,56], and semantic segmentation [11,57,21,22,58,59,60,61,62,63,64].
Typically, the encoder-decoder networks contain (1) an encoder module that
gradually reduces the feature maps and captures higher semantic information,
and (2) a decoder module that gradually recovers the spatial information.
Build-
ing on top of this idea, we propose to use DeepLabv3 [23] as the encoder module
and add a simple yet eﬀective decoder module to obtain sharper segmentations.
4
L.-C Chen, Y.
Zhu, G.
Papandreou, F.
Schroﬀ, and H.
Adam
1x1 Conv
3x3 Conv
rate 6
3x3 Conv
rate 12
3x3 Conv
rate 18
Image
Pooling
1x1 Conv
1x1 Conv
Low-Level
Features
Upsample
by 4
Concat
3x3 Conv
Encoder
Decoder
Atrous Conv
DCNN
Image
Prediction
Upsample
by 4
Fig.
2.
Our proposed DeepLabv3+ extends DeepLabv3 by employing a encoder-
decoder structure.
The encoder module encodes multi-scale contextual information by
applying atrous convolution at multiple scales, while the simple yet eﬀective decoder
module reﬁnes the segmentation results along object boundaries.
Depthwise separable convolution: Depthwise separable convolution [27,28]
or group convolution [7,65], a powerful operation to reduce the computation cost
and number of parameters while maintaining similar (or slightly better) perfor-
mance.
This operation has been adopted in many recent neural network designs
[66,67,26,29,30,31,68].
In particular, we explore the Xception model [26], similar
to [31] for their COCO 2017 detection challenge submission, and show improve-
ment in terms of both accuracy and speed for the task of semantic segmentation.
The sequence-to-sequence framework has demonstrated success in natural-language sequence trans-
duction tasks such as machine translation.
More recently, neural techniques have been applied to do
single-document, abstractive (paraphrasing) text summarization of news articles (Rush et al.
(2015),
Nallapati et al.
(2016)).
In this prior work, the input to supervised models ranged from the ﬁrst sen-
tence to the entire text of an article, and they are trained end-to-end to predict reference summaries.
Doing this end-to-end requires a signiﬁcant number of parallel article-summary pairs since language
understanding is a pre-requisite to generate ﬂuent summaries.
In contrast, we consider the task of multi-document summarization, where the input is a collection
of related documents from which a summary is distilled.
Prior work has focused on extractive
summarization, which select sentences or phrases from the input to form the summaries, rather
than generating new text.
There has been limited application of abstractive neural methods and one
possible reason is the paucity of large, labeled datasets.
In this work, we consider English Wikipedia as a supervised machine learning task for multi-
document summarization where the input is comprised of a Wikipedia topic (title of article) and
a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text.
We
describe the ﬁrst attempt to abstractively generate the ﬁrst section, or lead, of Wikipedia articles con-
ditioned on reference text.
In addition to running strong baseline models on the task, we modify the
Transformer architecture (Vaswani et al., 2017) to only consist of a decoder, which performs better
in the case of longer input sequences compared to recurrent neural network (RNN) and Transformer
encoder-decoder models.
Finally we show our modeling improvements allow us to generate entire
Wikipedia articles.
∗Joint ﬁrst-authors.
Ordered randomly.
1
arXiv:1801.10198v1  [cs.CL]  30 Jan 2018
Published as a conference paper at ICLR 2018
2
RELATED WORK
2.1
OTHER DATASETS USED IN NEURAL ABSTRACTIVE SUMMARIZATION
Neural abstractive summarization was pioneered in Rush et al.
(2015), where they train headline
generation models using the English Gigaword corpus (Graff & Cieri, 2003), consisting of news
articles from number of publishers.
However, the task is more akin to sentence paraphrasing than
summarization as only the ﬁrst sentence of an article is used to predict the headline, another sen-
tence.
RNN-based encoder-decoder models with attention (seq2seq) perform very well on this task
in both ROUGE (Lin, 2004), an automatic metric often used in summarization, and human evalua-
tion (Chopra et al., 2016).
In Nallapati et al.
(2016), an abstractive summarization dataset is proposed by modifying a question-
answering dataset of news articles paired with story highlights from Daily Mail and CNN.
This task
is more difﬁcult than headline-generation because the information used in the highlights may come
from many parts of the article and not only the ﬁrst sentence.
One downside of the dataset is that it
has an order-of-magnitude fewer parallel examples (310k vs.
3.8M) to learn from.
Standard seq2seq
models with attention do less well, and a number of techniques are used to augment performance.
Another downside is that it is unclear what the guidelines are for creating story highlights and it is
obvious that there are signiﬁcant stylistic differences between the two news publishers.
In our work we also train neural abstractive models, but in the multi-document regime with
Wikipedia.
As can be seen in Table 1, the input and output text are generally much larger, with
signiﬁcant variance depending on the article.
The summaries (Wikipedia lead) are multiple sen-
tences and sometimes multiple paragraphs, written in a fairly uniform style as encouraged by the
Wikipedia Manual of Style1.
However, the input documents may consist of documents of arbitrary
style originating from arbitrary sources.
We also show in Table 1 the ROUGE-1 recall scores of the output given the input, which is the
proportion of unigrams/words in the output co-occuring in the input.
A higher score corresponds
to a dataset more amenable to extractive summarization.
In particular, if the output is completely
embedded somewhere in the input (e.g.
a wiki-clone), the score would be 100.
Given a score of
only 59.2 compared to 76.1 and 78.7 for other summarization datasets shows that ours is the least
amenable to purely extractive methods.
2.2
TASKS INVOLVING WIKIPEDIA
There is a rich body of work incorporating Wikipedia for machine learning tasks, including question-
answering (Hewlett et al.
(2016), Rajpurkar et al.
(2016)) and information extraction (Lehmann
et al., 2015), and text generation from structured data (Lebret et al., 2016).
The closest work to ours involving generating Wikipedia is Sauper & Barzilay (2009), where articles
are generated extractively (instead of abstractively in our case) from reference documents using
learned templates.
The Wikipedia articles are restricted to two categories, whereas we use all article
types.
The reference documents are obtained from a search engine, with the Wikipedia topic used as
query similar to our search engine references.
However we also show results with documents only
found in the References section of the Wikipedia articles.
2.3
TRANSFORMER MODELS
Previous work on neural abstractive summarization relies on RNNs as fundamental modules, mir-
roring techniques successful in machine translation (MT).
Recently, state-of-the-art MT results were
obtained using a non-recurrent architecture, called the Transformer (Vaswani et al., 2017).
The lack
of recurrence enables greater within-training-example parallelization, at the cost of quadratic com-
plexity in the input sequence length.
We ﬁnd the Transformer transfers well to medium length, input
sequence summarization and describe modiﬁcations to better handle longer sequences.
1https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style
2
Published as a conference paper at ICLR 2018
Table 1: Order of magnitude input/output sizes and unigram recall for summarization datasets.
Dataset
Input
Output
# examples
ROUGE-1 R
Gigaword (Graff & Cieri, 2003)
101
101
106
78.7
CNN/DailyMail (Nallapati et al., 2016)
102–103
101
105
76.1
WikiSum (ours)
102–106
101–103
106
59.2
Table 2: Percentiles for different aspects of WikiSum dataset.
Size is in number of words.
Percentile
20
40
50
60
80
100
Lead Size
37
62
78
98
166
10,034
Num Citations
1
2
2
3
5
1,029
Citations Size
562
1,467
2,296
3,592
10,320
6,159,463
Num Search Results
10
20
26
31
46
2,095
Search Results Size
1,1691
33,989
49,222
68,681
135,533
5,355,671
3
ENGLISH WIKIPEDIA AS A MULTI-DOCUMENT SUMMARIZATION DATASET
Wikipedia, being an encyclopedia, can be viewed as a collection of summaries on various topics
given by their title, e.g.
”Canada” or ”Machine Learning”.
The source material to be summarized
can be viewed as all reputable documents on the Web or books; however, to make the problem more
tractable we consider the following subsets of all documents, D:
1.
Cited sources: A Wikipedia article that conforms to the style guidelines should be well-
supported by citations found in the References section of Wikipedia articles.
For each
article, ai, we extract all text without markup from crawlable citation documents, Ci ⊂D,
to use as input to our method.
2.
Web Search results: To expand the collection of reference documents, we crawl the search
results from the Google search engine, using the article section titles as queries.
For each
query, we collect 10 result pages.
From this collection we remove the Wikipedia article
itself, which is often among the top results.
We also remove ”clones”, which are detected
when there is a high-level of unigram overlap with the article (details provided in A.2.1).
We denote these reﬁned search results for an article, ai, as Si ⊂D.
Similar to Ci, we
extract only the text to use as input.
Table 2 describes overall properties of our WikiSum dataset.
Many articles have few citations,
motivating our supplementation of the source documents with web search results.
On the other
hand, citations when available, tend to be of higher-quality.
When counting the total words in the
entire dataset, it is orders-of-magnitude larger than previous summarization datasets.
To have consistent train/development/test data across corpus-comparison experiments, we restrict
the articles to those with at least one crawlable citation.
We divide the articles roughly into 80/10/10
for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respec-
tively.
4
METHODS AND MODELS
Because the amount of text in input reference documents (Ci, Si) can be very large (see Table 2) it is
infeasible to train an end-to-end abstractive model given the memory constraints of current hardware.
Hence, we ﬁrst coarsely select a subset of the input using extractive summarization.
The second
stage involves training an abstractive model that generates the Wikipedia text while conditioning on
this extraction.
This two-stage process is inspired by by how humans might summarize multiple long
documents: First highlight pertinent information, then conditionally generate the summary based on
the highlights.
3
Published as a conference paper at ICLR 2018
4.1
EXTRACTIVE STAGE
We investigate three extractive methods from the summarization literature, along with a trivial and
cheating method, to assess the importance of this stage.
For each article, ai we create a ranked list
of paragraphs, {pi
Ri(j)}, occurring in (Ci, Si) where Ri(j) is the rank of the jth paragraph pi
j of
(Ci, Si).
From this we select the ﬁrst L tokens as input to the second abstractive stage.
1.
Identity: As a trivial baseline extractor, we simply use the ﬁrst L tokens of the input.
2.
tf-idf: A non-trivial ranking is to consider ranking paragraphs as documents in a query-
retrieval problem, where the query is the title of the article, T(ai).
We compute tf-idf
(Ramos et al., 2003) for the query, with respect to the documents, {pi
j}.
That is, we sum-
mate for each word in the query
Nw · log( Nd
Ndw
)
where Nw, Nd, and Ndw are the count of the word in the document, total number of docu-
ments, and total number of documents containing the word, respectively.
3.
TextRank (Mihalcea & Tarau, 2004): A weighted graph is deﬁned where text units are
nodes and edges are deﬁned by a similarity measure based on word overlap.
An algorithm
similar to PageRank (Page et al., 1999) is then used to compute the ranking of text units.
We used paragraphs for the text units.
4.
SumBasic (Nenkova & Vanderwende, 2005): Word frequencies in the input text are used to
assign scores to words, which are in turn used to score sentences.
After selecting the best
scoring sentence, words in it have their scores reduced, and the process is repeated until the
desired summary length is reached.
5.
Cheating To further demonstrate the quality of extraction on the ﬁnal performance, we
implement a cheating extractor that ranks {pi
j} using recall of bigrams in the ground truth
text:
d(pi
j, ai) = bigrams(pi
j) ∩bigrams(ai)
bigrams(ai)
(1)
4.2
ABSTRACTIVE STAGE
4.2.1
DATA REPRESENTATION
Given the ordered paragraphs {pi
Ri(j)}, we derive the raw text input simply as the concatenation of
the paragraphs in order, the most relevant at the beginning, and preﬁxed with the title.
We then encode the text using sub-word tokenization similar to Wu et al.
(2016) with a vocabulary
size of 32,000 yielding tokenized input, xi:
texti = T(ai)∥{pi
Ri(j)}
tokenize(texti) = xi = (x1
i , x2
i , ..., xni
i )
For various values of L in experiments, we truncate the tokens to form the input sequence:
mL
i = (x1
i , ...xmin(L,ni)
i
)
For the output, we use the same vocabulary and tokenization for the Wikipedia lead text but do not
do any truncation across experiments.
Next we describe the abstractive models, W, that learn to write articles, ai = W(mL
i ), which we
treat as a sequence transduction problem from very long input sequences (up to L = 11000) to
medium output sequences (typically less than 500).
4
Published as a conference paper at ICLR 2018
4.2.2
BASELINE MODELS
As a baseline we apply the standard LSTM encoder-decoder with attention (seq2seq-att) as in Bah-
danau et al.
(2014) to this task.
As is typical we train to optimize the maximum-likelihood objective:
yi = tokenize(ai)
N
Y
i=1
p(yi|mL
i )
A stronger, more recent baseline that we use is the non-recurrent Transformer model described in
2.3, which also has symmetric encoder and decoder modules (T-ED).
4.2.3
TRANSFORMER DECODER (T-D)
We introduce a simple but effective modiﬁcation to T-ED for long sequences that drops the encoder
module (almost reducing model parameters by half for a given hyper-parameter set), combines the
input and output sequences into a single ”sentence” and is trained as a standard language model.
That is, we convert a sequence-transduction example (m1, ..., mn) 7→(y1, ..., yη) into the sentence
(w1, ..., wn+η+1) = (m1, ..., mn, δ, y1, ..., yη), where δ is a special separator token and train a
model to predict the next word given the previous ones:
p(w1, ..., wn+η) =
n+η
Y
j=1
p(wi|w1, ..., wj−1)
Since the model is forced to predict the next token in the input, m, as well as y, error signals are prop-
agated from both input and output time-steps during training.
We also suspect that for monolingual
text-to-text tasks redundant information is re-learned about language in the encoder and decoder.
We believe this allows for easier optimization and empirically observe this with longer sequences
(see Section 5.3).
Note that because of the self-attention of the Transformer, when generating the
next token, attention from both m and y are considered.
At inference we provide the input sequence,
mi, initially, and auto-regressively generate the output, yi, as normal.
4.2.4
TRANSFORMER DECODER WITH MEMORY-COMPRESSED ATTENTION (T-DMCA)
To re-use the terminology used to describe the Transformer, the attention is a function of a query
(Q) and set of key (K) and value (V ) pairs.
To handle longer sequences, we modify the multi-head
self-attention of the Transformer to reduce memory usage by limiting the dot products between Q
and K in:
Attention(Q, K, V ) = softmax(QKT
√dk
)V
Local attention: Sequence tokens are divided into blocks of similar length and attention is per-
formed in each block independently.
As the attention memory cost per block becomes constant, this
modiﬁcation allow us to keep the number of activations linear with respect to the sequence length.
In our experiments, we choose to have blocks of 256 tokens.
Memory-compressed attention: After projecting the tokens into the query, key, and value embed-
dings, we reduce the number of keys and values by using a strided convolution.
The number of
queries remains unchanged.
This modiﬁcation allows us to divide the number of activations by a
compression factor.
In our experiments we use convolution kernels of size 3 with stride 3.
In con-
trast to local attention layers, which only capture the local information within a block, the memory-
compressed attention layers are able to exchange information globally on the entire sequence.
These modiﬁcations (see Figure 1) allow us in practice to process sequences 3x in length over
the T-D model.
For both local and memory-compressed attention, masking is added to prevent
the queries from attending to future keys and values.
Our ﬁnal architecture is a 5-layer network
(LMLML) alternating between local-attention (L) layers and memory-compressed attention (M)
layers (in Vaswani et al.
(2017) it is 6 identical layers).
We also added in some experiments one
mixture of experts (MoE) layer (Shazeer et al., 2017) to increase the network’s capacity.
5
Published as a conference paper at ICLR 2018
Figure 1: The architecture of the self-attention layers used in the T-DMCA model.
Every attention
layer takes a sequence of tokens as input and produces a sequence of similar length as the output.
Left: Original self-attention as used in the transformer-decoder.
Middle: Memory-compressed
attention which reduce the number of keys/values.
Right: Local attention which splits the sequence
into individual smaller sub-sequences.
The sub-sequences are then merged together to get the ﬁnal
output sequence.
5
EXPERIMENTS
5.1
EVALUATION
In experiments we evaluate based on perplexity (per-wordpiece), a common language modeling
metric, and ROUGE-L F1 (version ROUGE-1.5.5), a common metric used in comparing candidate
and reference summaries.
Note the F1 ﬂavor of ROUGE is more appropriate in this setting as we do
not explicitly constrain the output length in abstractive models; it is the harmonic mean of ROUGE-
Recall (which favors long summaries) and ROUGE-Precision (which favors short summaries).
Although optimizing ROUGE directly has been shown to not always yield the best summaries as
evaluated by human judgment (Paulus et al., 2017), we found that for our task optimizing for per-
plexity correlates with increased ROUGE and human judgment.
We suspect that the relatively uni-
form style of Wikipedia articles makes ROUGE more appropriate here than in general abstractive
summarization tasks.
5.2
MODEL TRAINING DETAILS AND DECODING
For all abstractive model training, we use the open-source tensor2tensor2 library.
The seq2seq baseline had a hidden size of 128 with 2 layers (we use the hyper-parameter set deﬁned
in the library as lstm attention).
For
the
Transformer
encoder-decoder
(T-ED),
we
use
the
hyper-parameter
set
transfomer base v1 and train for 1 million steps.
Models exhibited very little over-
ﬁtting and did not require early-stopping.
The Transformer Decoder (T-D) was identical to the
decoder part of T-ED.
The T-DMCA model is similar to T-D, but with the enhancements described
in section 4.2.4.
Unless otherwise stated, during decoding we use a beam search of size 4 and length penalty α = 0.6
(Wu et al., 2016) and decode until an end-of-sequence token is reached.
2https://github.com/tensorflow/tensor2tensor
6
Published as a conference paper at ICLR 2018
Table 3: Comparison of extractive method and corpus with L = 500, and the Transformer E-D
model
Extractor
Corpus
Test log-perplexity
ROUGE-L
cheating
combined
1.72975
59.3
tf-idf
combined
2.46645
34.2
tf-idf
citations-only
3.04299
22.6
tf-idf
search-only
3.56593
2.8
identity
combined
4.80215
4.0
5.3
Transformer-based methods have achieved notable
performance in various NLP tasks (Vaswani et al.,
2017; Devlin et al., 2019; Brown et al., 2020).
In
particular, Brown et al.
(2020) indicated that the
larger parameter size we prepare, the better perfor-
mance the model achieves.
However, the model
which is composed of many parameters occupies
a large part of a GPU memory capacity.
Thus, it
is important to explore a parameter efficient way,
which achieves better performance than a basic
model with the same parameter size.
Parameter sharing is a widely used technique as
a parameter efficient way (Dehghani et al., 2019;
Dabre and Fujita, 2019; Lan et al., 2020).
De-
hghani et al.
(2019) proposed Universal Trans-
former which consists of parameters for only one
layer of a Transformer-based encoder-decoder, and
uses these parameters N times for an N-layered
∗A part of this work was done when the author was at
Tokyo Institute of Technology.
Input
1st layer
2nd layer
Share
3rd layer
4th layer
5th layer
6th layer
Share
Share
SEQUENCE
Input
1st layer
2nd layer
Share
3rd layer
4th layer
5th layer
6th layer
Share
Share
CYCLE
Input
1st layer
2nd layer
Share
3rd layer
4th layer
5th layer
6th layer
Share
Share
CYCLE (REV)
Figure 1: Examples of three parameter assignment
strategies proposed in this study when we set M = 3
and N = 6.
encoder-decoder.
Dabre and Fujita (2019) and
Lan et al.
(2020) also used such parameter shar-
ing across layers for their Transformers.
Dehghani et al.
(2019) reported that Universal
Transformer achieved better performance than the
vanilla Transformer in machine translation if the
parameter sizes of both models are (almost) the
same.
However, when we prepare the same num-
ber of parameters for Universal Transformer and
vanilla Transformer, the dimension sizes of each
layer in Universal Transformer are much larger
than ones in the vanilla Transformer.
Thus, Univer-
sal Transformer requires much more computational
time since its weight matrices are larger.
For exam-
ple, Universal Transformer requires twice as much
training time as the vanilla Transformer in WMT
English-to-German dataset, which is a widely used
machine translation dataset (see Table 1).
In this paper, we propose a new parameter shar-
ing method that is faster than using the same param-
eters for all layers such as Universal Transformers.
Universal Transformers raise their expressiveness
power by increasing the size of weight matrices
for each layer.
On the other hand, stacking (more)
layers is another promising approach to raise ex-
pressiveness power of neural methods (He et al.,
2016).
Thus, the most straight-forward way to
arXiv:2104.06022v4  [cs.CL]  2 Jun 2023
make Universal Transformers faster is stacking lay-
ers with smaller weight matrices for each layer.
However, the approach using the same parameters
for all layers limits the improvement of stacking
layers (Dabre and Fujita, 2019).
Therefore, in-
stead of preparing parameters for only one layer,
we prepare parameters for M layers to construct an
N-layered encoder-decoder, where 1 ≤M ≤N.
In other words, the proposed method relaxes the
parameter sharing strategy in previous studies (De-
hghani et al., 2019; Dabre and Fujita, 2019; Lan
et al., 2020).
Because this relaxation addresses the
above limitation of improvement by stacking lay-
ers, the proposed method can be fast by stacking
layers with using small weight matrices for each
layer.
For the actual parameter assignment strate-
gies, we provide several simple examples (Figure 1)
and investigate their performance empirically.
The
main focus of this study is to demonstrate that such
simple strategies can be a better alternative to the
existing parameter sharing strategy used in Univer-
sal Transformers.
We mainly conduct experiments on machine
translation datasets.
Experimental results show that
the proposed method achieves slightly better scores
to the previous method, that assigns parameters of
one layer to all layers, with smaller computational
time.
In addition, we indicate that the proposed
method outperforms the previous parameter shar-
ing method when we spend almost the same train-
ing time.
Moreover, we conduct experiments on au-
tomatic speech recognition and language modeling
tasks (Section 4 and Appendix A).
Experimental re-
sults on these tasks also indicate that the proposed
method are also efficient in these situations.
2
Proposed Method
As described in Section 1, we use parameters
for M layers in the construction of an N-layered
Transformer-based encoder-decoder.
We provide
three examples for the parameter assignment: SE-
QUENCE, CYCLE, and CYCLE (REV).
This section
describes these parameter assignment strategies.
Figure 1 shows examples of three parameter as-
signment strategies for an encoder side when we
set M = 3 and N = 6.
Let enci be the i-th layer
of an encoder.
Figure 2 describes the algorithm to
assign each parameter to each layer of the encoder.
For the decoder side, we assign each parameter
with the same manner.
Algorithm Encoder Construction
Input: the total number of layers N, number of
independent layers M, sharing strategy TYPE
∈{SEQUENCE, CYCLE, CYCLE (REV)}
Output: enc1, ..., encN
1: for i in [1, ..., N] do
2:
if i == 1 then
3:
enci ←CreateNewLayer
4:
else if TYPE == SEQUENCE then
5:
if (i −1) mod ⌊N/M⌋== 0 then
6:
enci ←CreateNewLayer
7:
else
8:
enci ←enci−1
9:
else if TYPE == CYCLE then
10:
if i ≤M then
11:
enci ←CreateNewLayer
12:
else
13:
enci ←enc((i−1) mod M)+1
14:
else if TYPE == CYCLE (REV) then
15:
if i ≤M then
16:
enci ←CreateNewLayer
17:
else if i ≤(M × (⌈N/M⌉−1)) then
18:
enci ←enc((i−1) mod M)+1
19:
else
20:
enci ←encM−((i−1) mod M)
Figure 2: Proposed parameter assignment strategies for
encoder construction.
CreateNewLayer is a function
that creates a new encoder layer.
2.1
SEQUENCE
The simplest strategy is to assign the same param-
eters to sequential ⌊N/M⌋layers.
We name this
strategy SEQUENCE.
For example, when we set
M = 3 and N = 6, two sequential layers share
their parameters as illustrated in Figure 1.
2.2
CYCLE
In CYCLE, we stack M layers whose parameters
are independent from each other.
Then, we repeat
stacking the M layers with the identical order to
the first M layers until the total number of layers
reaches N.
When we set M = 3 and N = 6, we
stack 3 layers twice as illustrated in Figure 1.
2.3
CYCLE (REV)
Liu et al.
(2020) and Takase et al.
(2022) reported
that higher decoder layers tends to obtain larger
gradient norms1.
Their report implies that higher
layers require more degrees of freedom than lower
layers for their expressiveness.
In other words,
lower layers probably have redundant parameters
compared to higher layers.
Thus, we propose the
CYCLE (REV) strategy reusing parameters of lower
layers in higher layers.
In this strategy, we repeat stacking M layers in
the same manner as CYCLE until M ∗(⌈N/M⌉−1)
layers.
For the remaining layers, we stack M layers
in the reverse order.
When we set M = 3 and
N = 6, we stack 3 layers and then stack the 3
layers in the reverse order as in Figure 1.
Thus, the
lowest layer and highest layer share parameters.
3
Experiments on Machine Translation
We investigate the efficiency of the proposed pa-
rameter sharing strategies.
In detail, we indicate
that our proposed strategies are faster than Uni-
versal Transformers while achieving comparable
(or better) performance when we use the same pa-
rameter size.
In this section, we conduct experi-
ments on machine translation datasets.
First, we
focus on the English-to-German translation task
because this task is widely used in the previous
studies (Vaswani et al., 2017; Ott et al., 2018; De-
hghani et al., 2019; Kiyono et al., 2020).
We con-
duct comparisons based on following aspects: (i)
comparison with Universal Transformers in terms
of efficiency and (ii) comparison with models with-
out parameter sharing across layers to investigate
whether our proposed strategies can achieve com-
parable (or better) performance to the models with
larger memory footprint.
In addition to the widely used training data, we
conduct experiments on a large amount of train-
ing dataset in the English-to-German translation
task.
Then, we investigate if our findings are con-
sistent in other language direction (i.e., German-
to-English) and other language pair (i.e., English-
to-French and French-to-English).
We describe
details in the following subsections.
3.1
Standard Setting
3.1.1
Datasets
We used the WMT 2016 training dataset, which
is widely used in previous studies (Vaswani et al.,
1In particular, this property is observed during warm-up
when we use the post layer normalization (Post-LN) setting,
which is originally used in Vaswani et al.
(2017) and widely
used in machine translation.
2017; Ott et al., 2018; Takase and Kiyono, 2021).
This dataset contains 4.5M English-German sen-
tence pairs.
Following previous studies, we con-
structed a vocabulary set with BPE (Sennrich et al.,
2016b) in the same manner.
We set the number of
BPE merge operations at 32K and shared the vocab-
ulary between the source and target languages.
We
measured case-sensitive detokenized BLEU with
SacreBLEU (Post, 2018)2.
3.1.2
Deep learning has witnessed an explosion of archi-
tectures of continuously growing capability and capacity
[29, 25, 52].
Aided by the rapid gains in hardware, mod-
els today can easily overﬁt one million images [13] and
begin to demand hundreds of millions of—often publicly
inaccessible—labeled images [16].
This appetite for data has been successfully addressed in
natural language processing (NLP) by self-supervised pre-
training.
The solutions, based on autoregressive language
modeling in GPT [42, 43, 4] and masked autoencoding in
BERT [14], are conceptually simple: they remove a portion
of the data and learn to predict the removed content.
These
methods now enable training of generalizable NLP models
containing over one hundred billion parameters [4].
The idea of masked autoencoders, a form of more gen-
eral denoising autoencoders [53], is natural and applicable
in computer vision as well.
Indeed, closely related research
encoder
....
....
decoder
input
target
Figure 1.
Our MAE architecture.
During pre-training, a large
random subset of image patches (e.g., 75%) is masked out.
The
encoder is applied to the small subset of visible patches.
Mask
tokens are introduced after the encoder, and the full set of en-
coded patches and mask tokens is processed by a small decoder
that reconstructs the original image in pixels.
After pre-training,
the decoder is discarded and the encoder is applied to uncorrupted
images (full sets of patches) for recognition tasks.
in vision [54, 41] preceded BERT.
However, despite signif-
icant interest in this idea following the success of BERT,
progress of autoencoding methods in vision lags behind
NLP.
We ask: what makes masked autoencoding different
between vision and language?
We attempt to answer this
question from the following perspectives:
(i) Until recently, architectures were different.
In vision,
convolutional networks [30] were dominant over the last
decade [29].
Convolutions typically operate on regular grids
and it is not straightforward to integrate ‘indicators’ such as
mask tokens [14] or positional embeddings [52] into con-
volutional networks.
This architectural gap, however, has
been addressed with the introduction of Vision Transform-
ers (ViT) [16] and should no longer present an obstacle.
(ii) Information density is different between language
and vision.
Languages are human-generated signals that
are highly semantic and information-dense.
When training
a model to predict only a few missing words per sentence,
this task appears to induce sophisticated language under-
standing.
Images, on the contrary, are natural signals with
heavy spatial redundancy—e.g., a missing patch can be re-
covered from neighboring patches with little high-level un-
1
arXiv:2111.06377v2  [cs.CV]  2 Dec 2021
Figure 2.
Example results on ImageNet validation images.
For each triplet, we show the masked image (left), our MAE reconstruction†
(middle), and the ground-truth (right).
The masking ratio is 80%, leaving only 39 out of 196 patches.
More examples are in the appendix.
patches to improve visual quality.
We intentionally opt not to do this, so we can more comprehensively demonstrate the method’s behavior.
Figure 3.
Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights as in Figure 2).
Observe the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible.
derstanding of parts, objects, and scenes.
To overcome this
difference and encourage learning useful features, we show
that a simple strategy works well in computer vision: mask-
ing a very high portion of random patches.
This strategy
largely reduces redundancy and creates a challenging self-
supervisory task that requires holistic understanding beyond
low-level image statistics.
To get a qualitative sense of our
reconstruction task, see Figures 2 – 4.
(iii) The autoencoder’s decoder, which maps the latent
representation back to the input, plays a different role be-
tween reconstructing text and images.
In vision, the decoder
reconstructs pixels, hence its output is of a lower semantic
level than common recognition tasks.
This is in contrast
to language, where the decoder predicts missing words that
contain rich semantic information.
While in BERT the de-
coder can be trivial (an MLP) [14], we found that for im-
ages, the decoder design plays a key role in determining the
semantic level of the learned latent representations.
Driven by this analysis, we present a simple, effective,
and scalable form of a masked autoencoder (MAE) for
visual representation learning.
Our MAE masks random
patches from the input image and reconstructs the missing
patches in the pixel space.
It has an asymmetric encoder-
decoder design.
Our encoder operates only on the visible
subset of patches (without mask tokens), and our decoder is
lightweight and reconstructs the input from the latent rep-
resentation along with mask tokens (Figure 1).
Shifting
the mask tokens to the small decoder in our asymmetric
encoder-decoder results in a large reduction in computation.
Under this design, a very high masking ratio (e.g., 75%) can
achieve a win-win scenario: it optimizes accuracy while al-
lowing the encoder to process only a small portion (e.g.,
25%) of patches.
This can reduce overall pre-training time
by 3× or more and likewise reduce memory consumption,
enabling us to easily scale our MAE to large models.
Our MAE learns very high-capacity models that gen-
eralize well.
With MAE pre-training, we can train data-
hungry models like ViT-Large/-Huge [16] on ImageNet-1K
with improved generalization performance.
With a vanilla
ViT-Huge model, we achieve 87.8% accuracy when ﬁne-
tuned on ImageNet-1K.
This outperforms all previous re-
sults that use only ImageNet-1K data.
We also evaluate
transfer learning on object detection, instance segmentation,
and semantic segmentation.
In these tasks, our pre-training
achieves better results than its supervised pre-training coun-
terparts, and more importantly, we observe signiﬁcant gains
by scaling up models.
These observations are aligned
with those witnessed in self-supervised pre-training in NLP
[14, 42, 43, 4] and we hope that they will enable our ﬁeld to
explore a similar trajectory.
2
original
mask 75%
mask 85%
mask 95%
Figure 4.
Reconstructions of ImageNet validation images using
an MAE pre-trained with a masking ratio of 75% but applied on
inputs with higher masking ratios.
The predictions differ plausibly
from the original images, showing that the method can generalize.
2.
Related Work
Masked language modeling and its autoregressive coun-
terparts, e.g., BERT [14] and GPT [42, 43, 4], are highly
successful methods for pre-training in NLP.
These methods
hold out a portion of the input sequence and train models
to predict the missing content.
These methods have been
shown to scale excellently [4] and a large abundance of ev-
idence indicates that these pre-trained representations gen-
eralize well to various downstream tasks.
Autoencoding is a classical method for learning representa-
tions.
It has an encoder that maps an input to a latent repre-
sentation and a decoder that reconstructs the input.
For ex-
ample, PCA and k-means are autoencoders [26].
Denoising
autoencoders (DAE) [53] are a class of autoencoders that
corrupt an input signal and learn to reconstruct the origi-
nal, uncorrupted signal.
A series of methods can be thought
of as a generalized DAE under different corruptions, e.g.,
masking pixels [54, 41, 6] or removing color channels [64].
Our MAE is a form of denoising autoencoding, but different
from the classical DAE in numerous ways.
Masked image encoding methods learn representations
from images corrupted by masking.
The pioneering work
of [54] presents masking as a noise type in DAE.
Context
Encoder [41] inpaints large missing regions using convolu-
tional networks.
Motivated by the success in NLP, related
recent methods [6, 16, 2] are based on Transformers [52].
iGPT [6] operates on sequences of pixels and predicts un-
known pixels.
The ViT paper [16] studies masked patch
prediction for self-supervised learning.
Most recently, BEiT
[2] proposes to predict discrete tokens [39, 45].
Self-supervised learning approaches have seen signiﬁcant
interest in computer vision, often focusing on different pre-
text tasks for pre-training [15, 55, 37, 64, 40, 17].
Re-
cently, contrastive learning [3, 22] has been popular, e.g.,
[56, 38, 23, 7], which models image similarity and dis-
similarity (or only similarity [21, 8]) between two or more
views.
Contrastive and related methods strongly depend on
data augmentation [7, 21, 8].
Autoencoding pursues a con-
ceptually different direction, and it exhibits different behav-
iors as we will present.
3.
Approach
Our masked autoencoder (MAE) is a simple autoencod-
ing approach that reconstructs the original signal given its
partial observation.
Like all autoencoders, our approach
has an encoder that maps the observed signal to a latent
representation, and a decoder that reconstructs the origi-
nal signal from the latent representation.
Unlike classical
autoencoders, we adopt an asymmetric design that allows
the encoder to operate only on the partial, observed signal
(without mask tokens) and a lightweight decoder that re-
constructs the full signal from the latent representation and
mask tokens.
Figure 1 illustrates the idea, introduced next.
Masking.
Following ViT [16], we divide an image into reg-
ular non-overlapping patches.
Then we sample a subset of
patches and mask (i.e., remove) the remaining ones.
Our
sampling strategy is straightforward: we sample random
patches without replacement, following a uniform distribu-
tion.
We simply refer to this as “random sampling”.
Random sampling with a high masking ratio (i.e., the ra-
tio of removed patches) largely eliminates redundancy, thus
creating a task that cannot be easily solved by extrapolation
from visible neighboring patches (see Figures 2 – 4).
The
uniform distribution prevents a potential center bias (i.e.,
more masked patches near the image center).
Finally, the
highly sparse input creates an opportunity for designing an
efﬁcient encoder, introduced next.
MAE encoder.
Our encoder is a ViT [16] but applied only
on visible, unmasked patches.
Just as in a standard ViT, our
encoder embeds patches by a linear projection with added
positional embeddings, and then processes the resulting set
via a series of Transformer blocks.
However, our encoder
only operates on a small subset (e.g., 25%) of the full set.
Masked patches are removed; no mask tokens are used.
This allows us to train very large encoders with only a frac-
tion of compute and memory.
The full set is handled by a
lightweight decoder, described next.
MAE decoder.
The input to the MAE decoder is the full
set of tokens consisting of (i) encoded visible patches, and
(ii) mask tokens.
See Figure 1.
Each mask token [14] is a
shared, learned vector that indicates the presence of a miss-
3
ing patch to be predicted.
We add positional embeddings to
all tokens in this full set; without this, mask tokens would
have no information about their location in the image.
The
decoder has another series of Transformer blocks.
The MAE decoder is only used during pre-training to
perform the image reconstruction task (only the encoder
is used to produce image representations for recognition).
Therefore, the decoder architecture can be ﬂexibly designed
in a manner that is independent of the encoder design.
We
experiment with very small decoders, narrower and shal-
lower than the encoder.
For example, our default decoder
has <10% computation per token vs.
the encoder.
With this
asymmetrical design, the full set of tokens are only pro-
cessed by the lightweight decoder, which signiﬁcantly re-
duces pre-training time.
Reconstruction target.
Our MAE reconstructs the input
by predicting the pixel values for each masked patch.
Each
element in the decoder’s output is a vector of pixel values
representing a patch.
The last layer of the decoder is a lin-
ear projection whose number of output channels equals the
number of pixel values in a patch.
The decoder’s output is
reshaped to form a reconstructed image.
Our loss function
computes the mean squared error (MSE) between the recon-
structed and original images in the pixel space.
We compute
the loss only on masked patches, similar to BERT [14].1
We also study a variant whose reconstruction target is
the normalized pixel values of each masked patch.
Specif-
ically, we compute the mean and standard deviation of all
pixels in a patch and use them to normalize this patch.
Us-
ing normalized pixels as the reconstruction target improves
representation quality in our experiments.
Simple implementation.
Our MAE pre-training can be im-
plemented efﬁciently, and importantly, does not require any
specialized sparse operations.
First we generate a token for
every input patch (by linear projection with an added po-
sitional embedding).
Next we randomly shufﬂe the list of
tokens and remove the last portion of the list, based on the
masking ratio.
This process produces a small subset of to-
kens for the encoder and is equivalent to sampling patches
without replacement.
After encoding, we append a list of
mask tokens to the list of encoded patches, and unshufﬂe
this full list (inverting the random shufﬂe operation) to align
all tokens with their targets.
The decoder is applied to this
full list (with positional embeddings added).
As noted, no
sparse operations are needed.
This simple implementation
introduces negligible overhead as the shufﬂing and unshuf-
ﬂing operations are fast.
1Computing the loss only on masked patches differs from traditional
denoising autoencoders [53] that compute the loss on all pixels.
This
choice is purely result-driven: computing the loss on all pixels leads to
a slight decrease in accuracy (e.g., ∼0.5%).
10
20
30
40
50
60
70
80
90
83
84
85
83.2
83.4
83.4
84.7
84.9
85.0
84.9 84.9
84.5
83.0
fine-tuning
masking ratio (%)
10
20
30
40
50
60
70
80
90
50
60
70
54.6
58.9
61.7
67.0
69.9
71.8
73.2 73.5 71.8
66.1
linear probing
masking ratio (%)
Figure 5.
Masking ratio.
A high masking ratio (75%) works well
for both ﬁne-tuning (top) and linear probing (bottom).
The y-axes
are ImageNet-1K validation accuracy (%) in all plots in this paper.
4.
ImageNet Experiments
We do self-supervised pre-training on the ImageNet-1K
(IN1K) [13] training set.
Then we do supervised training to
evaluate the representations with (i) end-to-end ﬁne-tuning
or (ii) linear probing.
We report top-1 validation accuracy
of a single 224×224 crop.
Details are in Appendix A.1.
Baseline: ViT-Large.
We use ViT-Large (ViT-L/16) [16]
as the backbone in our ablation study.
ViT-L is very big (an
order of magnitude bigger than ResNet-50 [25]) and tends
to overﬁt.
The following is a comparison between ViT-L
trained from scratch vs.
ﬁne-tuned from our baseline MAE:
scratch, original [16]
scratch, our impl.
baseline MAE
76.5
82.5
84.9
We note that it is nontrivial to train supervised ViT-L from
scratch and a good recipe with strong regularization is
needed (82.5%, see Appendix A.2).
Even so, our MAE pre-
training contributes a big improvement.
Here ﬁne-tuning is
only for 50 epochs (vs.
200 from scratch), implying that the
ﬁne-tuning accuracy heavily depends on pre-training.
4.1.
Main Properties
We ablate our MAE using the default settings in Table 1
(see caption).
Several intriguing properties are observed.
Masking ratio.
Figure 5 shows the inﬂuence of the mask-
ing ratio.
The optimal ratios are surprisingly high.
The ra-
tio of 75% is good for both linear probing and ﬁne-tuning.
This behavior is in contrast with BERT [14], whose typical
masking ratio is 15%.
Our masking ratios are also much
higher than those in related works [6, 16, 2] in computer
vision (20% to 50%).
The model infers missing patches to produce different,
yet plausible, outputs (Figure 4).
It makes sense of the
gestalt of objects and scenes, which cannot be simply com-
pleted by extending lines or textures.
We hypothesize that
this reasoning-like behavior is linked to the learning of use-
ful representations.
Figure 5 also shows that linear probing and ﬁne-tuning
results follow different trends.
For linear probing, the ac-
4
blocks
ft
lin
1
84.8
65.5
2
84.9
70.0
4
84.9
71.9
8
84.9
73.5
12
84.4
73.3
(a) Decoder depth.
A deep decoder can im-
prove linear probing accuracy.
dim
ft
lin
128
84.9
69.1
256
84.8
71.3
512
84.9
73.5
768
84.4
73.1
1024
84.3
73.1
(b) Decoder width.
The decoder can be nar-
rower than the encoder (1024-d).
case
ft
lin
FLOPs
encoder w/ [M]
84.2
59.6
3.3×
encoder w/o [M]
84.9
73.5
1×
(c) Mask token.
An encoder without mask to-
kens is more accurate and faster (Table 2).
case
ft
lin
pixel (w/o norm)
84.9
73.5
pixel (w/ norm)
85.4
73.9
PCA
84.6
72.3
dVAE token
85.3
71.6
(d) Reconstruction target.
Pixels as recon-
struction targets are effective.
case
ft
lin
none
84.0
65.7
crop, ﬁxed size
84.7
73.1
crop, rand size
84.9
73.5
crop + color jit
84.3
71.9
(e) Data augmentation.
Our MAE works with
minimal or no augmentation.
case
ratio
ft
lin
random
75
84.9
73.5
block
50
83.9
72.3
block
75
82.8
63.9
grid
75
84.0
66.0
(f) Mask sampling.
Random sampling works
the best.
See Figure 6 for visualizations.
Table 1.
MAE ablation experiments with ViT-L/16 on ImageNet-1K.
We report ﬁne-tuning (ft) and linear probing (lin) accuracy (%).
If
not speciﬁed, the default is: the decoder has depth 8 and width 512, the reconstruction target is unnormalized pixels, the data augmentation
is random resized cropping, the masking ratio is 75%, and the pre-training length is 800 epochs.
Default settings are marked in gray .
curacy increases steadily with the masking ratio until the
sweet point: the accuracy gap is up to ∼20% (54.6% vs.
73.5%).
For ﬁne-tuning, the results are less sensitive to the
ratios, and a wide range of masking ratios (40–80%) work
well.
All ﬁne-tuning results in Figure 5 are better than train-
ing from scratch (82.5%).
Decoder design.
Our MAE decoder can be ﬂexibly de-
signed, as studied in Table 1a and 1b.
Table 1a varies the decoder depth (number of Trans-
former blocks).
A sufﬁciently deep decoder is important
for linear probing.
This can be explained by the gap be-
tween a pixel reconstruction task and a recognition task: the
last several layers in an autoencoder are more specialized
for reconstruction, but are less relevant for recognition.
A
reasonably deep decoder can account for the reconstruction
specialization, leaving the latent representations at a more
abstract level.
This design can yield up to 8% improvement
in linear probing (Table 1a, ‘lin’).
However, if ﬁne-tuning
is used, the last layers of the encoder can be tuned to adapt
to the recognition task.
The decoder depth is less inﬂuential
for improving ﬁne-tuning (Table 1a, ‘ft’).
Interestingly, our MAE with a single-block decoder can
perform strongly with ﬁne-tuning (84.8%).
Note that a sin-
gle Transformer block is the minimal requirement to propa-
gate information from visible tokens to mask tokens.
Such
a small decoder can further speed up training.
In Table 1b we study the decoder width (number of chan-
nels).
We use 512-d by default, which performs well un-
der ﬁne-tuning and linear probing.
A narrower decoder also
works well with ﬁne-tuning.
Overall, our default MAE decoder is lightweight.
It has
8 blocks and a width of 512-d ( gray in Table 1).
It only
has 9% FLOPs per token vs.
ViT-L (24 blocks, 1024-d).
As such, while the decoder processes all tokens, it is still a
small fraction of the overall compute.
encoder
dec.
depth
ft acc
hours
speedup
ViT-L, w/ [M]
8
84.2
42.4
-
ViT-L
8
84.9
15.4
2.8×
ViT-L
1
84.8
11.6
3.7×
ViT-H, w/ [M]
8
-
119.6†
-
ViT-H
8
85.8
34.5
3.5×
ViT-H
1
85.9
29.3
4.1×
Table 2.
Wall-clock time of our MAE training (800 epochs),
benchmarked in 128 TPU-v3 cores with TensorFlow.
The speedup
is relative to the entry whose encoder has mask tokens (gray).
The
decoder width is 512, and the mask ratio is 75%.
†: This entry is
estimated by training ten epochs.
Mask token.
An important design of our MAE is to skip
the mask token [M] in the encoder and apply it later in the
lightweight decoder.
Table 1c studies this design.
If the encoder uses mask tokens, it performs worse: its
accuracy drops by 14% in linear probing.
In this case,
there is a gap between pre-training and deploying: this en-
coder has a large portion of mask tokens in its input in pre-
training, which does not exist in uncorrupted images.
This
gap may degrade accuracy in deployment.
By removing the
mask token from the encoder, we constrain the encoder to
always see real patches and thus improve accuracy.
Moreover, by skipping the mask token in the encoder,
we greatly reduce training computation.
In Table 1c, we
reduce the overall training FLOPs by 3.3×.
This leads to
a 2.8× wall-clock speedup in our implementation (see Ta-
ble 2).
The wall-clock speedup is even bigger (3.5–4.1×),
for a smaller decoder (1-block), a larger encoder (ViT-H),
or both.
Note that the speedup can be >4× for a masking
ratio of 75%, partially because the self-attention complexity
is quadratic.
In addition, memory is greatly reduced, which
can enable training even larger models or speeding up more
by large-batch training.
The time and memory efﬁciency
makes our MAE favorable for training very large models.
5
block 50%
grid 75%
random 75%
Figure 6.
Mask sampling strategies determine the pretext task
difﬁculty, inﬂuencing reconstruction quality and representations
(Table 1f).
Here each output is from an MAE trained with the spec-
iﬁed masking strategy.
Left: random sampling (our default).
Mid-
dle: block-wise sampling [2] that removes large random blocks.
Right: grid-wise sampling that keeps one of every four patches.
Images are from the validation set.
Reconstruction target.
We compare different reconstruc-
tion targets in Table 1d.
Our results thus far are based on
pixels without (per-patch) normalization.
Using pixels with
normalization improves accuracy.
This per-patch normal-
ization enhances the contrast locally.
In another variant, we
perform PCA in the patch space and use the largest PCA
coefﬁcients (96 here) as the target.
Doing so degrades ac-
curacy.
Both experiments suggest that the high-frequency
components are useful in our method.
We also compare an MAE variant that predicts tokens,
the target used in BEiT [2].
Speciﬁcally for this variant,
we use the DALLE pre-trained dVAE [45] as the tokenizer,
following [2].
Here the MAE decoder predicts the token in-
dices using cross-entropy loss.
This tokenization improves
ﬁne-tuning accuracy by 0.4% vs.
unnormalized pixels, but
has no advantage vs.
normalized pixels.
It also reduces lin-
ear probing accuracy.
In §5 we further show that tokeniza-
tion is not necessary in transfer learning.
Our pixel-based MAE is much simpler than tokeniza-
tion.
The dVAE tokenizer requires one more pre-training
stage, which may depend on extra data (250M images [45]).
The dVAE encoder is a large convolutional network (40%
FLOPs of ViT-L) and adds nontrivial overhead.
Using pix-
els does not suffer from these problems.
Data augmentation.
Table 1e studies the inﬂuence of data
augmentation on our MAE pre-training.
Our MAE works well using cropping-only augmenta-
tion, either ﬁxed-size or random-size (both having random
horizontal ﬂipping).
Adding color jittering degrades the re-
sults and so we do not use it in other experiments.
Surprisingly, our MAE behaves decently even if using
no data augmentation (only center-crop, no ﬂipping).
This
property is dramatically different from contrastive learning
and related methods [56, 23, 7, 21], which heavily rely
on data augmentation.
It was observed [21] that using
cropping-only augmentation reduces the accuracy by 13%
100
200
400
800
1600
82
83
84
85
82.3
83.3
84.3
84.9
85.1
fine-tuning
epochs (log-scale)
100
200
400
800
1600
60
65
70
75
57.3
64.4
69.7
73.5
75.1
linear probing
epochs (log-scale)
Figure 7.
Training schedules.
A longer training schedule gives a
noticeable improvement.
Here each point is a full training sched-
ule.
The model is ViT-L with the default setting in Table 1.
and 28% respectively for BYOL [21] and SimCLR [7].
In
addition, there is no evidence that contrastive learning can
work without augmentation: the two views of an image are
the same and can easily satisfy a trivial solution.
In MAE, the role of data augmentation is mainly per-
formed by random masking (ablated next).
The masks are
different for each iteration and so they generate new training
samples regardless of data augmentation.
The pretext task
is made difﬁcult by masking and requires less augmentation
to regularize training.
Mask sampling strategy.
In Table 1f we compare different
mask sampling strategies, illustrated in Figure 6.
The block-wise masking strategy, proposed in [2], tends
to remove large blocks (Figure 6 middle).
Our MAE with
block-wise masking works reasonably well at a ratio of
50%, but degrades at a ratio of 75%.
This task is harder
than that of random sampling, as a higher training loss is
observed.
The reconstruction is also blurrier.
We also study grid-wise sampling, which regularly keeps
one of every four patches (Figure 6 right).
This is an eas-
ier task and has lower training loss.
The reconstruction is
sharper.
However, the representation quality is lower.
Simple random sampling works the best for our MAE.
It
allows for a higher masking ratio, which provides a greater
speedup beneﬁt while also enjoying good accuracy.
Training schedule.
Our ablations thus far are based on
800-epoch pre-training.
Figure 7 shows the inﬂuence of the
training schedule length.
The accuracy improves steadily
with longer training.
Indeed, we have not observed sat-
uration of linear probing accuracy even at 1600 epochs.
This behavior is unlike contrastive learning methods, e.g.,
MoCo v3 [9] saturates at 300 epochs for ViT-L.
Note that
the MAE encoder only sees 25% of patches per epoch,
while in contrastive learning the encoder sees 200% (two-
crop) or even more (multi-crop) patches per epoch.
6
method
pre-train data
ViT-B
ViT-L
ViT-H
ViT-H448
scratch, our impl.
-
82.3
82.6
83.1
-
DINO [5]
IN1K
82.8
-
-
-
MoCo v3 [9]
IN1K
83.2
84.1
-
-
BEiT [2]
IN1K+DALLE
83.2
85.2
-
-
MAE
IN1K
83.6
85.9
86.9
87.8
Table 3.
Comparisons with previous results on ImageNet-
1K.
The pre-training data is the ImageNet-1K training set (ex-
cept the tokenizer in BEiT was pre-trained on 250M DALLE data
[45]).
All self-supervised methods are evaluated by end-to-end
ﬁne-tuning.
The ViT models are B/16, L/16, H/14 [16].
The best
for each column is underlined.
All results are on an image size of
224, except for ViT-H with an extra result on 448.
Here our MAE
reconstructs normalized pixels and is pre-trained for 1600 epochs.
0
200
400
600
76
78
80
82
84
86
88
ViT-B/16
ViT-L/16
ViT-H/14
MAE, IN1K
supervised, IN1K, our impl.
supervised, IN1K
supervised, JFT300M
[16]
[16]
params (M)
Figure 8.
MAE pre-training vs.
supervised pre-training, evalu-
ated by ﬁne-tuning in ImageNet-1K (224 size).
We compare with
the original ViT results [16] trained in IN1K or JFT300M.
4.2.
Comparisons with Previous Results
Comparisons with self-supervised methods.
In Table 3
we compare the ﬁne-tuning results of self-supervised ViT
models.
For ViT-B, all methods perform closely.
For ViT-L,
the gaps among methods are bigger, suggesting that a chal-
lenge for bigger models is to reduce overﬁtting.
Our MAE can scale up easily and has shown steady im-
provement from bigger models.
We obtain 86.9% accuracy
using ViT-H (224 size).
By ﬁne-tuning with a 448 size, we
achieve 87.8% accuracy, using only IN1K data.
The pre-
vious best accuracy, among all methods using only IN1K
data, is 87.1% (512 size) [61], based on advanced networks.
We improve over the state-of-the-art by a nontrivial margin
in the highly competitive benchmark of IN1K (no external
data).
Our result is based on vanilla ViT, and we expect
advanced networks will perform better.
Comparing with BEiT [2], our MAE is more accurate
while being simpler and faster.
Our method reconstructs
pixels, in contrast to BEiT that predicts tokens: BEiT re-
ported a 1.8% degradation [2] when reconstructing pixels
with ViT-B.2 We do not need dVAE pre-training.
More-
over, our MAE is considerably faster (3.5× per epoch) than
BEiT, for the reason as studied in Table 1c.
2We observed the degradation also in BEiT with ViT-L: it produces
85.2% (tokens) and 83.5% (pixels), reproduced from the ofﬁcial code.
0 1 2 
4 
6 
12
18
24
70
75
80
85
73.5
81.0
83.1
84.2
84.4
84.6
84.7
84.9
77.6
79.9
80.8
81.6
81.9
83.2
83.8
84.1
MAE baseline
MoCo v3
# blocks ﬁne-tuned
Figure 9.
Partial ﬁne-tuning results of ViT-L w.r.t.
the number
of ﬁne-tuned Transformer blocks under the default settings from
Table 1.
Tuning 0 blocks is linear probing; 24 is full ﬁne-tuning.
Our MAE representations are less linearly separable, but are con-
sistently better than MoCo v3 if one or more blocks are tuned.
The MAE models in Table 3 are pre-trained for 1600
epochs for better accuracy (Figure 7).
Even so, our total
pre-training time is less than the other methods when trained
on the same hardware.
For example, training ViT-L on 128
TPU-v3 cores, our MAE’s training time is 31 hours for 1600
epochs and MoCo v3’s is 36 hours for 300 epochs [9].
Comparisons with supervised pre-training.
In the origi-
nal ViT paper [16], ViT-L degrades when trained in IN1K.
Our implementation of supervised training (see A.2) works
better, but accuracy saturates.
See Figure 8.
Our MAE pre-training, using only IN1K, can general-
ize better: the gain over training from scratch is bigger for
higher-capacity models.
It follows a trend similar to the
JFT-300M supervised pre-training in [16].
This compari-
son shows that our MAE can help scale up model sizes.
4.3.
Partial Fine-tuning
Table 1 shows that linear probing and ﬁne-tuning results
are largely uncorrelated.
Linear probing has been a popular
protocol in the past few years; however, it misses the oppor-
tunity of pursuing strong but non-linear features—which is
indeed a strength of deep learning.
As a middle ground, we
study a partial ﬁne-tuning protocol: ﬁne-tune the last sev-
eral layers while freezing the others.
This protocol was also
used in early works, e.g., [59, 64, 37].
Figure 9 shows the results.
Notably, ﬁne-tuning only one
Transformer block boosts the accuracy signiﬁcantly from
73.5% to 81.0%.
Moreover, if we ﬁne-tune only “half” of
the last block (i.e., its MLP sub-block), we can get 79.1%,
much better than linear probing.
This variant is essentially
ﬁne-tuning an MLP head.
Fine-tuning a few blocks (e.g., 4
or 6) can achieve accuracy close to full ﬁne-tuning.
In Figure 9 we also compare with MoCo v3 [9], a con-
trastive method with ViT-L results available.
MoCo v3 has
higher linear probing accuracy; however, all of its partial
ﬁne-tuning results are worse than MAE.
The gap is 2.6%
when tuning 4 blocks.
While the MAE representations are
less linearly separable, they are stronger non-linear features
and perform well when a non-linear head is tuned.
7
APbox
APmask
method
pre-train data
ViT-B
ViT-L
ViT-B
ViT-L
supervised
IN1K w/ labels
47.9
49.3
42.9
43.9
MoCo v3
IN1K
47.9
49.3
42.7
44.0
BEiT
IN1K+DALLE
49.8
53.3
44.4
47.1
MAE
IN1K
50.3
53.3
44.9
47.2
Table 4.
COCO object detection and segmentation using a ViT
Mask R-CNN baseline.
All entries are based on our implementa-
tion.
Self-supervised entries use IN1K data without labels.
Mask
AP follows a similar trend as box AP.
These observations suggest that linear separability is not
the sole metric for evaluating representation quality.
It has
also been observed (e.g., [8]) that linear probing is not well
correlated with transfer learning performance, e.g., for ob-
ject detection.
To our knowledge, linear evaluation is not
often used in NLP for benchmarking pre-training.
5.
Transfer Learning Experiments
We evaluate transfer learning in downstream tasks using
the pre-trained models in Table 3.
Object detection and segmentation.
We ﬁne-tune Mask
R-CNN [24] end-to-end on COCO [33].
The ViT backbone
is adapted for use with FPN [32] (see A.3).
We apply this
approach for all entries in Table 4.
We report box AP for
object detection and mask AP for instance segmentation.
Compared to supervised pre-training, our MAE performs
better under all conﬁgurations (Table 4).
With the smaller
ViT-B, our MAE is 2.4 points higher than supervised pre-
training (50.3 vs.
47.9, APbox).
More signiﬁcantly, with the
larger ViT-L, our MAE pre-training outperforms supervised
pre-training by 4.0 points (53.3 vs.
49.3).
The pixel-based MAE is better than or on par with the
token-based BEiT, while MAE is much simpler and faster.
Both MAE and BEiT are better than MoCo v3 and MoCo
v3 is on par with supervised pre-training.
Semantic segmentation.
We experiment on ADE20K [66]
using UperNet [57] (see A.4).
Table 5 shows that our pre-
training signiﬁcantly improves results over supervised pre-
training, e.g., by 3.7 points for ViT-L.
Our pixel-based MAE
also outperforms the token-based BEiT.
These observations
are consistent with those in COCO.
Classiﬁcation tasks.
Table 6 studies transfer learning on
the iNaturalists [51] and Places [65] tasks (see A.5).
On
iNat, our method shows strong scaling behavior: accuracy
improves considerably with bigger models.
Our results sur-
pass the previous best results by large margins.
On Places,
our MAE outperforms the previous best results [19, 36],
which were obtained via pre-training on billions of images.
Pixels vs.
tokens.
Table 7 compares pixels vs.
tokens as the
MAE reconstruction target.
While using dVAE tokens is
better than using unnormalized pixels, it is statistically sim-
ilar to using normalized pixels across all cases we tested.
It
again shows that tokenization is not necessary for our MAE.
method
pre-train data
ViT-B
ViT-L
supervised
IN1K w/ labels
47.4
49.9
MoCo v3
IN1K
47.3
49.1
BEiT
IN1K+DALLE
47.1
53.3
MAE
IN1K
48.1
53.6
Table 5.
ADE20K semantic segmentation (mIoU) using Uper-
Net.
BEiT results are reproduced using the ofﬁcial code.
Other
entries are based on our implementation.
Self-supervised entries
use IN1K data without labels.
dataset
ViT-B
ViT-L
ViT-H
ViT-H448
prev best
iNat 2017
70.5
75.7
79.3
83.4
75.4 [50]
iNat 2018
75.4
80.1
83.0
86.8
81.2 [49]
iNat 2019
80.5
83.4
85.7
88.3
84.1 [49]
Places205
63.9
65.8
65.9
66.8
66.0 [19]†
Places365
57.9
59.4
59.8
60.3
58.0 [36]‡
Table 6.
Transfer learning accuracy on classiﬁcation datasets,
using MAE pre-trained on IN1K and then ﬁne-tuned.
We provide
system-level comparisons with the previous best results.
IN1K
COCO
ADE20K
ViT-B
ViT-L
ViT-H
ViT-B
ViT-L
ViT-B
ViT-L
pixel (w/o norm)
83.3
85.1
86.2
49.5
52.8
48.0
51.8
pixel (w/ norm)
83.6
85.9
86.9
50.3
53.3
48.1
53.6
dVAE token
83.6
85.7
86.9
50.3
53.2
48.1
53.4
△
0.0
-0.2
0.0
0.0
-0.1
0.0
-0.2
Table 7.
Pixels vs.
tokens as the MAE reconstruction target.
△is
the difference between using dVAE tokens and using normalized
pixels.
The difference is statistically insigniﬁcant.
6.
Discussion and Conclusion
Simple algorithms that scale well are the core of deep
learning.
In NLP, simple self-supervised learning methods
(e.g., [42, 14, 43, 4]) enable beneﬁts from exponentially
scaling models.
In computer vision, practical pre-training
paradigms are dominantly supervised (e.g.
[29, 46, 25, 16])
despite progress in self-supervised learning.
In this study,
we observe on ImageNet and in transfer learning that
an autoencoder—a simple self-supervised method similar
to techniques in NLP—provides scalable beneﬁts.
Self-
supervised learning in vision may now be embarking on a
similar trajectory as in NLP.
On the other hand, we note that images and languages
are signals of a different nature and this difference must
be addressed carefully.
Images are merely recorded light
without a semantic decomposition into the visual analogue
of words.
Instead of attempting to remove objects, we re-
move random patches that most likely do not form a seman-
tic segment.
Likewise, our MAE reconstructs pixels, which
are not semantic entities.
Nevertheless, we observe (e.g.,
Figure 4) that our MAE infers complex, holistic reconstruc-
tions, suggesting it has learned numerous visual concepts,
i.e., semantics.
We hypothesize that this behavior occurs
by way of a rich hidden representation inside the MAE.
We
hope this perspective will inspire future work.
8
Broader impacts.
The proposed method predicts content
based on learned statistics of the training dataset and as such
will reﬂect biases in those data, including ones with nega-
tive societal impacts.
The model may generate inexistent
content.
These issues warrant further research and consid-
eration when building upon this work to generate images.
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization.
arXiv:1607.06450, 2016.
[2] Hangbo Bao, Li Dong, and Furu Wei.
BEiT: BERT pre-
training of image transformers.
arXiv:2106.08254, 2021.
Accessed in June 2021.
[3] Suzanna Becker and Geoffrey E Hinton.
Self-organizing
neural network that discovers surfaces in random-dot stere-
ograms.
Nature, 1992.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei.
Language models are
few-shot learners.
In NeurIPS, 2020.
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Emerg-
ing properties in self-supervised vision transformers.
In
ICCV, 2021.
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever.
Generative pre-
training from pixels.
In ICML, 2020.
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton.
A simple framework for contrastive learning
of visual representations.
In ICML, 2020.
[8] Xinlei Chen and Kaiming He.
Exploring simple Siamese
representation learning.
In CVPR, 2021.
[9] Xinlei Chen, Saining Xie, and Kaiming He.
An empirical
study of training self-supervised Vision Transformers.
In
ICCV, 2021.
[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-
pher D Manning.
ELECTRA: Pre-training text encoders as
discriminators rather than generators.
In ICLR, 2020.
[11] Corinna Cortes and Vladimir Vapnik.
Support-vector net-
works.
Machine learning, 1995.
[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le.
Randaugment: Practical automated data augmentation
with a reduced search space.
In CVPR Workshops, 2020.
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei.
ImageNet: A large-scale hierarchical image
database.
In CVPR, 2009.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
BERT: Pre-training of deep bidirectional trans-
formers for language understanding.
In NAACL, 2019.
[15] Carl Doersch, Abhinav Gupta, and Alexei A Efros.
Unsuper-
vised visual representation learning by context prediction.
In
ICCV, 2015.
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby.
An image is
worth 16x16 words: Transformers for image recognition at
scale.
In ICLR, 2021.
[17] Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
Un-
supervised representation learning by predicting image rota-
tions.
In ICLR, 2018.
[18] Xavier Glorot and Yoshua Bengio.
Understanding the difﬁ-
culty of training deep feedforward neural networks.
In AIS-
TATS, 2010.
[19] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu,
Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchin-
sky, Ishan Misra, Armand Joulin, and Piotr Bojanowski.
Self-supervised pretraining of visual features in the wild.
arXiv:2103.01988, 2021.
[20] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He.
Accurate, large minibatch
SGD: Training ImageNet in 1 hour.
arXiv:1706.02677, 2017.
[21] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos,
and Michal Valko.
Bootstrap your own latent - a new ap-
proach to self-supervised learning.
In NeurIPS, 2020.
[22] Raia Hadsell, Sumit Chopra, and Yann LeCun.
Dimension-
ality reduction by learning an invariant mapping.
In CVPR,
2006.
[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick.
Momentum contrast for unsupervised visual rep-
resentation learning.
In CVPR, 2020.
[24] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick.
Mask R-CNN.
In ICCV, 2017.
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
2016.
[26] Geoffrey E Hinton and Richard S Zemel.
Autoencoders,
minimum description length, and helmholtz free energy.
In
NeurIPS, 1994.
[27] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
Weinberger.
Deep networks with stochastic depth.
In ECCV,
2016.
[28] Sergey Ioffe and Christian Szegedy.
Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift.
In ICML, 2015.
[29] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
Ima-
genet classiﬁcation with deep convolutional neural networks.
In NeurIPS, 2012.
[30] Yann LeCun, Bernhard Boser, John S Denker, Donnie
Henderson, Richard E Howard, Wayne Hubbard, and
Lawrence D Jackel.
Backpropagation applied to handwrit-
ten zip code recognition.
Neural computation, 1989.
9
[31] Yanghao Li, Saining Xie, Xinlei Chen, Piotr Doll´ar, Kaim-
ing He, and Ross Girshick.
Benchmarking detection transfer
learning with vision transformers.
In preparation, 2021.
[32] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie.
Feature pyramid
networks for object detection.
In CVPR, 2017.
[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick.
Microsoft COCO: Common objects in context.
In
ECCV, 2014.
[34] Ilya Loshchilov and Frank Hutter.
SGDR: Stochastic gradi-
ent descent with warm restarts.
In ICLR, 2017.
[35] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay
regularization.
In ICLR, 2019.
[36] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,
and Laurens van der Maaten.
Exploring the limits of weakly
supervised pretraining.
In ECCV, 2018.
[37] Mehdi Noroozi and Paolo Favaro.
Unsupervised learning of
visual representations by solving jigsaw puzzles.
In ECCV,
2016.
[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
Rep-
resentation learning with contrastive predictive coding.
arXiv:1807.03748, 2018.
[39] Aaron
van
den
Oord,
Oriol
Vinyals,
and
Koray
Kavukcuoglu.
Neural discrete representation learning.
In NeurIPS, 2017.
[40] Deepak Pathak, Ross Girshick, Piotr Doll´ar, Trevor Darrell,
and Bharath Hariharan.
Learning features by watching ob-
jects move.
In CVPR, 2017.
[41] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros.
Context encoders: Feature
learning by inpainting.
In CVPR, 2016.
[42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever.
Improving language understanding by generative
pre-training.
2018.
[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, and Ilya Sutskever.
Language models are unsuper-
vised multitask learners.
2019.
[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J.
Liu.
Exploring the limits of transfer learning with a
uniﬁed text-to-text transformer.
JMLR, 2020.
[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation.
In ICML, 2021.
[46] Karen Simonyan and Andrew Zisserman.
Very deep convo-
lutional networks for large-scale image recognition.
In ICLR,
2015.
[47] Christian Szegedy,
Vincent Vanhoucke,
Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna.
Rethinking the in-
ception architecture for computer vision.
In CVPR, 2016.
[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv´e J´egou.
Training
data-efﬁcient image transformers & distillation through at-
tention.
In ICML, 2021.
[49] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze,
Matthieu Cord, and Herv´e J´egou.
Graﬁt: Learning ﬁne-
grained image representations with coarse labels.
In ICCV,
2021.
[50] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and
Herv´e J´egou.
Fixing the train-test resolution discrepancy.
arXiv:1906.06423, 2019.
[51] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and
Serge Belongie.
The iNaturalist species classiﬁcation and
detection dataset.
In CVPR, 2018.
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin.
Attention is all you need.
In NeurIPS, 2017.
[53] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol.
Extracting and composing robust
features with denoising autoencoders.
In ICML, 2008.
[54] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
Bengio,
Pierre-Antoine
Manzagol,
and
L´eon
Bottou.
Stacked denoising autoencoders: Learning useful represen-
tations in a deep network with a local denoising criterion.
JMLR, 2010.
[55] Xiaolong Wang and Abhinav Gupta.
Unsupervised learning
of visual representations using videos.
In ICCV, 2015.
[56] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin.
Un-
supervised feature learning via non-parametric instance dis-
crimination.
In CVPR, 2018.
[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun.
Uniﬁed perceptual parsing for scene understand-
ing.
In ECCV, 2018.
[58] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr
Doll´ar, and Ross Girshick.
Early convolutions help trans-
formers see better.
In NeurIPS, 2021.
[59] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks?
In
NeurIPS, 2014.
[60] Yang You, Igor Gitman, and Boris Ginsburg.
Large batch
training of convolutional networks.
arXiv:1708.03888, 2017.
[61] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and
Shuicheng Yan.
VOLO: Vision outlooker for visual recogni-
tion.
arXiv:2106.13112, 2021.
[62] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regu-
larization strategy to train strong classiﬁers with localizable
features.
In ICCV, 2019.
[63] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz.
mixup: Beyond empirical risk minimiza-
tion.
In ICLR, 2018.
[64] Richard Zhang, Phillip Isola, and Alexei A Efros.
Colorful
image colorization.
In ECCV, 2016.
[65] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-
ralba, and Aude Oliva.
Learning deep features for scene
recognition using Places database.
In NeurIPS, 2014.
[66] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-
dler, Adela Barriuso, and Antonio Torralba.
Semantic un-
derstanding of scenes through the ADE20K dataset.
IJCV,
2019.
10
A.
Implementation Details
A.1.
ImageNet Experiments
ViT architecture.
We follow the standard ViT architecture
[16].
It has a stack of Transformer blocks [52], and each
block consists of a multi-head self-attention block and an
MLP block, both having LayerNorm (LN) [1].
The encoder
ends with LN.
As the MAE encoder and decoder have dif-
ferent width, we adopt a linear projection layer after the
encoder to match it.
Our MAE adds positional embeddings
[52] (the sine-cosine version) to both the encoder and de-
coder inputs.
Our MAE does not use relative position or
layer scaling (which are used in the code of [2]).
We extract features from the encoder output for ﬁne-
tuning and linear probing.
As ViT has a class token [16],
to adapt to this design, in our MAE pre-training we append
an auxiliary dummy token to the encoder input.
This token
will be treated as the class token for training the classiﬁer in
linear probing and ﬁne-tuning.
Our MAE works similarly
well without this token (with average pooling).
Pre-training.
The default setting is in Table 8.
We do
not use color jittering, drop path, or gradient clip.
We use
xavier uniform [18] to initialize all Transformer blocks, fol-
lowing ViT’s ofﬁcial code [16].
We use the linear lr scaling
rule [20]: lr = base lr×batchsize / 256.
End-to-end ﬁne-tuning.
Our ﬁne-tuning follows common
practice of supervised ViT training.
The default setting is in
Table 9.
We use layer-wise lr decay [10] following [2].
Linear probing.
Our linear classiﬁer training follows [9].
See Table 10.
We observe that linear probing requires a very
different recipe than end-to-end ﬁne-tuning.
In particular,
regularization is in general harmful for linear probing.
Fol-
lowing [9], we disable many common regularization strate-
gies: we do not use mixup [63], cutmix [62], drop path [27],
or color jittering, and we set weight decay as zero.
It is a common practice to normalize the classiﬁer input
when training a classical linear classiﬁer (e.g., SVM [11]).
Similarly, it is beneﬁcial to normalize the pre-trained fea-
tures when training the linear probing classiﬁer.
Follow-
ing [15], we adopt an extra BatchNorm layer [28] without
afﬁne transformation (affine=False).
This layer is ap-
plied on the pre-trained features produced by the encoder,
and is before the linear classiﬁer.
We note that the layer
does not break the linear property, and it can be absorbed
into the linear classiﬁer after training: it is essentially a re-
parameterized linear classiﬁer.3 Introducing this layer helps
calibrate the feature magnitudes across different variants in
our ablations, so that they can use the same setting without
further lr search.
3Alternatively, we can pre-compute the mean and std of the features
and use the normalized features to train linear classiﬁers.
conﬁg
value
optimizer
AdamW [35]
base learning rate
1.5e-4
weight decay
0.05
optimizer momentum
β1, β2=0.9, 0.95 [6]
batch size
4096
learning rate schedule
cosine decay [34]
warmup epochs [20]
40
augmentation
RandomResizedCrop
Table 8.
Pre-training setting.
conﬁg
value
optimizer
AdamW
base learning rate
1e-3
weight decay
0.05
optimizer momentum
β1, β2=0.9, 0.999
layer-wise lr decay [10, 2]
0.75
batch size
1024
learning rate schedule
cosine decay
warmup epochs
5
training epochs
100 (B), 50 (L/H)
augmentation
RandAug (9, 0.5) [12]
label smoothing [47]
0.1
mixup [63]
0.8
cutmix [62]
1.0
drop path [27]
0.1 (B/L) 0.2 (H)
Table 9.
End-to-end ﬁne-tuning setting.
conﬁg
value
optimizer
LARS [60]
base learning rate
0.1
weight decay
0
optimizer momentum
0.9
batch size
16384
learning rate schedule
cosine decay
warmup epochs
10
training epochs
90
augmentation
RandomResizedCrop
Table 10.
Linear probing setting.
We use LARS with a large
batch for faster training; SGD works similarly with a 4096 batch.
Partial ﬁne-tuning.
Our MAE partial ﬁne-tuning (§4.3)
follows the setting in Table 9, except that we adjust the num-
ber of ﬁne-tuning epochs.
We observe that tuning fewer
blocks requires a longer schedule.
We set the numbers of
ﬁne-tuning epochs as {50, 100, 200} and use the optimal
one for each number of blocks tuned.
A.2.
Supervised Training ViT-L/H from Scratch
We ﬁnd that it is nontrivial to train supervised ViT-L/H
from scratch on ImageNet-1K.
The training is unstable.
While there have been strong baselines with publicly avail-
able implementations [48] for smaller models, the recipes
for the larger ViT-L/H are unexplored.
Directly applying
the previous recipes to these larger models does not work.
A NaN loss is frequently observed during training.
We provide our recipe in Table 11.
We use a wd of 0.3,
a large batch size of 4096, and a long warmup, following
the original ViT [16].
We use β2=0.95 following [6].
We
use the regularizations listed in Table 11 and disable others,
following [58].
All these choices are for improving training
stability.
Our recipe can ﬁnish training with no NaN loss.
11
conﬁg
value
optimizer
AdamW
base learning rate
1e-4
weight decay
0.3
optimizer momentum
β1, β2=0.9, 0.95
batch size
4096
learning rate schedule
cosine decay
warmup epochs
20
training epochs
300 (B), 200 (L/H)
augmentation
RandAug (9, 0.5) [12]
label smoothing [47]
0.1
mixup [63]
0.8
cutmix [62]
1.0
drop path [27]
0.1 (B), 0.2 (L/H)
exp.
moving average (EMA)
0.9999
Table 11.
Supervised training ViT from scratch.
The accuracy is 82.6% for ViT-L (81.5% w/o EMA), and
83.1% for ViT-H (80.9% w/o EMA).
Both ViT-L and ViT-H
show an overﬁtting trend if not using EMA.
As a by-product, our recipe for ViT-B has 82.3% accu-
racy (82.1% w/o EMA), vs.
81.8% in [48].
A.3.
Object Detection and Segmentation in COCO
We adapt the vanilla ViT for the use of an FPN backbone
[32] in Mask R-CNN [24].
ViT has a stack of Transformer
blocks that all produce feature maps at a single scale (e.g.,
stride 16).
We equally divide this stack into 4 subsets and
apply convolutions to upsample or downsample the inter-
mediate feature maps for producing different scales (stride
4, 8, 16, or 32, the same as a standard ResNet [25]).
FPN is
built on these multi-scale maps.
For fair comparisons among different methods, we
search for hyper-parameters for each entry in Table 4 (in-
cluding all competitors).
The hyper-parameters we search
for are the learning rate, weight decay, drop path rate, and
ﬁne-tuning epochs.
We will release code along with the
speciﬁc conﬁgurations.
For full model and training details,
plus additional experiments, see [31].
A.4.
Semantic Segmentation in ADE20K
We use UperNet [57] following the semantic segmenta-
tion code of [2].
We ﬁne-tune end-to-end for 100 epochs
with a batch size of 16.
We search for the optimal lr for
each entry in Table 5 (including all competitors).
The semantic segmentation code of [2] uses relative po-
sition bias [44].
Our MAE pre-training does not use it.
For
fair comparison, we turn on relative position bias only dur-
ing transfer learning, initialized as zero.
We note that our
BEiT reproduction uses relative position bias in both pre-
training and ﬁne-tuning, following their code.
A.5.
Additional Classiﬁcation Tasks
We follow the setting in Table 9 for iNaturalist and
Places ﬁne-tuning (Table 6).
We adjust the lr and ﬁne-
tuning epochs for each individual dataset.
method
model
params
acc
iGPT [6]
iGPT-L
1362 M
69.0
iGPT [6]
iGPT-XL
6801 M
72.0
BEiT [2]
ViT-L
304 M
52.1†
MAE
ViT-B
86 M
68.0
MAE
ViT-L
304 M
75.8
MAE
ViT-H
632 M
76.6
Table 12.
Linear probing results of masked encoding methods.
Our ﬁne-tuning results are in Table 3.
†: our implementation.
B.
Comparison on Linear Probing Results
In §4.3 we have shown that linear probing accuracy and
ﬁne-tuning accuracy are largely uncorrelated and they have
different focuses about linear separability.
We notice that
existing masked image encoding methods are generally less
competitive in linear probing (e.g., than contrastive learn-
ing).
For completeness, in Table 12 we compare on linear
probing accuracy with masking-based methods.
Our MAE with ViT-L has 75.8% linear probing accu-
racy.
This is substantially better than previous masking-
based methods.
On the other hand, it still lags behind con-
trastive methods under this protocol: e.g., MoCo v3 [9] has
77.6% linear probing accuracy for the ViT-L (Figure 9).
12
Figure 10.
Uncurated random samples on ImageNet validation images.
For each triplet, we show the masked image (left), our MAE
reconstruction (middle), and the ground-truth (right).
The masking ratio is 75%.
13
Figure 11.
Uncurated random samples on COCO validation images, using an MAE trained on ImageNet.
For each triplet, we show the
masked image (left), our MAE reconstruction (middle), and the ground-truth (right).
The masking ratio is 75%.
14
Neural machine translation is a newly emerging approach to machine translation, recently proposed
by Kalchbrenner and Blunsom (2013), Sutskever et al.
(2014) and Cho et al.
(2014b).
Unlike the
traditional phrase-based translation system (see, e.g., Koehn et al., 2003) which consists of many
small sub-components that are tuned separately, neural machine translation attempts to build and
train a single, large neural network that reads a sentence and outputs a correct translation.
Most of the proposed neural machine translation models belong to a family of encoder–
decoders (Sutskever et al., 2014; Cho et al., 2014a), with an encoder and a decoder for each lan-
guage, or involve a language-speciﬁc encoder applied to each sentence whose outputs are then com-
pared (Hermann and Blunsom, 2014).
An encoder neural network reads and encodes a source sen-
tence into a ﬁxed-length vector.
A decoder then outputs a translation from the encoded vector.
The
whole encoder–decoder system, which consists of the encoder and the decoder for a language pair,
is jointly trained to maximize the probability of a correct translation given a source sentence.
A potential issue with this encoder–decoder approach is that a neural network needs to be able to
compress all the necessary information of a source sentence into a ﬁxed-length vector.
This may
make it difﬁcult for the neural network to cope with long sentences, especially those that are longer
than the sentences in the training corpus.
Cho et al.
(2014b) showed that indeed the performance of
a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.
In order to address this issue, we introduce an extension to the encoder–decoder model which learns
to align and translate jointly.
Each time the proposed model generates a word in a translation, it
(soft-)searches for a set of positions in a source sentence where the most relevant information is
concentrated.
The model then predicts a target word based on the context vectors associated with
these source positions and all the previous generated target words.
∗CIFAR Senior Fellow
1
arXiv:1409.0473v7  [cs.CL]  19 May 2016
Published as a conference paper at ICLR 2015
The most important distinguishing feature of this approach from the basic encoder–decoder is that
it does not attempt to encode a whole input sentence into a single ﬁxed-length vector.
Instead, it en-
codes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively
while decoding the translation.
This frees a neural translation model from having to squash all the
information of a source sentence, regardless of its length, into a ﬁxed-length vector.
We show this
allows a model to cope better with long sentences.
In this paper, we show that the proposed approach of jointly learning to align and translate achieves
signiﬁcantly improved translation performance over the basic encoder–decoder approach.
The im-
provement is more apparent with longer sentences, but can be observed with sentences of any
length.
On the task of English-to-French translation, the proposed approach achieves, with a single
model, a translation performance comparable, or close, to the conventional phrase-based system.
Furthermore, qualitative analysis reveals that the proposed model ﬁnds a linguistically plausible
(soft-)alignment between a source sentence and the corresponding target sentence.
NEURAL MACHINE TRANSLATION
From a probabilistic perspective, translation is equivalent to ﬁnding a target sentence y that max-
imizes the conditional probability of y given a source sentence x, i.e., arg maxy p(y | x).
In
neural machine translation, we ﬁt a parameterized model to maximize the conditional probability
of sentence pairs using a parallel training corpus.
Once the conditional distribution is learned by a
translation model, given a source sentence a corresponding translation can be generated by searching
for the sentence that maximizes the conditional probability.
Recently, a number of papers have proposed the use of neural networks to directly learn this condi-
tional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al.,
2014; Cho et al., 2014b; Forcada and ˜Neco, 1997).
This neural machine translation approach typ-
ically consists of two components, the ﬁrst of which encodes a source sentence x and the second
decodes to a target sentence y.
For instance, two recurrent neural networks (RNN) were used by
(Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a
ﬁxed-length vector and to decode the vector into a variable-length target sentence.
Despite being a quite new approach, neural machine translation has already shown promising results.
Sutskever et al.
(2014) reported that the neural machine translation based on RNNs with long short-
term memory (LSTM) units achieves close to the state-of-the-art performance of the conventional
phrase-based machine translation system on an English-to-French translation task.1 Adding neural
components to existing translation systems, for instance, to score the phrase pairs in the phrase
table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to
surpass the previous state-of-the-art performance level.
2.1
RNN ENCODER–DECODER
Here, we describe brieﬂy the underlying framework, called RNN Encoder–Decoder, proposed by
Cho et al.
(2014a) and Sutskever et al.
(2014) upon which we build a novel architecture that learns
to align and translate simultaneously.
In the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors
x = (x1, · · · , xTx), into a vector c.2 The most common approach is to use an RNN such that
ht = f (xt, ht−1)
(1)
and
c = q ({h1, · · · , hTx}) ,
where ht ∈Rn is a hidden state at time t, and c is a vector generated from the sequence of the
hidden states.
f and q are some nonlinear functions.
Sutskever et al.
(2014) used an LSTM as f and
q ({h1, · · · , hT }) = hT , for instance.
1 We mean by the state-of-the-art performance, the performance of the conventional phrase-based system
without using any neural network-based component.
2 Although most of the previous works (see, e.g., Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and
Blunsom, 2013) used to encode a variable-length input sentence into a ﬁxed-length vector, it is not necessary,
and even it may be beneﬁcial to have a variable-length vector, as we will show later.
2
Published as a conference paper at ICLR 2015
The decoder is often trained to predict the next word yt′ given the context vector c and all the
previously predicted words {y1, · · · , yt′−1}.
In other words, the decoder deﬁnes a probability over
the translation y by decomposing the joint probability into the ordered conditionals:
p(y) =
T
Y
t=1
p(yt | {y1, · · · , yt−1} , c),
(2)
where y =
 y1, · · · , yTy

.
With an RNN, each conditional probability is modeled as
p(yt | {y1, · · · , yt−1} , c) = g(yt−1, st, c),
(3)
where g is a nonlinear, potentially multi-layered, function that outputs the probability of yt, and st is
the hidden state of the RNN.
It should be noted that other architectures such as a hybrid of an RNN
and a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).
3
LEARNING TO ALIGN AND TRANSLATE
In this section, we propose a novel architecture for neural machine translation.
The new architecture
consists of a bidirectional RNN as an encoder (Sec.
3.2) and a decoder that emulates searching
through a source sentence during decoding a translation (Sec.
3.1).
3.1
DECODER: GENERAL DESCRIPTION
x1
x2
x3
xT
+
αt,1
αt,2
αt,3
αt,T
yt-1
yt
h1
h2
h3
hT
h1
h2
h3
hT
s t-1
s t
Figure 1: The graphical illus-
tration of the proposed model
trying to generate the t-th tar-
get word yt given a source
sentence (x1, x2, .
.
.
, xT ).
In a new model architecture, we deﬁne each conditional probability
in Eq.
(2) as:
p(yi|y1, .
.
.
, yi−1, x) = g(yi−1, si, ci),
(4)
where si is an RNN hidden state for time i, computed by
si = f(si−1, yi−1, ci).
It should be noted that unlike the existing encoder–decoder ap-
proach (see Eq.
(2)), here the probability is conditioned on a distinct
context vector ci for each target word yi.
The context vector ci depends on a sequence of annotations
(h1, · · · , hTx) to which an encoder maps the input sentence.
Each
annotation hi contains information about the whole input sequence
with a strong focus on the parts surrounding the i-th word of the
input sequence.
We explain in detail how the annotations are com-
puted in the next section.
The context vector ci is, then, computed as a weighted sum of these
annotations hi:
ci =
Tx
X
j=1
αijhj.
(5)
The weight αij of each annotation hj is computed by
αij =
exp (eij)
PTx
k=1 exp (eik)
,
(6)
where
eij = a(si−1, hj)
is an alignment model which scores how well the inputs around position j and the output at position
i match.
The score is based on the RNN hidden state si−1 (just before emitting yi, Eq.
(4)) and the
j-th annotation hj of the input sentence.
We parametrize the alignment model a as a feedforward neural network which is jointly trained with
all the other components of the proposed system.
Note that unlike in traditional machine translation,
3
Published as a conference paper at ICLR 2015
the alignment is not considered to be a latent variable.
Instead, the alignment model directly com-
putes a soft alignment, which allows the gradient of the cost function to be backpropagated through.
This gradient can be used to train the alignment model as well as the whole translation model jointly.
We can understand the approach of taking a weighted sum of all the annotations as computing an
expected annotation, where the expectation is over possible alignments.
Let αij be a probability that
the target word yi is aligned to, or translated from, a source word xj.
Then, the i-th context vector
ci is the expected annotation over all the annotations with probabilities αij.
The probability αij, or its associated energy eij, reﬂects the importance of the annotation hj with
respect to the previous hidden state si−1 in deciding the next state si and generating yi.
Intuitively,
this implements a mechanism of attention in the decoder.
The decoder decides parts of the source
sentence to pay attention to.
By letting the decoder have an attention mechanism, we relieve the
encoder from the burden of having to encode all information in the source sentence into a ﬁxed-
length vector.
With this new approach the information can be spread throughout the sequence of
annotations, which can be selectively retrieved by the decoder accordingly.
3.2
ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
The usual RNN, described in Eq.
(1), reads an input sequence x in order starting from the ﬁrst
symbol x1 to the last one xTx.
However, in the proposed scheme, we would like the annotation
of each word to summarize not only the preceding words, but also the following words.
Hence,
we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been
successfully used recently in speech recognition (see, e.g., Graves et al., 2013).
A BiRNN consists of forward and backward RNN’s.
The forward RNN −→f reads the input sequence
as it is ordered (from x1 to xTx) and calculates a sequence of forward hidden states (−→h 1, · · · , −→h Tx).
The backward RNN ←−f reads the sequence in the reverse order (from xTx to x1), resulting in a
sequence of backward hidden states (←−h 1, · · · , ←−h Tx).
We obtain an annotation for each word xj by concatenating the forward hidden state −→h j and the
backward one ←−h j, i.e., hj =
h−→h ⊤
j ; ←−h ⊤
j
i⊤
.
In this way, the annotation hj contains the summaries
of both the preceding words and the following words.
Due to the tendency of RNNs to better
represent recent inputs, the annotation hj will be focused on the words around xj.
This sequence
of annotations is used by the decoder and the alignment model later to compute the context vector
(Eqs.
(5)–(6)).
See Fig.
1 for the graphical illustration of the proposed model.
4
EXPERIMENT SETTINGS
We evaluate the proposed approach on the task of English-to-French translation.
We use the bilin-
gual, parallel corpora provided by ACL WMT ’14.3 As a comparison, we also report the perfor-
mance of an RNN Encoder–Decoder which was proposed recently by Cho et al.
(2014a).
We use
the same training procedures and the same dataset for both models.4
4.1
DATASET
WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words.
Following the procedure described in Cho et al.
(2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al.
(2011).5
We do not use any monolingual data other than the mentioned parallel corpora, although it may be
possible to use a much larger monolingual corpus to pretrain an encoder.
We concatenate news-test-
3 http://www.statmt.org/wmt14/translation-task.html
4 Implementations are available at https://github.com/lisa-groundhog/GroundHog.
5 Available online at http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/.
4
Published as a conference paper at ICLR 2015
0
10
20
30
40
50
60
Sentence length
0
5
10
15
20
25
30
BLEU score
RNNsearch-50
RNNsearch-30
RNNenc-50
RNNenc-30
Figure 2: The BLEU scores
of the generated translations
on the test set with respect
to the lengths of the sen-
tences.
The results are on
the full test set which in-
cludes sentences having un-
known words to the models.
2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test
set (news-test-2014) from WMT ’14, which consists of 3003 sentences not present in the training
data.
After a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to
train our models.
Any word not included in the shortlist is mapped to a special token ([UNK]).
We
do not apply any other special preprocessing, such as lowercasing or stemming, to the data.
4.2
MODELS
We train two types of models.
The ﬁrst one is an RNN Encoder–Decoder (RNNencdec, Cho et al.,
2014a), and the other is the proposed model, to which we refer as RNNsearch.
We train each model
twice: ﬁrst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then
with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).
The encoder and decoder of the RNNencdec have 1000 hidden units each.7 The encoder of the
RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000
hidden units.
Its decoder has 1000 hidden units.
In both cases, we use a multilayer network with a
single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each
target word (Pascanu et al., 2014).
We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler,
2012) to train each model.
Each SGD update direction is computed using a minibatch of 80 sen-
tences.
We trained each model for approximately 5 days.
Once a model is trained, we use a beam search to ﬁnd a translation that approximately maximizes the
conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013).
Sutskever
et al.
(2014) used this approach to generate translations from their neural machine translation model.
For more details on the architectures of the models and training procedure used in the experiments,
see Appendices A and B.
Fine-tuning is the prevalent paradigm for using
large pretrained language models (LMs) (Radford
et al., 2019; Devlin et al., 2019) to perform down-
stream tasks (e.g., summarization), but it requires
updating and storing all the parameters of the LM.
Consequently, to build and deploy NLP systems
that rely on large pretrained LMs, one currently
needs to store a modiﬁed copy of the LM parame-
ters for each task.
This can be prohibitively expen-
sive, given the large size of current LMs; for exam-
ple, GPT-2 has 774M parameters (Radford et al.,
2019) and GPT-3 has 175B parameters (Brown
et al., 2020).
A natural approach to this problem is lightweight
ﬁne-tuning, which freezes most of the pretrained
parameters and augments the model with small
trainable modules.
For example, adapter-tuning
Figure 1:
Fine-tuning (top) updates all Transformer
parameters (the red Transformer box) and requires stor-
ing a full model copy for each task.
We propose
preﬁx-tuning (bottom), which freezes the Transformer
parameters and only optimizes the preﬁx (the red pre-
ﬁx blocks).
Consequently, we only need to store the
preﬁx for each task, making preﬁx-tuning modular and
space-efﬁcient.
Note that each vertical block denote
transformer activations at one time step.
(Rebufﬁet al., 2017; Houlsby et al., 2019) inserts
additional task-speciﬁc layers between the layers
of pretrained language models.
Adapter-tuning
has promising performance on natural language
understanding and generation benchmarks, attain-
ing comparable performance with ﬁne-tuning while
adding only around 2-4% task-speciﬁc parameters
(Houlsby et al., 2019; Lin et al., 2020).
On the extreme end, GPT-3 (Brown et al., 2020)
can be deployed without any task-speciﬁc tuning.
Instead, users prepend a natural language task in-
struction (e.g., TL;DR for summarization) and a
few examples to the task input; then generate the
output from the LM.
This approach is known as
in-context learning or prompting.
In this paper, we propose preﬁx-tuning, a
lightweight alternative to ﬁne-tuning for natural lan-
guage generation (NLG) tasks, inspired by prompt-
ing.
Consider the task of generating a textual de-
arXiv:2101.00190v1  [cs.CL]  1 Jan 2021
scription of a data table, as shown in Figure 1,
where the task input is a linearized table (e.g.,
“name: Starbucks | type: coffee shop”) and the out-
put is a textual description (e.g., “Starbucks serves
coffee.”).
Preﬁx-tuning prepends a sequence of
continuous task-speciﬁc vectors to the input, which
we call a preﬁx, depicted by red blocks in Figure 1
(bottom).
For subsequent tokens, the Transformer
can attend to the preﬁx as if it were a sequence of
“virtual tokens”, but unlike prompting, the preﬁx
consists entirely of free parameters which do not
correspond to real tokens.
In contrast to ﬁne-tuning
in Figure 1 (top), which updates all Transformer
parameters and thus requires storing a tuned copy
of the model for each task, preﬁx-tuning only op-
timizes the preﬁx.
Consequently, we only need
to store one copy of the large Transformer and a
learned task-speciﬁc preﬁx, yielding a very small
overhead for each additional task (e.g., 250K pa-
rameters for table-to-text).
In contrast to ﬁne-tuning, preﬁx-tuning is mod-
ular: we train an upstream preﬁx which steers a
downstream LM, which remains unmodiﬁed.
Thus,
a single LM can support many tasks at once.
In
the context of personalization where the tasks cor-
respond to different users (Shokri and Shmatikov,
2015; McMahan et al., 2016), we could have a sep-
arate preﬁx for each user trained only on that user’s
data, thereby avoiding data cross-contamination.
Moreover, the preﬁx-based architecture enables us
to even process examples from multiple users/tasks
in a single batch, something that is not possible
with other lightweight ﬁne-tuning approaches.
We evaluate preﬁx-tuning on table-to-text gen-
eration using GPT-2 and abstractive summariza-
tion using BART.
In terms of storage, preﬁx-tuning
stores 1000x fewer parameters than ﬁne-tuning.
In
terms of performance when trained on full datasets,
preﬁx-tuning and ﬁne-tuning are comparable for
table-to-text (§6.1), while preﬁx-tuning suffers a
small degradation for summarization (§6.2).
In low-
data settings, preﬁx-tuning on average outperforms
ﬁne-tuning on both tasks (§6.3).
Preﬁx-tuning also
extrapolates better to tables (for table-to-text) and
articles (for summarization) with unseen topics
(§6.4).
2
Related Work
Fine-tuning for natural language generation.
Current state-of-the-art systems for natural lan-
guage generation are based on ﬁne-tuning pre-
trained LMs.
For table-to-text generation, Kale
(2020) ﬁne-tunes a sequence-to-sequence model
(T5; Raffel et al., 2020).
For extractive and abstrac-
tive summarization, researchers ﬁne-tune masked
language models (e.g., BERT; Devlin et al., 2019)
and encode-decoder models (e.g., BART; Lewis
et al., 2020) respectively (Zhong et al., 2020; Liu
and Lapata, 2019; Raffel et al., 2020).
For other
conditional NLG tasks such as machine transla-
tion and dialogue generation, ﬁne-tuning is also the
prevalent paradigm (Zhang et al., 2020c; Stickland
et al., 2020; Zhu et al., 2020; Liu et al., 2020).
In
this paper, we focus on table-to-text using GPT-2
and summarization using BART, but preﬁx-tuning
can be applied to other generation tasks and pre-
trained models.
Lightweight
ﬁne-tuning.
Lightweight
ﬁne-
tuning freezes most of the pretrained parameters
and modiﬁes the pretrained model with small
trainable modules.
The key challenge is to identify
high-performing architectures of the modules and
the subset of pretrained parameters to tune.
One
line of research considers removing parameters:
some model weights are ablated away by training
a binary mask over model parameters (Zhao et al.,
2020; Radiya-Dixit and Wang, 2020).
Another
line of research considers inserting parameters.
For example, Zhang et al.
(2020a) trains a “side”
network that is fused with the pretrained model via
summation; adapter-tuning inserts task-speciﬁc lay-
ers (adapters) between each layer of the pretrained
LM (Houlsby et al., 2019; Lin et al., 2020; Rebufﬁ
et al., 2017; Pfeiffer et al., 2020).
Compared to
this line of work, which tunes around 3.6% of the
LM parameters, our method obtains a further 30x
reduction in task-speciﬁc parameters, tuning only
0.1% while maintaining comparable performance.
Prompting.
Prompting means prepending in-
structions and a few examples to the task input and
generating the output from the LM.
GPT-3 (Brown
et al., 2020) uses manually designed prompts to
adapt its generation for different tasks, and this
framework is termed in-context learning.
However,
since Transformers can only condition on a
bounded-length context (e.g., 2048 tokens for GPT-
3), in-context learning is unable to fully exploit
training sets longer than the context window.
Sun
and Lai (2020) also prompt by keywords to control
for sentiment or topic of the generated sentence.
In natural language understanding tasks, prompt
engineering has been explored in prior works for
models like BERT and RoBERTa (Liu et al., 2019;
Jiang et al., 2020; Schick and Sch¨utze, 2020).
For
example, AutoPrompt (Shin et al., 2020) searches
for a sequence of discrete trigger words and con-
catenates it with each input to elicit sentiment or
factual knowledge from a masked LM.
In contrast
with AutoPrompt, our method optimizes contin-
uous preﬁxes, which are more expressive (§7.2);
moreover, we focus on language generation tasks.
Continuous vectors have been used to steer lan-
guage models; for example, Subramani et al.
(2020)
showed that a pretrained LSTM language model
can reconstruct arbitrary sentences by optimizing
a continuous vector for each sentence, making the
vector input-speciﬁc.
In contrast, preﬁx-tuning op-
timizes a task-speciﬁc preﬁx that applies to all in-
stances of that task.
As a result, unlike the previous
work whose application is limited to sentence re-
construction, preﬁx-tuning can be applied to NLG
tasks.
Controllable generation.
Controllable genera-
tion aims to steer a pretrained language model to
match a sentence level attribute (e.g., positive senti-
ment or topic on sports).
Such control can happen
at training time: Keskar et al.
(2019) pretrains the
language model (CTRL) to condition on metadata
such as keywords or URLs.
Additionally, the con-
trol can happen at decoding time, by weighted de-
coding (GeDi, Krause et al., 2020) or iteratively up-
dating the past activations (PPLM, Dathathri et al.,
2020).
However, there is no straightforward way
to apply these controllable generation techniques
to enforce ﬁne-grained control over generated con-
tents, as demanded by tasks like table-to-text and
summarization.
3
Problem Statement
Consider a conditional generation task where the
input is a context x and the output y is a sequence
of tokens.
We focus on two tasks, shown in Fig-
ure 2 (right): In table-to-text, x corresponds to a
linearized data table and y is a textual description;
in summarization, x is an article and y is a short
A key signature of human intelligence is the ability to make “inﬁnite use of ﬁnite means” (Humboldt,
1836; Chomsky, 1965), in which a small set of elements (such as words) can be productively
composed in limitless ways (such as into new sentences).
This reﬂects the principle of combinatorial
generalization, that is, constructing new inferences, predictions, and behaviors from known building
blocks.
Here we explore how to improve modern AI’s capacity for combinatorial generalization by
∗Corresponding author: peterbattaglia@google.com
1
arXiv:1806.01261v3  [cs.LG]  17 Oct 2018
biasing learning towards structured representations and computations, and in particular, systems
that operate on graphs.
Humans’ capacity for combinatorial generalization depends critically on our cognitive mecha-
nisms for representing structure and reasoning about relations.
We represent complex systems as
compositions of entities and their interactions1 (Navon, 1977; McClelland and Rumelhart, 1981;
Plaut et al., 1996; Marcus, 2001; Goodwin and Johnson-Laird, 2005; Kemp and Tenenbaum, 2008),
such as judging whether a haphazard stack of objects is stable (Battaglia et al., 2013).
We use
hierarchies to abstract away from ﬁne-grained diﬀerences, and capture more general commonalities
between representations and behaviors (Botvinick, 2008; Tenenbaum et al., 2011), such as parts of
an object, objects in a scene, neighborhoods in a town, and towns in a country.
We solve novel
problems by composing familiar skills and routines (Anderson, 1982), for example traveling to a
new location by composing familiar procedures and objectives, such as “travel by airplane”, “to
San Diego”, “eat at”, and “an Indian restaurant”.
We draw analogies by aligning the relational
structure between two domains and drawing inferences about one based on corresponding knowledge
about the other (Gentner and Markman, 1997; Hummel and Holyoak, 2003).
Kenneth Craik’s “The Nature of Explanation” (1943), connects the compositional structure of
the world to how our internal mental models are organized:
...[a human mental model] has a similar relation-structure to that of the process it
imitates.
By ‘relation-structure’ I do not mean some obscure non-physical entity which
attends the model, but the fact that it is a working physical model which works in the
same way as the process it parallels...
physical reality is built up, apparently, from a few
fundamental types of units whose properties determine many of the properties of the
most complicated phenomena, and this seems to aﬀord a suﬃcient explanation of the
emergence of analogies between mechanisms and similarities of relation-structure among
these combinations without the necessity of any theory of objective universals.
(Craik,
1943, page 51-55)
That is, the world is compositional, or at least, we understand it in compositional terms.
When
learning, we either ﬁt new knowledge into our existing structured representations, or adjust the
structure itself to better accommodate (and make use of) the new and the old (Tenenbaum et al.,
2006; Griﬃths et al., 2010; Ullman et al., 2017).
The question of how to build artiﬁcial systems which exhibit combinatorial generalization has
been at the heart of AI since its origins, and was central to many structured approaches, including
logic, grammars, classic planning, graphical models, causal reasoning, Bayesian nonparametrics, and
probabilistic programming (Chomsky, 1957; Nilsson and Fikes, 1970; Pearl, 1986, 2009; Russell and
Norvig, 2009; Hjort et al., 2010; Goodman et al., 2012; Ghahramani, 2015).
Entire sub-ﬁelds have
focused on explicit entity- and relation-centric learning, such as relational reinforcement learning
(Dˇzeroski et al., 2001) and statistical relational learning (Getoor and Taskar, 2007).
A key reason
why structured approaches were so vital to machine learning in previous eras was, in part, because
data and computing resources were expensive, and the improved sample complexity aﬀorded by
structured approaches’ strong inductive biases was very valuable.
In contrast with past approaches in AI, modern deep learning methods (LeCun et al., 2015;
Schmidhuber, 2015; Goodfellow et al., 2016) often follow an “end-to-end” design philosophy which
emphasizes minimal a priori representational and computational assumptions, and seeks to avoid
explicit structure and “hand-engineering”.
This emphasis has ﬁt well with—and has perhaps been
aﬃrmed by—the current abundance of cheap data and cheap computing resources, which make
1Whether this entails a “language of thought” (Fodor, 1975) is beyond the scope of this work.
2
trading oﬀsample eﬃciency for more ﬂexible learning a rational choice.
The remarkable and rapid
advances across many challenging domains, from image classiﬁcation (Krizhevsky et al., 2012;
Szegedy et al., 2017), to natural language processing (Sutskever et al., 2014; Bahdanau et al., 2015),
to game play (Mnih et al., 2015; Silver et al., 2016; Moravˇc´ık et al., 2017), are a testament to this
minimalist principle.
A prominent example is from language translation, where sequence-to-sequence
approaches (Sutskever et al., 2014; Bahdanau et al., 2015) have proven very eﬀective without using
explicit parse trees or complex relationships between linguistic entities.
Despite deep learning’s successes, however, important critiques (Marcus, 2001; Shalev-Shwartz
et al., 2017; Lake et al., 2017; Lake and Baroni, 2018; Marcus, 2018a,b; Pearl, 2018; Yuille and
Liu, 2018) have highlighted key challenges it faces in complex language and scene understanding,
reasoning about structured data, transferring learning beyond the training conditions, and learning
from small amounts of experience.
These challenges demand combinatorial generalization, and so it
is perhaps not surprising that an approach which eschews compositionality and explicit structure
struggles to meet them.
When deep learning’s connectionist (Rumelhart et al., 1987) forebears were faced with analogous
critiques from structured, symbolic positions (Fodor and Pylyshyn, 1988; Pinker and Prince, 1988),
there was a constructive eﬀort (Bobrow and Hinton, 1990; Marcus, 2001) to address the challenges
directly and carefully.
A variety of innovative sub-symbolic approaches for representing and reasoning
about structured objects were developed in domains such as analogy-making, linguistic analysis,
symbol manipulation, and other forms of relational reasoning (Smolensky, 1990; Hinton, 1990;
Pollack, 1990; Elman, 1991; Plate, 1995; Eliasmith, 2013), as well as more integrative theories for
how the mind works (Marcus, 2001).
Such work also helped cultivate more recent deep learning
advances which use distributed, vector representations to capture rich semantic content in text
(Mikolov et al., 2013; Pennington et al., 2014), graphs (Narayanan et al., 2016, 2017), algebraic and
logical expressions (Allamanis et al., 2017; Evans et al., 2018), and programs (Devlin et al., 2017;
Chen et al., 2018b).
We suggest that a key path forward for modern AI is to commit to combinatorial generalization
as a top priority, and we advocate for integrative approaches to realize this goal.
Just as biology does
not choose between nature versus nurture—it uses nature and nurture jointly, to build wholes which
are greater than the sums of their parts—we, too, reject the notion that structure and ﬂexibility are
somehow at odds or incompatible, and embrace both with the aim of reaping their complementary
strengths.
In the spirit of numerous recent examples of principled hybrids of structure-based methods
and deep learning (e.g., Reed and De Freitas, 2016; Garnelo et al., 2016; Ritchie et al., 2016; Wu
et al., 2017; Denil et al., 2017; Hudson and Manning, 2018), we see great promise in synthesizing
new techniques by drawing on the full AI toolkit and marrying the best approaches from today
with those which were essential during times when data and computation were at a premium.
Recently, a class of models has arisen at the intersection of deep learning and structured
approaches, which focuses on approaches for reasoning about explicitly structured data, in particular
graphs (e.g.
Scarselli et al., 2009b; Bronstein et al., 2017; Gilmer et al., 2017; Wang et al., 2018c; Li
et al., 2018; Kipf et al., 2018; Gulcehre et al., 2018).
What these approaches all have in common
is a capacity for performing computation over discrete entities and the relations between them.
What sets them apart from classical approaches is how the representations and structure of the
entities and relations—and the corresponding computations—can be learned, relieving the burden
of needing to specify them in advance.
Crucially, these methods carry strong relational inductive
biases, in the form of speciﬁc architectural assumptions, which guide these approaches towards
learning about entities and relations (Mitchell, 1980), which we, joining many others (Spelke et al.,
1992; Spelke and Kinzler, 2007; Marcus, 2001; Tenenbaum et al., 2011; Lake et al., 2017; Lake and
Baroni, 2018; Marcus, 2018b), suggest are an essential ingredient for human-like intelligence.
3
Box 1: Relational reasoning
We deﬁne structure as the product of composing a set of known building blocks.
“Structured
representations” capture this composition (i.e., the arrangement of the elements) and “structured
computations” operate over the elements and their composition as a whole.
Relational reasoning,
then, involves manipulating structured representations of entities and relations, using rules
for how they can be composed.
We use these terms to capture notions from cognitive science,
theoretical computer science, and AI, as follows:
◦An entity is an element with attributes, such as a physical object with a size and mass.
◦A relation is a property between entities.
Relations between two objects might include
same size as, heavier than, and distance from.
Relations can have attributes as
well.
The relation more than X times heavier than takes an attribute, X, which
determines the relative weight threshold for the relation to be true vs.
false.
Relations
can also be sensitive to the global context.
For a stone and a feather, the relation falls
with greater acceleration than depends on whether the context is in air vs.
in a
vacuum.
Here we focus on pairwise relations between entities.
◦A rule is a function (like a non-binary logical predicate) that maps entities and relations
to other entities and relations, such as a scale comparison like is entity X large?
and
is entity X heavier than entity Y?.
Here we consider rules which take one or two
arguments (unary and binary), and return a unary property value.
As an illustrative example of relational reasoning in machine learning, graphical models (Pearl,
1988; Koller and Friedman, 2009) can represent complex joint distributions by making explicit
random conditional independences among random variables.
Such models have been very
successful because they capture the sparse structure which underlies many real-world generative
processes and because they support eﬃcient algorithms for learning and reasoning.
For example,
hidden Markov models constrain latent states to be conditionally independent of others given
the state at the previous time step, and observations to be conditionally independent given the
latent state at the current time step, which are well-matched to the relational structure of many
real-world causal processes.
Explicitly expressing the sparse dependencies among variables
provides for various eﬃcient inference and reasoning algorithms, such as message-passing, which
apply a common information propagation procedure across localities within a graphical model,
resulting in a composable, and partially parallelizable, reasoning procedure which can be applied
to graphical models of diﬀerent sizes and shape.
In the remainder of the paper, we examine various deep learning methods through the lens of
their relational inductive biases, showing that existing methods often carry relational assumptions
which are not always explicit or immediately evident.
We then present a general framework for
entity- and relation-based reasoning—which we term graph networks—for unifying and extending
existing methods which operate on graphs, and describe key design principles for building powerful
architectures using graph networks as building blocks.
We have also released an open-source library
for building graph networks, which can be found here: github.com/deepmind/graph nets.
2
Relational inductive biases
Many approaches in machine learning and AI which have a capacity for relational reasoning
4
Box 2: Inductive biases
Learning is the process of apprehending useful knowledge by observing and interacting with the
world.
It involves searching a space of solutions for one expected to provide a better explanation
of the data or to achieve higher rewards.
But in many cases, there are multiple solutions which
are equally good (Goodman, 1955).
An inductive bias allows a learning algorithm to prioritize
one solution (or interpretation) over another, independent of the observed data (Mitchell,
1980).
In a Bayesian model, inductive biases are typically expressed through the choice and
parameterization of the prior distribution (Griﬃths et al., 2010).
In other contexts, an inductive
bias might be a regularization term (McClelland, 1994) added to avoid overﬁtting, or it might
be encoded in the architecture of the algorithm itself.
Inductive biases often trade ﬂexibility
for improved sample complexity and can be understood in terms of the bias-variance tradeoﬀ
(Geman et al., 1992).
Ideally, inductive biases both improve the search for solutions without
substantially diminishing performance, as well as help ﬁnd solutions which generalize in a
desirable way; however, mismatched inductive biases can also lead to suboptimal performance
by introducing constraints that are too strong.
Inductive biases can express assumptions about either the data-generating process or the space
of solutions.
For example, when ﬁtting a 1D function to data, linear least squares follows
the constraint that the approximating function be a linear model, and approximation errors
should be minimal under a quadratic penalty.
This reﬂects an assumption that the data-
generating process can be explained simply, as a line process corrupted by additive Gaussian
noise.
Similarly, L2 regularization prioritizes solutions whose parameters have small values,
and can induce unique solutions and global structure to otherwise ill-posed problems.
This can
be interpreted as an assumption about the learning process: that searching for good solutions
is easier when there is less ambiguity among solutions.
Note, these assumptions need not be
explicit—they reﬂect interpretations of how a model or algorithm interfaces with the world.
(Box 1) use a relational inductive bias.
While not a precise, formal deﬁnition, we use this term to
refer generally to inductive biases (Box 2) which impose constraints on relationships and interactions
among entities in a learning process.
Creative new machine learning architectures have rapidly proliferated in recent years, with
(perhaps not surprisingly given the thesis of this paper) practitioners often following a design pattern
of composing elementary building blocks to form more complex, deep2 computational hierarchies and
graphs3.
Building blocks such as “fully connected” layers are stacked into “multilayer perceptrons”
(MLPs), “convolutional layers” are stacked into “convolutional neural networks” (CNNs), and a
standard recipe for an image processing network is, generally, some variety of CNN composed with
a MLP.
This composition of layers provides a particular type of relational inductive bias—that
of hierarchical processing—in which computations are performed in stages, typically resulting in
increasingly long range interactions among information in the input signal.
As we explore below, the
building blocks themselves also carry various relational inductive biases (Table 1).
Though beyond
the scope of this paper, various non-relational inductive biases are used in deep learning as well: for
example, activation non-linearities, weight decay, dropout (Srivastava et al., 2014), batch and layer
normalization (Ioﬀe and Szegedy, 2015; Ba et al., 2016), data augmentation, training curricula, and
optimization algorithms all impose constraints on the trajectory and outcome of learning.
2This pattern of composition in depth is ubiquitous in deep learning, and is where the “deep” comes from.
3Recent methods (Liu et al., 2018) even automate architecture construction via learned graph editing procedures.
5
Component
Entities
Relations
Rel.
inductive bias
Invariance
Fully connected
Units
All-to-all
Weak
-
Convolutional
Grid elements
Local
Locality
Spatial translation
Recurrent
Timesteps
Sequential
Sequentiality
Time translation
Graph network
Nodes
Edges
Arbitrary
Node, edge permutations
Table 1: Various relational inductive biases in standard deep learning components.
See also Section 2.
To explore the relational inductive biases expressed within various deep learning methods, we
must identify several key ingredients, analogous to those in Box 1: what are the entities, what are
the relations, and what are the rules for composing entities and relations, and computing their
implications?
In deep learning, the entities and relations are typically expressed as distributed
representations, and the rules as neural network function approximators; however, the precise forms
of the entities, relations, and rules vary between architectures.
To understand these diﬀerences
between architectures, we can further ask how each supports relational reasoning by probing:
◦The arguments to the rule functions (e.g., which entities and relations are provided as input).
◦How the rule function is reused, or shared, across the computational graph (e.g., across diﬀerent
entities and relations, across diﬀerent time or processing steps, etc.).
◦How the architecture deﬁnes interactions versus isolation among representations (e.g., by
applying rules to draw conclusions about related entities, versus processing them separately).
2.1
Relational inductive biases in standard deep learning building blocks
2.1.1
Fully connected layers
Perhaps the most common building block is a fully connected layer (Rosenblatt, 1961).
Typically
implemented as a non-linear vector-valued function of vector inputs, each element, or “unit”, of
the output vector is the dot product between a weight vector, followed by an added bias term, and
ﬁnally a non-linearity such as a rectiﬁed linear unit (ReLU).
As such, the entities are the units in
the network, the relations are all-to-all (all units in layer i are connected to all units in layer j),
and the rules are speciﬁed by the weights and biases.
The argument to the rule is the full input
signal, there is no reuse, and there is no isolation of information (Figure 1a).
The implicit relational
inductive bias in a fully connected layer is thus very weak: all input units can interact to determine
any output unit’s value, independently across outputs (Table 1).
2.1.2
Convolutional layers
Another common building block is a convolutional layer (Fukushima, 1980; LeCun et al., 1989).
It is
implemented by convolving an input vector or tensor with a kernel of the same rank, adding a bias
term, and applying a point-wise non-linearity.
The entities here are still individual units (or grid
elements, e.g.
pixels), but the relations are sparser.
The diﬀerences between a fully connected layer
and a convolutional layer impose some important relational inductive biases: locality and translation
invariance (Figure 1b).
Locality reﬂects that the arguments to the relational rule are those entities in
close proximity with one another in the input signal’s coordinate space, isolated from distal entities.
Translation invariance reﬂects reuse of the same rule across localities in the input.
These biases
are very eﬀective for processing natural image data because there is high covariance within local
6
(a) Fully connected
Sharing in space
(b) Convolutional
Sharing in time
(c) Recurrent
Figure 1: Reuse and sharing in common deep learning building blocks.
(a) Fully connected layer,
in which all weights are independent, and there is no sharing.
(b) Convolutional layer, in which
a local kernel function is reused multiple times across the input.
Shared weights are indicated by
arrows with the same color.
(c) Recurrent layer, in which the same function is reused across diﬀerent
processing steps.
neighborhoods, which diminishes with distance, and because the statistics are mostly stationary
across an image (Table 1).
2.1.3
Recurrent layers
A third common building block is a recurrent layer (Elman, 1990), which is implemented over a
sequence of steps.
Here, we can view the inputs and hidden states at each processing step as the
entities, and the Markov dependence of one step’s hidden state on the previous hidden state and
the current input, as the relations.
The rule for combining the entities takes a step’s inputs and
hidden state as arguments to update the hidden state.
The rule is reused over each step (Figure 1c),
which reﬂects the relational inductive bias of temporal invariance (similar to a CNN’s translational
invariance in space).
For example, the outcome of some physical sequence of events should not
depend on the time of day.
RNNs also carry a bias for locality in the sequence via their Markovian
structure (Table 1).
2.2
Computations over sets and graphs
While the standard deep learning toolkit contains methods with various forms of relational inductive
biases, there is no “default” deep learning component which operates on arbitrary relational structure.
We need models with explicit representations of entities and relations, and learning algorithms which
ﬁnd rules for computing their interactions, as well as ways of grounding them in data.
Importantly,
entities in the world (such as objects and agents) do not have a natural order; rather, orderings
can be deﬁned by the properties of their relations.
For example, the relations between the sizes of
a set of objects can potentially be used to order them, as can their masses, ages, toxicities, and
prices.
Invariance to ordering—except in the face of relations—is a property that should ideally be
reﬂected by a deep learning component for relational reasoning.
Sets are a natural representation for systems which are described by entities whose order is
undeﬁned or irrelevant; in particular, their relational inductive bias does not come from the presence
of something, but rather from the absence.
For illustration, consider the task of predicting the center
7
H
H
O
The brown
dog jumped.
The
brown
dog
jumped
(a)
(b)
(c)
(d)
(e)
(f)
Molecule
Mass-Spring System
n-body System
Rigid Body System
Sentence and Parse Tree
Image and Fully-Connected Scene Graph
Figure 2: Diﬀerent graph representations.
(a) A molecule, in which each atom is represented as a
node and edges correspond to bonds (e.g.
Duvenaud et al., 2015).
(b) A mass-spring system, in
which the rope is deﬁned by a sequence of masses which are represented as nodes in the graph (e.g.
Battaglia et al., 2016; Chang et al., 2017).
(c) A n-body system, in which the bodies are nodes and
the underlying graph is fully connected (e.g.
Battaglia et al., 2016; Chang et al., 2017).
(d) A rigid
body system, in which the balls and walls are nodes, and the underlying graph deﬁnes interactions
between the balls and between the balls and the walls (e.g.
Battaglia et al., 2016; Chang et al., 2017).
(e) A sentence, in which the words correspond to leaves in a tree, and the other nodes and edges
could be provided by a parser (e.g.
Socher et al., 2013).
Alternately, a fully connected graph could
be used (e.g.
Vaswani et al., 2017).
(f) An image, which can be decomposed into image patches
corresponding to nodes in a fully connected graph (e.g.
Santoro et al., 2017; Wang et al., 2018c).
of mass of a solar system comprised of n planets, whose attributes (e.g., mass, position, velocity,
etc.) are denoted by {x1, x2, .
.
.
, xn}.
For such a computation, the order in which we consider the
planets does not matter because the state can be described solely in terms of aggregated, averaged
quantities.
However, if we were to use a MLP for this task, having learned the prediction for a
particular input (x1, x2, .
.
.
, xn) would not necessarily transfer to making a prediction for the same
inputs under a diﬀerent ordering (xn, x1, .
.
.
, x2).
Since there are n!
such possible permutations, in
the worst case, the MLP could consider each ordering as fundamentally diﬀerent, and thus require
an exponential number of input/output training examples to learn an approximating function.
A natural way to handle such combinatorial explosion is to only allow the prediction to depend
on symmetric functions of the inputs’ attributes.
This might mean computing shared per-object
8
features {f(x1), .
.
.
, f(xn)} which are then aggregated in a symmetric way (for example, by taking
their mean).
Such an approach is the essence of the Deep Sets and related models (Zaheer et al.,
2017; Edwards and Storkey, 2016; Pevn`y and Somol, 2017), which we explore further in Section 4.2.3.
Of course, permutation invariance is not the only important form of underlying structure in
many problems.
For example, each object in a set may be aﬀected by pairwise interactions with
the other objects in the set (Hartford et al., 2018).
In our planets scenario, consider now the
task of predicting each individual planet’s position after a time interval, ∆t.
In this case, using
aggregated, averaged information is not enough because the movement of each planet depends on
the forces the other planets are exerting on it.
Instead, we could compute the state of each object
as x′
i = f(xi, P
j g(xi, xj)), where g could compute the force induced by the j-th planet on the
i-th planet, and f could compute the future state of the i-th planet which results from the forces
and dynamics.
The fact that we use the same g everywhere is again a consequence of the global
permutation invariance of the system; however, it also supports a diﬀerent relational structure
because g now takes two arguments rather than one.4
The above solar system examples illustrate two relational structures: one in which there are
no relations, and one which consists of all pairwise relations.
Many real-world systems (such as
in Figure 2) have a relational structure somewhere in between these two extremes, however, with
some pairs of entities possessing a relation and others lacking one.
In our solar system example, if
the system instead consists of the planets and their moons, one may be tempted to approximate
it by neglecting the interactions between moons of diﬀerent planets.
In practice, this means
computing interactions only between some pairs of objects, i.e.
x′
i = f(xi, P
j∈δ(i) g(xi, xj)), where
δ(i) ⊆{1, .
.
.
, n} is a neighborhood around node i.
This corresponds to a graph, in that the i-th
object only interacts with a subset of the other objects, described by its neighborhood.
Note, the
updated states still do not depend in the order in which we describe the neighborhood.5
Graphs, generally, are a representation which supports arbitrary (pairwise) relational struc-
ture, and computations over graphs aﬀord a strong relational inductive bias beyond that which
convolutional and recurrent layers can provide.
3
Graph networks
Neural networks that operate on graphs, and structure their computations accordingly, have been
developed and explored extensively for more than a decade under the umbrella of “graph neural
networks” (Gori et al., 2005; Scarselli et al., 2005, 2009a; Li et al., 2016), but have grown rapidly
in scope and popularity in recent years.
We survey the literature on these methods in the next
sub-section (3.1).
Then in the remaining sub-sections, we present our graph networks framework,
which generalizes and extends several lines of work in this area.
3.1
Models in the graph neural network family (Gori et al., 2005; Scarselli et al., 2005, 2009a; Li et al.,
2016) have been explored in a diverse range of problem domains, across supervised, semi-supervised,
unsupervised, and reinforcement learning settings.
They have been eﬀective at tasks thought to
have rich relational structure, such as visual scene understanding tasks (Raposo et al., 2017; Santoro
4We could extend this same analysis to increasingly entangled structures that depend on relations among triplets
(i.e., g(xi, xj, xk)), quartets, and so on.
We note that if we restrict these functions to only operate on subsets of xi
which are spatially close, then we end back up with something resembling CNNs.
In the most entangled sense, where
there is a single relation function g(x1, .
.
.
, xn), we end back up with a construction similar to a fully connected layer.
5The invariance which this model enforces is the invariance under isomorphism of the graph.
9
et al., 2017) and few-shot learning (Garcia and Bruna, 2018).
They have also been used to learn
the dynamics of physical systems (Battaglia et al., 2016; Chang et al., 2017; Watters et al., 2017;
van Steenkiste et al., 2018; Sanchez-Gonzalez et al., 2018) and multi-agent systems (Sukhbaatar
et al., 2016; Hoshen, 2017; Kipf et al., 2018), to reason about knowledge graphs (Bordes et al., 2013;
O˜noro-Rubio et al., 2017; Hamaguchi et al., 2017), to predict the chemical properties of molecules
(Duvenaud et al., 2015; Gilmer et al., 2017), to predict traﬃc on roads (Li et al., 2017; Cui et al.,
2018), to classify and segment images and videos (Wang et al., 2018c; Hu et al., 2017) and 3D
meshes and point clouds (Wang et al., 2018d), to classify regions in images (Chen et al., 2018a), to
perform semi-supervised text classiﬁcation (Kipf and Welling, 2017), and in machine translation
(Vaswani et al., 2017; Shaw et al., 2018; Gulcehre et al., 2018).
They have been used within both
model-free (Wang et al., 2018b) and model-based (Hamrick et al., 2017; Pascanu et al., 2017;
Sanchez-Gonzalez et al., 2018) continuous control, for model-free reinforcement learning (Hamrick
et al., 2018; Zambaldi et al., 2018), and for more classical approaches to planning (Toyer et al.,
2017).
Many traditional computer science problems, which involve reasoning about discrete entities and
structure, have also been explored with graph neural networks, such as combinatorial optimization
(Bello et al., 2016; Nowak et al., 2017; Dai et al., 2017), boolean satisﬁability (Selsam et al., 2018),
program representation and veriﬁcation (Allamanis et al., 2018; Li et al., 2016), modeling cellular
automata and Turing machines (Johnson, 2017), and performing inference in graphical models
(Yoon et al., 2018).
Recent work has also focused on building generative models of graphs (Li et al.,
2018; De Cao and Kipf, 2018; You et al., 2018; Bojchevski et al., 2018), and unsupervised learning of
graph embeddings (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Garc´ıa-Dur´an
and Niepert, 2017).
The works cited above are by no means an exhaustive list, but provide a representative cross-
section of the breadth of domains for which graph neural networks have proven useful.
We point
interested readers to a number of existing reviews which examine the body of work on graph neural
networks in more depth.
In particular, Scarselli et al.
(2009a) provides an authoritative overview of
early graph neural network approaches.
Bronstein et al.
(2017) provides an excellent survey of deep
learning on non-Euclidean data, and explores graph neural nets, graph convolution networks, and
related spectral approaches.
Recently, Gilmer et al.
(2017) introduced the message-passing neural
network (MPNN), which uniﬁed various graph neural network and graph convolutional network
approaches (Monti et al., 2017; Bruna et al., 2014; Henaﬀet al., 2015; Deﬀerrard et al., 2016;
Niepert et al., 2016; Kipf and Welling, 2017; Bronstein et al., 2017) by analogy to message-passing
in graphical models.
In a similar vein, Wang et al.
(2018c) introduced the non-local neural network
(NLNN), which uniﬁed various “self-attention”-style methods (Vaswani et al., 2017; Hoshen, 2017;
Veliˇckovi´c et al., 2018) by analogy to methods from computer vision and graphical models for
capturing long range dependencies in signals.
3.2
Graph network (GN) block
We now present our graph networks (GN) framework, which deﬁnes a class of functions for
relational reasoning over graph-structured representations.
Our GN framework generalizes and
extends various graph neural network, MPNN, and NLNN approaches (Scarselli et al., 2009a; Gilmer
et al., 2017; Wang et al., 2018c), and supports constructing complex architectures from simple
building blocks.
Note, we avoided using the term “neural” in the “graph network” label to reﬂect
that they can be implemented with functions other than neural networks, though here our focus is
on neural network implementations.
The main unit of computation in the GN framework is the GN block, a “graph-to-graph” module
10
Box 3: Our deﬁnition of “graph”
Attributes
vi
ek
u
vsk
vrk
u
vi
ek
Here we use “graph” to mean a directed, attributed multi-graph with a global attribute.
In our
terminology, a node is denoted as vi, an edge as ek, and the global attributes as u.
We also use
sk and rk to indicate the indices of the sender and receiver nodes (see below), respectively, for
edge k.
To be more precise, we deﬁne these terms as:
Directed : one-way edges, from a “sender” node to a “receiver” node.
Attribute : properties that can be encoded as a vector, set, or even another graph.
Attributed : edges and vertices have attributes associated with them.
Global attribute : a graph-level attribute.
Multi-graph : there can be more than one edge between vertices, including self-edges.
Figure 2 shows a variety of diﬀerent types of graphs corresponding to real data that we may be
interested in modeling, including physical systems, molecules, images, and text.
which takes a graph as input, performs computations over the structure, and returns a graph as
output.
As described in Box 3, entities are represented by the graph’s nodes, relations by the edges,
and system-level properties by global attributes.
The GN framework’s block organization emphasizes
customizability and synthesizing new architectures which express desired relational inductive biases.
The key design principles are: Flexible representations (see Section 4.1); Conﬁgurable within-block
structure (see Section 4.2); and Composable multi-block architectures (see Section 4.3).
We introduce a motivating example to help make the GN formalism more concrete.
Consider
predicting the movements a set of rubber balls in an arbitrary gravitational ﬁeld, which, instead of
bouncing against one another, each have one or more springs which connect them to some (or all) of
the others.
We will refer to this running example throughout the deﬁnitions below, to motivate the
graph representation and the computations operating over it.
Figure 2 depicts some other common
scenarios that can be represented by graphs and reasoned over using graph networks.
3.2.1
Deﬁnition of “graph”
Within our GN framework, a graph is deﬁned as a 3-tuple G = (u, V, E) (see Box 3 for details of
graph representations).
The u is a global attribute; for example, u might represent the gravitational
ﬁeld.
The V = {vi}i=1:Nv is the set of nodes (of cardinality Nv), where each vi is a node’s attribute.
For example, V might represent each ball, with attributes for position, velocity, and mass.
The
E = {(ek, rk, sk)}k=1:Ne is the set of edges (of cardinality Ne), where each ek is the edge’s attribute,
rk is the index of the receiver node, and sk is the index of the sender node.
For example, E might
represent the presence of springs between diﬀerent balls, and their corresponding spring constants.
11
Algorithm 1 Steps of computation in a full GN block.
function GraphNetwork(E, V , u)
for k ∈{1 .
.
.
Ne} do
e′
k ←φe (ek, vrk, vsk, u)
▷1.
Compute updated edge attributes
end for
for i ∈{1 .
.
.
Nn} do
let E′
i = {(e′
k, rk, sk)}rk=i, k=1:Ne
¯e′
i ←ρe→v (E′
i)
▷2.
Aggregate edge attributes per node
v′
i ←φv (¯e′
i, vi, u)
▷3.
Compute updated node attributes
end for
let V ′ = {v′}i=1:Nv
let E′ = {(e′
k, rk, sk)}k=1:Ne
¯e′ ←ρe→u (E′)
▷4.
Aggregate edge attributes globally
¯v′ ←ρv→u (V ′)
▷5.
Aggregate node attributes globally
u′ ←φu (¯e′, ¯v′, u)
▷6.
Compute updated global attribute
return (E′, V ′, u′)
end function
3.2.2
Internal structure of a GN block
A GN block contains three “update” functions, φ, and three “aggregation” functions, ρ,
e′
k = φe (ek, vrk, vsk, u)
v′
i = φv  ¯e′
i, vi, u

u′ = φu  ¯e′, ¯v′, u

¯e′
i = ρe→v  E′
i

¯e′ = ρe→u  E′
¯v′ = ρv→u  V ′
(1)
where E′
i = {(e′
k, rk, sk)}rk=i, k=1:Ne, V ′ = {v′
i}i=1:Nv, and E′ = S
i E′
i = {(e′
k, rk, sk)}k=1:Ne.
The φe is mapped across all edges to compute per-edge updates, the φv is mapped across all
nodes to compute per-node updates, and the φu is applied once as the global update.
The ρ
functions each take a set as input, and reduce it to a single element which represents the aggregated
information.
Crucially, the ρ functions must be invariant to permutations of their inputs, and should
take variable numbers of arguments (e.g., elementwise summation, mean, maximum, etc.).
3.2.3
Computational steps within a GN block
When a graph, G, is provided as input to a GN block, the computations proceed from the edge, to
the node, to the global level.
Figure 3 shows a depiction of which graph elements are involved in
each of these computations, and Figure 4a shows a full GN block, with its update and aggregation
functions.
Algorithm 1 shows the following steps of computation:
1.
φe is applied per edge, with arguments (ek, vrk, vsk, u), and returns e′
k.
In our springs example,
this might correspond to the forces or potential energies between two connected balls.
The
set of resulting per-edge outputs for each node, i, is, E′
i = {(e′
k, rk, sk)}rk=i, k=1:Ne.
And
E′ = S
i E′
i = {(e′
k, rk, sk)}k=1:Ne is the set of all per-edge outputs.
2.
ρe→v is applied to E′
i, and aggregates the edge updates for edges that project to vertex i, into
¯e′
i, which will be used in the next step’s node update.
In our running example, this might
correspond to summing all the forces or potential energies acting on the ith ball.
12
vi
u
e0
k
(a) Edge update
u
e0
k
v0
i
(b) Node update
u0
e0
k
v0
i
(c) Global update
Figure 3: Updates in a GN block.
Blue indicates the element that is being updated, and black
indicates other elements which are involved in the update (note that the pre-update value of the
blue element is also used in the update).
See Equation 1 for details on the notation.
3.
φv is applied to each node i, to compute an updated node attribute, v′
i.
In our running
example, φv may compute something analogous to the updated position, velocity, and kinetic
energy of each ball.
The set of resulting per-node outputs is, V ′ = {v′
i}i=1:Nv.
4.
ρe→u is applied to E′, and aggregates all edge updates, into ¯e′, which will then be used in the
next step’s global update.
In our running example, ρe→u may compute the summed forces
(which should be zero, in this case, due to Newton’s third law) and the springs’ potential
energies.
5.
ρv→u is applied to V ′, and aggregates all node updates, into ¯v′, which will then be used in
the next step’s global update.
In our running example, ρv→u might compute the total kinetic
energy of the system.
6.
φu is applied once per graph, and computes an update for the global attribute, u′.
In our
running example, φu might compute something analogous to the net forces and total energy
of the physical system.
Note, though we assume this sequence of steps here, the order is not strictly enforced: it is possible
to reverse the update functions to proceed from global, to per-node, to per-edge updates, for example.
Kearnes et al.
(2016) computes edge updates from nodes in a similar manner.
3.2.4
Relational inductive biases in graph networks
Our GN framework imposes several strong relational inductive biases when used as components in
a learning process.
First, graphs can express arbitrary relationships among entities, which means
the GN’s input determines how representations interact and are isolated, rather than those choices
being determined by the ﬁxed architecture.
For example, the assumption that two entities have a
relationship—and thus should interact—is expressed by an edge between the entities’ corresponding
nodes.
Similarly, the absence of an edge expresses the assumption that the nodes have no relationship
and should not inﬂuence each other directly.
Second, graphs represent entities and their relations as sets, which are invariant to permutations.
This means GNs are invariant to the order of these elements6, which is often desirable.
For example,
the objects in a scene do not have a natural ordering (see Sec.
2.2).
Third, a GN’s per-edge and per-node functions are reused across all edges and nodes, respectively.
This means GNs automatically support a form of combinatorial generalization (see Section 5.1):
because graphs are composed of edges, nodes, and global features, a single GN can operate on
graphs of diﬀerent sizes (numbers of edges and nodes) and shapes (edge connectivity).
6Note, an ordering can be imposed by encoding the indices in the node or edge attributes, or via the edges
themselves (e.g.
by encoding a chain or partial ordering).
13
4
Design principles for graph network architectures
The GN framework can be used to implement a wide variety of architectures, in accordance with
the design principles listed above in Section 3.2, which also correspond to the sub-sections (4.1,
4.2, and 4.3) below.
In general, the framework is agnostic to speciﬁc attribute representations and
functional forms.
Here, however, we focus mainly on deep learning architectures, which allow GNs
to act as learnable graph-to-graph function approximators.
4.1
Flexible representations
Graph networks support highly ﬂexible graph representations in two ways: ﬁrst, in terms of the
representation of the attributes; and second, in terms of the structure of the graph itself.
4.1.1
Attributes
The global, node, and edge attributes of a GN block can use arbitrary representational formats.
In
deep learning implementations, real-valued vectors and tensors are most common.
However, other
data structures such as sequences, sets, or even graphs could also be used.
The requirements of the problem will often determine what representations should be used for
the attributes.
For example, when the input data is an image, the attributes might be represented
as tensors of image patches; however, when the input data is a text document, the attributes might
be sequences of words corresponding to sentences.
For each GN block within a broader architecture, the edge and node outputs typically correspond
to lists of vectors or tensors, one per edge or node, and the global outputs correspond to a single
vector or tensor.
This allows a GN’s output to be passed to other deep learning building blocks
such as MLPs, CNNs, and RNNs.
The output of a GN block can also be tailored to the demands of the task.
In particular,
◦An edge-focused GN uses the edges as output, for example to make decisions about interactions
among entities (Kipf et al., 2018; Hamrick et al., 2018).
◦A node-focused GN uses the nodes as output, for example to reason about physical systems
(Battaglia et al., 2016; Chang et al., 2017; Wang et al., 2018b; Sanchez-Gonzalez et al., 2018).
◦A graph-focused GN uses the globals as output, for example to predict the potential energy of
a physical system (Battaglia et al., 2016), the properties of a molecule (Gilmer et al., 2017),
or answers to questions about a visual scene (Santoro et al., 2017).
The nodes, edges, and global outputs can also be mixed-and-matched depending on the task.
For
example, Hamrick et al.
(2018) used both the output edge and global attributes to compute a policy
over actions.
4.1.2
Graph structure
When deﬁning how the input data will be represented as a graph, there are generally two scenarios:
ﬁrst, the input explicitly speciﬁes the relational structure; and second, the relational structure must
be inferred or assumed.
These are not hard distinctions, but extremes along a continuum.
Examples of data with more explicitly speciﬁed entities and relations include knowledge graphs,
social networks, parse trees, optimization problems, chemical graphs, road networks, and physical
systems with known interactions.
Figures 2a-d illustrate how such data can be expressed as graphs.
Examples of data where the relational structure is not made explicit, and must be inferred or
assumed, include visual scenes, text corpora, programming language source code, and multi-agent
14
systems.
In these types of settings, the data may be formatted as a set of entities without relations,
or even just a vector or tensor (e.g., an image).
If the entities are not speciﬁed explicitly, they might
be assumed, for instance, by treating each word in a sentence (Vaswani et al., 2017) or each local
feature vector in a CNN’s output feature map, as a node (Watters et al., 2017; Santoro et al., 2017;
Wang et al., 2018c) (Figures 2e-f).
Or, it might be possible to use a separate learned mechanism to
infer entities from an unstructured signal (Luong et al., 2015; Mnih et al., 2014; Eslami et al., 2016;
van Steenkiste et al., 2018).
If relations are not available, the simplest approach is to instantiate all
possible directed edges between entities (Figure 2f).
This can be prohibitive for large numbers of
entities, however, because the number of possible edges grows quadratically with the number of
nodes.
Thus developing more sophisticated ways of inferring sparse structure from unstructured
data (Kipf et al., 2018) is an important future direction.
4.2
Conﬁgurable within-block structure
The structure and functions within a GN block can be conﬁgured in diﬀerent ways, which oﬀers
ﬂexibility in what information is made available as inputs to its functions, as well as how output edge,
node, and global updates are produced.
In particular, each φ in Equation 1 must be implemented
with some function, f, where f’s argument signature determines what information it requires as
input; in Figure 4, the incoming arrows to each φ depict whether u, V , and E are taken as inputs.
Hamrick et al.
(2018) and Sanchez-Gonzalez et al.
(2018) used the full GN block shown in Figure 4a.
Their φ implementations used neural networks (denoted NNe, NNv, and NNu below, to indicate that
they are diﬀerent functions with diﬀerent parameters).
Their ρ implementations used elementwise
summation, but averages and max/min could also be used,
φe (ek, vrk, vsk, u) := fe (ek, vrk, vsk, u) = NNe ([ek, vrk, vsk, u])
(2)
φv  ¯e′
i, vi, u
 := fv  ¯e′
i, vi, u

= NNv
 [¯e′
i, vi, u]

φu  ¯e′, ¯v′, u
 := fu  ¯e′, ¯v′, u

= NNu
 [¯e′, ¯v′, u]

ρe→v  E′
i
 :=
=
X
{k: rk=i}
e′
k
ρv→u  V ′ :=
=
X
i
v′
i
ρe→u  E′ :=
=
X
k
e′
k
where [x, y, z] indicates vector/tensor concatenation.
For vector attributes, a MLP is often used for
φ, while for tensors such as image feature maps, CNNs may be more suitable.
The φ functions can also use RNNs, which requires an additional hidden state as input and
output.
Figure 4b shows a very simple version of a GN block with RNNs as φ functions: there is no
message-passing in this formulation, and this type of block might be used for recurrent smoothing of
some dynamic graph states.
Of course, RNNs as φ functions could also be used in a full GN block
(Figure 4a).
A variety of other architectures can be expressed in the GN framework, often as diﬀerent
function choices and within-block conﬁgurations.
The remaining sub-sections explore how a GN’s
within-block structure can be conﬁgured in diﬀerent ways, with examples of published work which
uses such conﬁgurations.
See the Appendix for details.
15
(a) Full GN block
u0, u0
hid
E0, E0
hid
V 0, V 0
hid
E, Ehid
V, Vhid
u, uhid
Edge block
Node block
Global block
φu
G
BjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux3ik2jApbu1Q
0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQa
aLkowjK9H4dRlmhLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOB3iGV3jzpPfivXsf09YlbzZzAH/gf4AqE6Qnw=</latexit>
o
KNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGSXiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVki
bkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJm
RqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit>
o
KNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGSXiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVki
bkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJm
RqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit>
S
1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmGDKaF0O6IGBZfYsNwKbKcaRIJbEWjm5nfekJtuJL3dpxi
mNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYh
WZvU76XCOzYuwIZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gf5A53Djxw=</latexit>
φv
φe
M8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWldW19YLG8XNre2d3dLevq9lphjWmRSNUOqUfAE64Ybgc1UIY1DgY1w
cDX2G4+oNJfJrRmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1E+uXZETqzSJZFUthJDJurviZzGWg/j0HbG
1PT1vDcW/NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpC
P</latexit>
N
rxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk
0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaH
tjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8
/TZOSnA=</latexit>
N
rxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk
0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaH
tjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8
/TZOSnA=</latexit>
L
XzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmGDKaF0O6IGBZfYsNwKbKcaRIJbEWj
m5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm
4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjw=</l
atexit>
(b) Independent recurrent block
Edge block
Node block
Global block
V 0
u0
φu
g
rweUa+jkM8=">AB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6e
Eo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyura+uFjeLm1vbObmlvPzQy04TWieRSN
2NsKGeC1i2znDaVpjiNOW3Eg6ux3ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6m
H4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNF
/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaLkowjK9H4dRlmhLh45gopm7FZE+1phYF1DR
hRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOB3iGV3jzpPfivXsf09YlbzZzAH/gf4Aq
E6Qnw=</latexit>
l
XL9yCq0no=">AB7XicbVC7SgNBFL0bXzG+oKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOz
yrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGS
XiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5
FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmV
LGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJmRqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtM
jA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/c
D5+AGXTkqw=</latexit>
l
XL9yCq0no=">AB7XicbVC7SgNBFL0bXzG+oKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOz
yrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGS
XiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5
FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmV
LGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJmRqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtM
jA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/c
D5+AGXTkqw=</latexit>
i
0ptSwAiKP4=">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveE
EPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmGDKaF0O
6IGBZfYsNwKbKcaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe
+Vv7p9xbIEpWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2P
GSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZq7WwkbUk2ZdQGVXAjB8sur
pHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gf5A53Djxw=<
/latexit>
⇢v!u
φv
E
d7vqyiRfd+2dI=">AB7XicbVA9SwNBEJ2LXzF+nQo2NotBsAp3NlpYBGwsI5hLIDnj3mYv
WbO3e+zuRcKR/2BjoYit/8fOf+Pmo9DEBwOP92aYmRelnGnjed9OYWV1bX2juFna2t7Z3XP3
DwItM0VonUguVTPCmnImaN0w2kzVRQnEaeNaHA98RtDqjST4s6MUhomuCdYzAg2VgraZ/d
Dztu2at4U6Bl4s9JuXoUPD0AQK3jfrW7kmQJFYZwrHXL91IT5lgZRjgdl9qZpikmA9yjLUs
FTqgO8+m1Y3RqlS6KpbIlDJqvydynGg9SiLbmWDT14veRPzPa2UmvgxzJtLMUEFmi+KMIyP
R5HXUZYoSw0eWYKYvRWRPlaYGBtQyYbgL768TILziu9V/FubxhXMUIRjOIEz8OECqnADNag
DgUd4hld4c6Tz4rw7H7PWgjOfOYQ/cD5/AKnSkKA=</latexit>
P
VKPYM/GTIarLQ=">AB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJ
mNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy73RJRbTzvy1laXldWy9sFDe3tnd2
S3v7gRapwqSOBROqGSFNGOWkbqhpCkVQUnESCMaXI39xpAoTQW/NZkYJ6nMYUI2OloC37
9G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5
pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6
Za4Q7ft3tUkWwYZklCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMp+HAOVbi
GtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit>
P
VKPYM/GTIarLQ=">AB7XicbVC7SgNBFL3rM8ZXVLCxWQyCVdi10cIiYGMZwWwCyRpmJ7PJ
mNmZYWY2siz5BxsLRbT0f+z8AP/DyaPQxAMXDufcy73RJRbTzvy1laXldWy9sFDe3tnd2
S3v7gRapwqSOBROqGSFNGOWkbqhpCkVQUnESCMaXI39xpAoTQW/NZkYJ6nMYUI2OloC37
9G7YKZW9ijeBu0j8GSlXD4MHmX2/1zqlz3ZX4DQh3GCGtG75njRhjpShmJFRsZ1qIhEeoB5
pWcpRQnSYT64duSdW6bqxULa4cSfq74kcJVpnSWQ7E2T6et4bi/95rdTEF2FOuUwN4Xi6KE6
Za4Q7ft3tUkWwYZklCtqb3VxHymEjQ2oaEPw519eJMFZxfcq/o1N4xKmKMARHMp+HAOVbi
GtQBwz08wjO8OMJ5cl6dt2nrkjObOYA/cD5+AGdXkq0=</latexit>
z/CDji4ezWDqY=">AB7XicbVA9SwNBEJ3zM8avqKXNYhCswp2NFhYBG8sI5gOSM+xtNsma
vd1jdy4QjvwHGwtFbP0/dv4bN8kVmvhg4PHeDPzokQKi7/7a2tb2xubRd2irt7+weHpaPj
htWpYbzOtNSmFVHLpVC8jgIlbyWG0ziSvBmNbmd+c8yNFVo94CThYUwHSvQFo+ikRicZisdx
t1T2K/4cZJUEOSlDjlq39NXpaZbGXCGT1Np24CcYZtSgYJPi53U8oSyER3wtqOKxtyG2fz
aKTl3So/0tXGlkMzV3xMZja2dxJHrjCkO7bI3E/z2in2r8NMqCRFrthiUT+VBDWZvU56wnC
GcuIZUa4WwkbUkMZuoCKLoRg+eV0risBH4luPfL1Zs8jgKcwhlcQABXUIU7qEdGDzBM7z
Cm6e9F+/d+1i0rn5zAn8gf5A59Hjx0=</latexit>
⇢e!v
φe
W
y0cvo5FmPz8I=">AB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaS
NXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWldW19YLG8XNre2d3dLe
vq9lphjWmRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmGMS0l/CIM2qs5LfTPr/D
TqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJ
j1E+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW/NamYkugpwnaWYwYdNFUSaIkWT
8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sD
gAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit>
nLY3EY87InuhI=">AB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxN
xszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3v
lHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gB
u4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQ
tFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrF
J8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE
6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA=</latexit>
nLY3EY87InuhI=">AB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxN
xszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3v
lHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gB
u4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQ
tFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrF
J8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE
6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA=</latexit>
e
Wp/G4byg43mK0=">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es
2ds9dveEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+P
mkZlmGDKaF0O6IGBZfYsNwKbKcaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF7
5Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhGrLmcBpqZsZTCkb0QF2HJU0QRNO5td
OyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18i
sGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7
hzVPei/fufSxaC14+cwx/4H3+AIWDjw=</latexit>
E
V
(c) Message-passing neural network
Edge block
Node block
Global block
V 0
φv
⇢e!v
φe
z
M8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaSNXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWldW19YLG8XNre2d3dLevq9lphjWmRSNUOqUfAE64Ybgc1UIY1DgY1w
cDX2G4+oNJfJrRmGMS0l/CIM2qs5LfTPr/DTqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJj1E+uXZETqzSJZFUthJDJurviZzGWg/j0Hb
G1PT1vDcW/NamYkugpwnaWYwYdNFUSaIkWT8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sDgAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpC
P</latexit>
N
rxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk
0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRa
HtjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8
/TZOSnA=</latexit>
N
rxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk
0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gBu4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQtFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRa
HtjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrFJ8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8
/TZOSnA=</latexit>
L
XzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmGDKaF0O6IGBZfYsNwKbKcaRIJbEWj
m5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF75Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhGrLmcBpqZsZTCkb0QF2HJU0QRNO5tdOyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZ
m4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18isGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7hzVPei/fufSxaC14+cwx/4H3+AIWDjw=</l
atexit>
E
V
(d) Non-local neural network
Edge block
Node block
Global block
u0
φu
g
rweUa+jkM8=">AB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6e
Eo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyura+uFjeLm1vbObmlvPzQy04TWieRSN
2NsKGeC1i2znDaVpjiNOW3Eg6ux3ik2jApbu1Q0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6m
H4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNF
/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQaLkowjK9H4dRlmhLh45gopm7FZE+1phYF1DR
hRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOB3iGV3jzpPfivXsf09YlbzZzAH/gf4Aq
E6Qnw=</latexit>
l
XL9yCq0no=">AB7XicbVC7SgNBFL0bXzG+oKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOz
yrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGS
XiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5
FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmV
LGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJmRqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtM
jA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/c
D5+AGXTkqw=</latexit>
l
XL9yCq0no=">AB7XicbVC7SgNBFL0bXzG+oKNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOz
yrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGS
XiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVkibkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5
FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmV
LGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJmRqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtM
jA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/c
D5+AGXTkqw=</latexit>
i
0ptSwAiKP4=">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveE
EPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmGDKaF0O
6IGBZfYsNwKbKcaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe
+Vv7p9xbIEpWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2P
GSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYhWZvU76XCOzYuwIZq7WwkbUk2ZdQGVXAjB8sur
pHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gf5A53Djxw=<
/latexit>
⇢e!u
φe
W
y0cvo5FmPz8I=">AB7XicbVA9SwNBEJ3zM8avqGBjsxgEq3Bno4VFwMYygrkEkjPubeaS
NXu3x+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvDAVXBvX/XaWldW19YLG8XNre2d3dLe
vq9lphjWmRSNUOqUfAE64Ybgc1UIY1DgY1wcDX2G4+oNJfJrRmGMS0l/CIM2qs5LfTPr/D
TqnsVtwJyCLxZqRcPfSf7gGg1il9tbuSZTEmhgmqdctzUxPkVBnOBI6K7UxjStmA9rBlaUJ
j1E+uXZETqzSJZFUthJDJurviZzGWg/j0HbG1PT1vDcW/NamYkugpwnaWYwYdNFUSaIkWT
8OulyhcyIoSWUKW5vJaxPFWXGBlS0IXjzLy8S/6ziuRXvxqZxCVMU4AiO4RQ8OIcqXEMN6sD
gAZ7hFd4c6bw4787HtHXJmc0cwB84nz+QDpCP</latexit>
nLY3EY87InuhI=">AB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxN
xszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3v
lHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gB
u4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQ
tFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrF
J8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE
6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA=</latexit>
nLY3EY87InuhI=">AB7XicbVC7SgNBFJ2NrxhfUcHGZjAIVmHXRguLgI1lBLMJGuYndxN
xszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3v
lHf3fJ2kikKDJjxRrZBo4ExAwzDoSUVkDjk0AyHl2O/eQ9Ks0TcmExCEJO+YBGjxFjJ78gB
u4VueJW3QnwIvFmpFI78B9k9v1e75Y/O72EpjEIQznRu250gQ5UYZRDqNSJ9UgCR2SPrQ
tFSQGHeSTa0f42Co9HCXKljB4ov6eyEmsdRaHtjMmZqDnvbH4n9dOTXQe5EzI1ICg0VRyrF
J8Ph13GMKqOGZJYQqZm/FdEAUocYGVLIhePMvLxL/tOq5Ve/apnGBpiQ3SETpCHzlANXaE
6aiCK7tAjekYvTuI8Oa/O27S14Mxm9tEfOB8/TZOSnA=</latexit>
e
Wp/G4byg43mK0=">AB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5stLAI2FhGMB+QnGFvM5es
2ds9dveEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+P
mkZlmGDKaF0O6IGBZfYsNwKbKcaRIJbEWjm5nfekJtuJL3dpximNCB5DFn1Dqp2U2H/AF7
5Ypf9ecgqyTISQVy1Hvlr25fsSxBaZmgxnQCP7XhGrLmcBpqZsZTCkb0QF2HJU0QRNO5td
OyZlT+iRW2pW0ZK7+npjQxJhxErnOhNqhWfZm4n9eJ7PxVTjhMs0sSrZYFGeCWEVmr5M+18i
sGDtCmebuVsKGVFNmXUAlF0Kw/PIqaV5UA78a3PmV2nUeRxFO4BTOIYBLqMEt1KEBDB7hGV7
hzVPei/fufSxaC14+cwx/4H3+AIWDjw=</latexit>
E
V
(e) Relation network
Edge block
Node block
Global block
u0
φu
G
BjsxgEq3Bno4VFwMYygrkEkjPubfaSNXu7y+6eEo78BxsLRWz9P3b+GzcfhSY+GHi8N8PMvFhxZqzvf3tLyura+uFjeLm1vbObmlvPzQy04TWieRSN2NsKGeC1i2znDaVpjiNOW3Eg6ux3ik2jApbu1Q
0SjFPcESRrB1UthWfXaXdUplv+JPgBZJMCPl6mH4dA8AtU7pq92VJEupsIRjY1qBr2yUY20Z4XRUbGeGKkwGuEdbjgqcUhPlk2tH6MQpXZRI7UpYNF/T+Q4NWaYxq4zxbZv5r2x+J/XymxyEeVMqMxSQa
aLkowjK9H4dRlmhLh45gopm7FZE+1phYF1DRhRDMv7xIwrNK4FeCG5fGJUxRgCM4hlMI4ByqcA01qAOB3iGV3jzpPfivXsf09YlbzZzAH/gf4AqE6Qnw=</latexit>
o
KNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGSXiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVki
bkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJm
RqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit>
o
KNzWAQrMKujRYWARvLCGYTSNYwO5lNxszODjOzyrLkH2wsFNHS/7HzA/wPJ49CEw9cOJxzL/feE0rOtHdL6ewtLyulZcL21sbm3vlHf3fJ2kitAGSXiWiHWlDNBG4YZTltSURyHnDbD4eXYb95TpVki
bkwmaRDjvmARI9hYye/IAbtNu+WKW3UnQIvEm5FK7cB/kNn3e71b/uz0EpLGVBjCsdZtz5UmyLEyjHA6KnVSTSUmQ9ynbUsFjqkO8sm1I3RslR6KEmVLGDRf0/kONY6i0PbGWMz0PeWPzPa6cmOg9yJm
RqCDTRVHKkUnQ+HXUY4oSwzNLMFHM3orIACtMjA2oZEPw5l9eJP5p1XOr3rVN4wKmKMIhHMEJeHAGNbiCOjSAwB08wjO8OInz5Lw6b9PWgjOb2Yc/cD5+AGXTkqw=</latexit>
S
1tFoNgFe5stLAI2FhGMB+QnGFvM5es2ds9dveEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wtr6xuVXcLu3s7u0flA+PmkZlmGDKaF0O6IGBZfYsNwKbKcaRIJbEWjm5nfekJtuJL3dpxi
mNCB5DFn1Dqp2U2H/CHrlSt+1Z+DrJIgJxXIUe+Vv7p9xbIEpWCGtMJ/NSGE6otZwKnpW5mMKVsRAfYcVTSBE04mV87JWdO6ZNYaVfSkrn6e2JCE2PGSeQ6E2qHZtmbif95nczGV+GEyzSzKNliUZwJYh
WZvU76XCOzYuwIZq7WwkbUk2ZdQGVXAjB8surpHlRDfxqcOdXatd5HEU4gVM4hwAuoQa3UIcGMHiEZ3iFN095L96797FoLXj5zDH8gf5A53Djxw=</latexit>
⇢v!u
φv
V
u
(f) Deep set
Figure 4: Diﬀerent internal GN block conﬁgurations.
See Section 3.2 for details on the notation,
and Section 4 for details about each variant.
(a) A full GN predicts node, edge, and global output
attributes based on incoming node, edge, and global attributes.
(b) An independent, recurrent
update block takes input and hidden graphs, and the φ functions are RNNs (Sanchez-Gonzalez
et al., 2018).
(c) An MPNN (Gilmer et al., 2017) predicts node, edge, and global output attributes
based on incoming node, edge, and global attributes.
Note that the global prediction does not
include aggregated edges.
(d) A NLNN (Wang et al., 2018c) only predicts node output attributes.
(e) A relation network (Raposo et al., 2017; Santoro et al., 2017) only uses the edge predictions
to predict global attributes.
(f) A Deep Set (Zaheer et al., 2017) bypasses the edge update and
predicts updated global attributes.
4.2.1
Message-passing neural network (MPNN)
Gilmer et al.
(2017)’s MPNN generalizes a number of previous architectures and can be translated
naturally into the GN formalism.
Following the MPNN paper’s terminology (see Gilmer et al.
(2017), pages 2-4):
◦the message function, Mt, plays the role of the GN’s φe, but does not take u as input,
◦elementwise summation is used for the GN’s ρe→v,
◦the update function, Ut, plays the role of the GN’s φv,
16
V
↵e
βe
⇢e!v
φe
jl1NYWV1b3hl2d3b3HLZ1kiGTJSJRYBFi03AjJMHEB0MZ+6V58NMU/Zj2JY848ZKjbt26U5Bl41J5fLj+4EAM1/9KEZTFKTV5fFAlXYWI+9i1VNIYtZ9P9TE6EJEULWIVP3dkdNY61E2GRMUAhP6+bj51UHJZOiTBCTkMRJOQKREjShT3O5K2IA9Tk+V8Z0W/V3E9AYhEl4A51IYGNIFBCI/DC8O
d56V+dtFi0485D+AP/Qd/I8A</ltit>
N
47Li0LKW8b2bRd2d6ThXDGtFJ+1Si4JhRAUjX2D71+O/MYtK1jW0GCXYi2U85IK1VCkW35I5B5k3JWL96+7ft81/tIOYRFKTVW5ilkVBOBA7VRjQlfdFlQR6k423NIjSkDBWtQhY/V3R0YjQRb5MRNT09643E/7WKTZlkhUbDITAUMRkdTQKkBkIQ2hPWY1+TtE7Zk+dJ/TkSW6hbL5BDbhEI7B1MVUAYMAiH
R3hPDk+iCM+3Z9X8AGkQ8</ltit>
N
47Li0LKW8b2bRd2d6ThXDGtFJ+1Si4JhRAUjX2D71+O/MYtK1jW0GCXYi2U85IK1VCkW35I5B5k3JWL96+7ft81/tIOYRFKTVW5ilkVBOBA7VRjQlfdFlQR6k423NIjSkDBWtQhY/V3R0YjQRb5MRNT09643E/7WKTZlkhUbDITAUMRkdTQKkBkIQ2hPWY1+TtE7Zk+dJ/TkSW6hbL5BDbhEI7B1MVUAYMAiH
R3hPDk+iCM+3Z9X8AGkQ8</ltit>
2
G5tb25Wd67/Fh7i4Y9PNHUNL2RWKlFGU0MYERD38787MVb6ASZCBI21DKWKGTW+1O3QO8UtShI/WIU4QiNXN+TMCZQiW1UFRb4A1F31HNEGDY7lJ7JSJtTSOb74CJdZOktAlE4Yj+NP+8f7dVBIUNF98FOKYER5NIGFRTRh3Ei3K+EjZhHV03VlA7KONb813R1VOAUACfLiCJtBC9AIYJIU3T3Yi
XMCfB9/kDCGMA</ltit> φv
V 0
Figure 5: NLNNs as GNs.
A schematic showing how NLNNs (Wang et al., 2018c) are implemented
by the φe and ρe→v under the GN framework.
Typically, NLNNs assume that diﬀerent regions of
an image (or words in a sentence) correspond to nodes in a fully connected graph, and the attention
mechanism deﬁnes a weighted sum over nodes during the aggregation step.
◦the readout function, R, plays the role of the GN’s φu, but does not take u or E′ as input,
and thus an analog to the GN’s ρe→u is not required;
◦dmaster serves a roughly similar purpose to the GN’s u, but is deﬁned as an extra node
connected to all others, and thus does not inﬂuence the edge and global updates directly.
It
can then be represented in the GN’s V .
Figure 4c shows how an MPNN is structured, according to the GN framework.
For details and
various MPNN architectures, see the Appendix.
4.2.2
Non-local neural networks (NLNN)
Wang et al.
(2018c)’s NLNN, which uniﬁes various “intra-/self-/vertex-/graph-attention” approaches
(Lin et al., 2017; Vaswani et al., 2017; Hoshen, 2017; Veliˇckovi´c et al., 2018; Shaw et al., 2018),
can also be translated into the GN formalism.
The label “attention” refers to how the nodes are
updated: each node update is based on a weighted sum of (some function of) the node attributes of
its neighbors, where the weight between a node and one of its neighbors is computed by a scalar
pairwise function between their attributes (and then normalized across neighbors).
The published
NLNN formalism does not explicitly include edges, and instead computes pairwise attention weights
between all nodes.
But various NLNN-compliant models, such as the vertex attention interaction
network (Hoshen, 2017) and graph attention network (Veliˇckovi´c et al., 2018), are able to handle
explicit edges by eﬀectively setting to zero the weights between nodes which do not share an edge.
As Figures 4d and 5 illustrate, the φe is factored into the scalar pairwise-interaction function
which returns the unnormalized attention term, denoted αe (vrk, vsk) = a′
k, and a vector-valued
non-pairwise term, denoted βe (vsk) = b′
k.
In the ρe→v aggregation, the a′
k terms are normalized
across each receiver’s edges, b′
k, and elementwise summed:
φe (ek, vrk, vsk, u) := fe (vrk, vsk)
= (αe (vrk, vsk) , βe (vsk)) = (a′
k, b′
k) = e′
k
φv  ¯e′
i, vi, u
 := fv(¯e′
i)
ρe→v  E′
i
 :=
1
P
{k: rk=i} a′
k
X
{k: rk=i}
a′
kb′
k
In the NLNN paper’s terminology (see Wang et al.
(2018c), pages 2-4):
◦their f plays the role of the above α,
17
◦their g plays the role of the above β.
This formulation may be helpful for focusing only on those interactions which are most relevant for
the downstream task, especially when the input entities were a set, from which a graph was formed
by adding all possible edges between them.
Vaswani et al.
(2017)’s multi-headed self-attention mechanism adds an interesting feature, where
the φe and ρe→v are implemented by a parallel set of functions, whose results are concatenated
together as the ﬁnal step of ρe→v.
This can be interpreted as using typed edges, where the diﬀerent
types index into diﬀerent φe component functions, analogous to Li et al.
(2016).
For details and various NLNN architectures, see the Appendix.
4.2.3
Other graph network variants
The full GN (Equation 2) can be used to predict a full graph, or any subset of (u′, V ′, E′), as
outlined in Section 4.1.1.
For example, to predict a global property of a graph, V ′ and E′ can just
be ignored.
Similarly, if global, node, or edge attributes are unspeciﬁed in the inputs, those vectors
can be zero-length, i.e., not taken as explicit input arguments.
The same idea applies for other GN
variants which do not use the full set of mapping (φ) and reduction (ρ) functions.
For instance,
Interaction Networks (Battaglia et al., 2016; Watters et al., 2017) and the Neural Physics Engine
(Chang et al., 2017) use a full GN but for the absence of the global to update the edge properties
(see Appendix for details).
Various models, including CommNet (Sukhbaatar et al., 2016), structure2vec (Dai et al., 2016)
(in the version of (Dai et al., 2017)), and Gated Graph Sequence Neural Networks (Li et al., 2016)
have used a φe which does not directly compute pairwise interactions, but instead ignore the receiver
node, operating only on the sender node and in some cases an edge attribute.
This can be expressed
by implementations of φe with the following signatures, such as:
φe (ek, vrk, vsk, u) := fe (vsk)
or
φe (ek, vrk, vsk, u) := vsk + fe (ek)
or
φe (ek, vrk, vsk, u) := fe (ek, vsk) .
See the Appendix for further details.
Relation Networks (Raposo et al., 2017; Santoro et al., 2017) bypass the node update entirely
and predict the global output from pooled edge information directly (see also Figure 4e),
φe (ek, vrk, vsk, u) := fe (vrk, vsk) = NNe ([vrk, vsk])
φu  ¯e′, ¯v′, u
 := fu  ¯e′
= NNu
 ¯e′
ρe→u  E′ :=
=
X
k
e′
k
Deep Sets (Zaheer et al., 2017) bypass the edges update completely and predict the global output
from pooled nodes information directly (Figure 4f),
φv (¯ei, vi, u) := fv (vi, u) = NNv ([vi, u])
φu  ¯e′, ¯v′, u
 := fu  ¯v′
= NNu
 ¯v′
ρv→u  V ′ :=
=
X
i
v′
i
PointNet (Qi et al., 2017) use similar update rule, with a max-aggregation for ρv→u and a two-step
node update.
18
GM
GN1
GN2
GNM
...
G1
G0
GM
G0
GNcore
⇥M
(a) Composition of GN blocks
GNenc
GNdec
GNcore
⇥M
Gout
Ginp
(b) Encode-process-decode
GNenc
GNdec
GNcore
⇥M
Gt
hid
Gt−1
hid
Gt
out
Gt
inp
(c) Recurrent GN architecture
Figure 6: (a) An example composing multiple GN blocks in sequence to form a GN “core”.
Here,
the GN blocks can use shared weights, or they could be independent.
(b) The encode-process-decode
architecture, which is a common choice for composing GN blocks (see Section 4.3).
Here, a GN
encodes an input graph, which is then processed by a GN core.
The output of the core is decoded
by a third GN block into an output graph, whose nodes, edges, and/or global attributes would be
used for task-speciﬁc purposes.
(c) The encode-process-decode architecture applied in a sequential
setting in which the core is also unrolled over time (potentially using a GRU or LSTM architecture),
in addition to being repeated within each time step.
Here, merged lines indicate concatenation, and
split lines indicate copying.
4.3
Composable multi-block architectures
A key design principle of graph networks is constructing complex architectures by composing GN
blocks.
We deﬁned a GN block as always taking a graph comprised of edge, node, and global
elements as input, and returning a graph with the same constituent elements as output (simply
passing through the input elements to the output when those elements are not explicitly updated).
This graph-to-graph input/output interface ensures that the output of one GN block can be passed
as input to another, even if their internal conﬁgurations are diﬀerent, similar to the tensor-to-tensor
interface of the standard deep learning toolkit.
In the most basic form, two GN blocks, GN1 and
GN2, can be composed as GN1 ◦GN2 by passing the output of the ﬁrst as input to the second:
G′ = GN2(GN1(G)).
Arbitrary numbers of GN blocks can be composed, as show in Figure 6a.
The blocks can
be unshared (diﬀerent functions and/or parameters, analogous to layers of a CNN), GN1 ̸=
GN2 ̸= · · · ̸= GNM, or shared (reused functions and parameters, analogous to an unrolled RNN),
GN1 = GN2 = · · · = GNM.
The white box around the GNcore in Figure 6a represents M repeated
internal processing sub-steps, with either shared or unshared GN blocks.
Shared conﬁgurations
are analogous to message-passing (Gilmer et al., 2017), where the same local update procedure is
applied iteratively to propagate information across the structure (Figure 7).
If we exclude the global
u (which aggregates information from across the nodes and edges), the information that a node
has access to after m steps of propagation is determined by the set of nodes and edges that are at
most m hops away.
This can be interpreted as breaking down a complex computation into smaller
elementary steps.
The steps can also be used to capture sequentiality in time.
In our ball-spring
example, if each propagation step predicts the physical dynamics over one time step of duration ∆t,
then the M propagation steps result in a total simulation time of, M ·∆t.
A common architecture design is what we call the encode-process-decode conﬁguration (Hamrick
et al.
(2018); also see Figure 6ba): an input graph, Ginp is transformed into a latent representation,
G0, by an encoder, GNenc; a shared core block, GNcore, is applied M times to return GM; and
ﬁnally an output graph, Gout, is decoded by GNdec.
For example, in our running example, the
encoder might compute the initial forces and interaction energies between the balls, the core might
19
m = 0
m = 1
m = 2
m = 3
m = 0
m = 1
m = 2
m = 3
Figure 7: Example of message passing.
Each row highlights the information that diﬀuses through
the graph starting from a particular node.
In the top row, the node of interest is in the upper
right; in the bottom row, the node of interest is in the bottom right.
Shaded nodes indicate how far
information from the original node can travel in m steps of message passing; bolded edges indicate
which edges that information has the potential to travel across.
Note that during the full message
passing procedure, this propagation of information happens simultaneously for all nodes and edges
in the graph (not just the two shown here).
apply an elementary dynamics update, and the decoder might read out the ﬁnal positions from the
updated graph state.
Similar to the encode-process-decode design, recurrent GN-based architectures can be built by
maintaining a hidden graph, Gt
hid, taking as input an observed graph, Gt
inp, and returning an output
graph, Gt
out, on each step (see Figure 6c).
This type of architecture can be particularly useful for
predicting sequences of graphs, such as predicting the trajectory of a dynamical system over time
(e.g.
Sanchez-Gonzalez et al., 2018).
The encoded graph, output by GNenc, must have the same
structure as Gt
hid, and they can be easily combined by concatenating their corresponding ek, vi,
and u vectors (where the upward arrow merges into the left-hand horizontal arrow in Figure 6c),
before being passed to GNcore.
For the output, the Gt
hid is copied (where the right-hand horizontal
arrow splits into the downward arrow in Figure 6c) and decoded by GNdec.
This design reuses GN
blocks in several ways: GNenc, GNdec, and GNcore are shared across each step, t; and within each
step, GNcore may perform multiple shared sub-steps.
Various other techniques for designing GN-based architectures can be useful.
Graph skip
connections, for example, would concatenate a GN block’s input graph, Gm, with its output graph,
Gm+1, before proceeding to further computations.
Merging and smoothing input and hidden graph
information, as in Figure 6c, can use LSTM- or GRU-style gating schemes, instead of simple
concatenation (Li et al., 2016).
Or distinct, recurrent GN blocks (e.g.
Figure 4b) can be composed
before and/or after other GN blocks, to improve stability in the representations over multiple
propagation steps (Sanchez-Gonzalez et al., 2018).
4.4
Implementing graph networks in code
Similar to CNNs (see Figure 1), which are naturally parallelizable (e.g.
on GPUs), GNs have a
natural parallel structure: since the φe and φv functions in Equation 1 are shared over the edges
and nodes, respectively, they can be computed in parallel.
In practice, this means that with respect
20
Box 4: Graph Nets open-source software library: github.com/deepmind/graph nets
We have released an open-source library for building GNs in Tensorﬂow/Sonnet.
It includes
demos of how to create, manipulate, and train GNs to reason about graph-structured data, on
a shortest path-ﬁnding task, a sorting task, and a physical prediction task.
Each demo uses the
same GN architecture, which highlights the ﬂexibility of the approach.
Shortest path demo: tinyurl.com/gn-shortest-path-demo
This demo creates random graphs, and trains a GN to label the nodes and edges on the shortest
path between any two nodes.
Over a sequence of message-passing steps (as depicted by each
step’s plot), the model reﬁnes its prediction of the shortest path.
Sort demo: tinyurl.com/gn-sort-demo
This demo creates lists of random numbers, and trains a GN to sort the list.
After a sequence
of message-passing steps, the model makes an accurate prediction of which elements (columns
in the ﬁgure) come next after each other (rows).
Physics demo: tinyurl.com/gn-physics-demo
This demo creates random mass-spring physical systems, and trains a GN to predict the state of
the system on the next timestep.
The model’s next-step predictions can be fed back in as input
to create a rollout of a future trajectory.
Each subplot below shows the true and predicted
mass-spring system states over 50 timesteps.
This is similar to the model and experiments in
(Battaglia et al., 2016)’s “interaction networks”.
21
to φe and φv, the nodes and edges can be treated like the batch dimension in typical mini-batch
training regimes.
Moreover, several graphs can be naturally batched together by treating them as
disjoint components of a larger graph.
With some additional bookkeeping, this allows batching
together the computations made on several independent graphs.
Reusing φe and φv also improves GNs’ sample eﬃciency.
Again, analogous to a convolutional
kernel, the number of samples which are used to optimize a GN’s φe and φv functions is the number
of edges and nodes, respectively, across all training graphs.
For example, in the balls example
from Sec.
3.2, a scene with four balls which are all connected by springs will provide twelve (4 × 3)
examples of the contact interaction between them.
We have released an open-source software library for building GNs, which can be found here:
github.com/deepmind/graph nets.
See Box 4 for an overview.
4.5
Recurrent neural networks (RNNs) are a rich class of dynamic models that have
been used to generate sequences in domains as diverse as music [6, 4], text [30]
and motion capture data [29].
RNNs can be trained for sequence generation by
processing real data sequences one step at a time and predicting what comes
next.
Assuming the predictions are probabilistic, novel sequences can be gener-
ated from a trained network by iteratively sampling from the network’s output
distribution, then feeding in the sample as input at the next step.
In other
words by making the network treat its inventions as if they were real, much like
a person dreaming.
Although the network itself is deterministic, the stochas-
ticity injected by picking samples induces a distribution over sequences.
This
distribution is conditional, since the internal state of the network, and hence its
predictive distribution, depends on the previous inputs.
RNNs are ‘fuzzy’ in the sense that they do not use exact templates from
the training data to make predictions, but rather—like other neural networks—
use their internal representation to perform a high-dimensional interpolation
between training examples.
This distinguishes them from n-gram models and
compression algorithms such as Prediction by Partial Matching [5], whose pre-
dictive distributions are determined by counting exact matches between the
recent history and the training set.
The result—which is immediately appar-
1
arXiv:1308.0850v5  [cs.NE]  5 Jun 2014
ent from the samples in this paper—is that RNNs (unlike template-based al-
gorithms) synthesise and reconstitute the training data in a complex way, and
rarely generate the same thing twice.
Furthermore, fuzzy predictions do not suf-
fer from the curse of dimensionality, and are therefore much better at modelling
real-valued or multivariate data than exact matches.
In principle a large enough RNN should be suﬃcient to generate sequences
of arbitrary complexity.
In practice however, standard RNNs are unable to
store information about past inputs for very long [15].
As well as diminishing
their ability to model long-range structure, this ‘amnesia’ makes them prone to
instability when generating sequences.
The problem (common to all conditional
generative models) is that if the network’s predictions are only based on the last
few inputs, and these inputs were themselves predicted by the network, it has
little opportunity to recover from past mistakes.
Having a longer memory has
a stabilising eﬀect, because even if the network cannot make sense of its recent
history, it can look further back in the past to formulate its predictions.
The
problem of instability is especially acute with real-valued data, where it is easy
for the predictions to stray from the manifold on which the training data lies.
One remedy that has been proposed for conditional models is to inject noise into
the predictions before feeding them back into the model [31], thereby increasing
the model’s robustness to surprising inputs.
However we believe that a better
memory is a more profound and eﬀective solution.
Long Short-term Memory (LSTM) [16] is an RNN architecture designed to
be better at storing and accessing information than standard RNNs.
LSTM has
recently given state-of-the-art results in a variety of sequence processing tasks,
including speech and handwriting recognition [10, 12].
The main goal of this
paper is to demonstrate that LSTM can use its memory to generate complex,
realistic sequences containing long-range structure.
Section 2 deﬁnes a ‘deep’ RNN composed of stacked LSTM layers, and ex-
plains how it can be trained for next-step prediction and hence sequence gener-
ation.
Section 3 applies the prediction network to text from the Penn Treebank
and Hutter Prize Wikipedia datasets.
The network’s performance is compet-
itive with state-of-the-art language models, and it works almost as well when
predicting one character at a time as when predicting one word at a time.
The
highlight of the section is a generated sample of Wikipedia text, which showcases
the network’s ability to model long-range dependencies.
Section 4 demonstrates
how the prediction network can be applied to real-valued data through the use
of a mixture density output layer, and provides experimental results on the IAM
Online Handwriting Database.
It also presents generated handwriting samples
proving the network’s ability to learn letters and short words direct from pen
traces, and to model global features of handwriting style.
Section 5 introduces
an extension to the prediction network that allows it to condition its outputs on
a short annotation sequence whose alignment with the predictions is unknown.
This makes it suitable for handwriting synthesis, where a human user inputs
a text and the algorithm generates a handwritten version of it.
The synthesis
network is trained on the IAM database, then used to generate cursive hand-
writing samples, some of which cannot be distinguished from real data by the
2
Figure 1: Deep recurrent neural network prediction architecture.
The
circles represent network layers, the solid lines represent weighted connections
and the dashed lines represent predictions.
naked eye.
A method for biasing the samples towards higher probability (and
greater legibility) is described, along with a technique for ‘priming’ the sam-
ples on real data and thereby mimicking a particular writer’s style.
Finally,
concluding remarks and directions for future work are given in Section 6.
2
Prediction Network
Fig.
1 illustrates the basic recurrent neural network prediction architecture used
in this paper.
An input vector sequence x = (x1, .
.
.
, xT ) is passed through
weighted connections to a stack of N recurrently connected hidden layers to
compute ﬁrst the hidden vector sequences hn = (hn
1, .
.
.
, hn
T ) and then the
output vector sequence y = (y1, .
.
.
, yT ).
Each output vector yt is used to
parameterise a predictive distribution Pr(xt+1|yt) over the possible next inputs
xt+1.
The ﬁrst element x1 of every input sequence is always a null vector whose
entries are all zero; the network therefore emits a prediction for x2, the ﬁrst
real input, with no prior information.
The network is ‘deep’ in both space
and time, in the sense that every piece of information passing either vertically
or horizontally through the computation graph will be acted on by multiple
successive weight matrices and nonlinearities.
Note the ‘skip connections’ from the inputs to all hidden layers, and from
all hidden layers to the outputs.
These make it easier to train deep networks,
3
by reducing the number of processing steps between the bottom of the network
and the top, and thereby mitigating the ‘vanishing gradient’ problem [1].
In
the special case that N = 1 the architecture reduces to an ordinary, single layer
next step prediction RNN.
The hidden layer activations are computed by iterating the following equa-
tions from t = 1 to T and from n = 2 to N:
h1
t = H
 Wih1xt + Wh1h1h1
t−1 + b1
h

(1)
hn
t = H
 Wihnxt + Whn−1hnhn−1
t
+ Whnhnhn
t−1 + bn
h

(2)
where the W terms denote weight matrices (e.g.
Wihn is the weight matrix
connecting the inputs to the nth hidden layer, Wh1h1 is the recurrent connection
at the ﬁrst hidden layer, and so on), the b terms denote bias vectors (e.g.
by is
output bias vector) and H is the hidden layer function.
Given the hidden sequences, the output sequence is computed as follows:
ˆyt = by +
N
X
n=1
Whnyhn
t
(3)
yt = Y(ˆyt)
(4)
where Y is the output layer function.
The complete network therefore deﬁnes
a function, parameterised by the weight matrices, from input histories x1:t to
output vectors yt.
The output vectors yt are used to parameterise the predictive distribution
Pr(xt+1|yt) for the next input.
The form of Pr(xt+1|yt) must be chosen carefully
to match the input data.
In particular, ﬁnding a good predictive distribution
for high-dimensional, real-valued data (usually referred to as density modelling),
can be very challenging.
The probability given by the network to the input sequence x is
Pr(x) =
T
Y
t=1
Pr(xt+1|yt)
(5)
and the sequence loss L(x) used to train the network is the negative logarithm
of Pr(x):
L(x) = −
T
X
t=1
log Pr(xt+1|yt)
(6)
The partial derivatives of the loss with respect to the network weights can be
eﬃciently calculated with backpropagation through time [33] applied to the
computation graph shown in Fig.
1, and the network can then be trained with
gradient descent.
2.1
Long Short-Term Memory
In most RNNs the hidden layer function H is an elementwise application of a
sigmoid function.
However we have found that the Long Short-Term Memory
4
Figure 2: Long Short-term Memory Cell
(LSTM) architecture [16], which uses purpose-built memory cells to store infor-
mation, is better at ﬁnding and exploiting long range dependencies in the data.
Fig.
2 illustrates a single LSTM memory cell.
For the version of LSTM used in
this paper [7] H is implemented by the following composite function:
it = σ (Wxixt + Whiht−1 + Wcict−1 + bi)
(7)
ft = σ (Wxfxt + Whfht−1 + Wcfct−1 + bf)
(8)
ct = ftct−1 + it tanh (Wxcxt + Whcht−1 + bc)
(9)
ot = σ (Wxoxt + Whoht−1 + Wcoct + bo)
(10)
ht = ot tanh(ct)
(11)
where σ is the logistic sigmoid function, and i, f, o and c are respectively the
input gate, forget gate, output gate, cell and cell input activation vectors, all of
which are the same size as the hidden vector h.
The weight matrix subscripts
have the obvious meaning, for example Whi is the hidden-input gate matrix,
Wxo is the input-output gate matrix etc.
The weight matrices from the cell
to gate vectors (e.g.
Wci) are diagonal, so element m in each gate vector only
receives input from element m of the cell vector.
The bias terms (which are
added to i, f, c and o) have been omitted for clarity.
The original LSTM algorithm used a custom designed approximate gradi-
ent calculation that allowed the weights to be updated after every timestep [16].
However the full gradient can instead be calculated with backpropagation through
time [11], the method used in this paper.
One diﬃculty when training LSTM
with the full gradient is that the derivatives sometimes become excessively large,
5
leading to numerical problems.
To prevent this, all the experiments in this pa-
per clipped the derivative of the loss with respect to the network inputs to the
LSTM layers (before the sigmoid and tanh functions are applied) to lie within
a predeﬁned range1.
3
Text Prediction
Text data is discrete, and is typically presented to neural networks using ‘one-
hot’ input vectors.
That is, if there are K text classes in total, and class k is fed
in at time t, then xt is a length K vector whose entries are all zero except for
the kth, which is one.
Pr(xt+1|yt) is therefore a multinomial distribution, which
can be naturally parameterised by a softmax function at the output layer:
Pr(xt+1 = k|yt) = yk
t =
exp
 ˆyk
t

PK
k′=1 exp
 ˆyk′
t

(12)
Substituting into Eq.
(6) we see that
L(x) = −
T
X
t=1
log yxt+1
t
(13)
=⇒∂L(x)
∂ˆyk
t
= yk
t −δk,xt+1
(14)
The only thing that remains to be decided is which set of classes to use.
In
most cases, text prediction (usually referred to as language modelling) is per-
formed at the word level.
K is therefore the number of words in the dictionary.
This can be problematic for realistic tasks, where the number of words (in-
cluding variant conjugations, proper names, etc.) often exceeds 100,000.
As
well as requiring many parameters to model, having so many classes demands a
huge amount of training data to adequately cover the possible contexts for the
words.
In the case of softmax models, a further diﬃculty is the high computa-
tional cost of evaluating all the exponentials during training (although several
methods have been to devised make training large softmax layers more eﬃcient,
including tree-based models [25, 23], low rank approximations [27] and stochas-
tic derivatives [26]).
Furthermore, word-level models are not applicable to text
data containing non-word strings, such as multi-digit numbers or web addresses.
Character-level language modelling with neural networks has recently been
considered [30, 24], and found to give slightly worse performance than equiv-
alent word-level models.
Nonetheless, predicting one character at a time is
more interesting from the perspective of sequence generation, because it allows
the network to invent novel words and strings.
In general, the experiments in
this paper aim to predict at the ﬁnest granularity found in the data, so as to
maximise the generative ﬂexibility of the network.
1In fact this technique was used in all my previous papers on LSTM, and in my publicly
available LSTM code, but I forgot to mention it anywhere—mea culpa.
6
3.1
Penn Treebank Experiments
The ﬁrst set of text prediction experiments focused on the Penn Treebank por-
tion of the Wall Street Journal corpus [22].
This was a preliminary study whose
main purpose was to gauge the predictive power of the network, rather than to
generate interesting sequences.
Although a relatively small text corpus (a little over a million words in total),
the Penn Treebank data is widely used as a language modelling benchmark.
The
training set contains 930,000 words, the validation set contains 74,000 words and
the test set contains 82,000 words.
The vocabulary is limited to 10,000 words,
with all other words mapped to a special ‘unknown word’ token.
The end-of-
sentence token was included in the input sequences, and was counted in the
sequence loss.
The start-of-sentence marker was ignored, because its role is
already fulﬁlled by the null vectors that begin the sequences (c.f.
Section 2).
The experiments compared the performance of word and character-level
LSTM predictors on the Penn corpus.
In both cases, the network architecture
was a single hidden layer with 1000 LSTM units.
For the character-level network
the input and output layers were size 49, giving approximately 4.3M weights in
total, while the word-level network had 10,000 inputs and outputs and around
54M weights.
The comparison is therefore somewhat unfair, as the word-level
network had many more parameters.
However, as the dataset is small, both net-
works were easily able to overﬁt the training data, and it is not clear whether the
character-level network would have beneﬁted from more weights.
All networks
were trained with stochastic gradient descent, using a learn rate of 0.0001 and a
momentum of 0.99.
The LSTM derivates were clipped in the range [−1, 1] (c.f.
Section 2.1).
Neural networks are usually evaluated on test data with ﬁxed weights.
For
prediction problems however, where the inputs are the targets, it is legitimate
to allow the network to adapt its weights as it is being evaluated (so long as
it only sees the test data once).
Mikolov refers to this as dynamic evaluation.
Dynamic evaluation allows for a fairer comparison with compression algorithms,
for which there is no division between training and test sets, as all data is only
predicted once.
Since both networks overﬁt the training data, we also experiment with two
types of regularisation: weight noise [18] with a std.
deviation of 0.075 applied
to the network weights at the start of each training sequence, and adaptive
weight noise [8], where the variance of the noise is learned along with the weights
using a Minimum description Length (or equivalently, variational inference) loss
function.
When weight noise was used, the network was initialised with the
ﬁnal weights of the unregularised network.
Similarly, when adaptive weight
noise was used, the weights were initialised with those of the network trained
with weight noise.
We have found that retraining with iteratively increased
regularisation is considerably faster than training from random weights with
regularisation.
Adaptive weight noise was found to be prohibitively slow for
the word-level network, so it was regularised with ﬁxed-variance weight noise
only.
One advantage of adaptive weight is that early stopping is not needed
7
Table 1: Penn Treebank Test Set Results.
‘BPC’ is bits-per-character.
‘Error’ is next-step classiﬁcation error rate, for either characters or words.
Input
Regularisation
Dynamic
BPC
Perplexity
Error (%)
Epochs
Char
none
no
1.32
167
28.5
9
char
none
yes
1.29
148
28.0
9
char
weight noise
no
1.27
140
27.4
25
char
weight noise
yes
1.24
124
26.9
25
char
adapt.
wt.
noise
no
1.26
133
27.4
26
char
adapt.
wt.
noise
yes
1.24
122
26.9
26
word
none
no
1.27
138
77.8
11
word
none
yes
1.25
126
76.9
11
word
weight noise
no
1.25
126
76.9
14
word
weight noise
yes
1.23
117
76.2
14
(the network can safely be stopped at the point of minimum total ‘description
length’ on the training data).
However, to keep the comparison fair, the same
training, validation and test sets were used for all experiments.
The results are presented with two equivalent metrics: bits-per-character
(BPC), which is the average value of −log2 Pr(xt+1|yt) over the whole test set;
and perplexity which is two to the power of the average number of bits per word
(the average word length on the test set is about 5.6 characters, so perplexity ≈
25.6BP C).
Perplexity is the usual performance measure for language modelling.
Table 1 shows that the word-level RNN performed better than the character-
level network, but the gap appeared to close when regularisation is used.
Overall
the results compare favourably with those collected in Tomas Mikolov’s the-
sis [23].
For example, he records a perplexity of 141 for a 5-gram with Keyser-
Ney smoothing, 141.8 for a word level feedforward neural network, 131.1 for the
state-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-
uated word-level RNN.
However by combining multiple RNNs, a 5-gram and a
cache model in an ensemble, he was able to achieve a perplexity of 89.4.
Inter-
estingly, the beneﬁt of dynamic evaluation was far more pronounced here than
in Mikolov’s thesis (he records a perplexity improvement from 124.7 to 123.2
with word-level RNNs).
This suggests that LSTM is better at rapidly adapting
to new data than ordinary RNNs.
3.2
Wikipedia Experiments
In 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following
challenge, commonly known as Hutter prize [17]: to compress the ﬁrst 100
million bytes of the complete English Wikipedia data (as it was at a certain
time on March 3rd 2006) to as small a ﬁle as possible.
The ﬁle had to include
not only the compressed data, but also the code implementing the compression
algorithm.
Its size can therefore be considered a measure of the minimum
description length [13] of the data using a two part coding scheme.
Wikipedia data is interesting from a sequence generation perspective because
8
it contains not only a huge range of dictionary words, but also many character
sequences that would not be included in text corpora traditionally used for
language modelling.
For example foreign words (including letters from non-
Latin alphabets such as Arabic and Chinese), indented XML tags used to deﬁne
meta-data, website addresses, and markup used to indicate page formatting such
as headings, bullet points etc.
An extract from the Hutter prize dataset is shown
in Figs.
3 and 4.
The ﬁrst 96M bytes in the data were evenly split into sequences of 100 bytes
and used to train the network, with the remaining 4M were used for validation.
The data contains a total of 205 one-byte unicode symbols.
The total number
of characters is much higher, since many characters (especially those from non-
Latin languages) are deﬁned as multi-symbol sequences.
In keeping with the
principle of modelling the smallest meaningful units in the data, the network
predicted a single byte at a time, and therefore had size 205 input and output
layers.
Wikipedia contains long-range regularities, such as the topic of an article,
which can span many thousand words.
To make it possible for the network to
capture these, its internal state (that is, the output activations ht of the hidden
layers, and the activations ct of the LSTM cells within the layers) were only reset
every 100 sequences.
Furthermore the order of the sequences was not shuﬄed
during training, as it usually is for neural networks.
The network was therefore
able to access information from up to 10K characters in the past when making
predictions.
The error terms were only backpropagated to the start of each 100
byte sequence, meaning that the gradient calculation was approximate.
This
form of truncated backpropagation has been considered before for RNN lan-
guage modelling [23], and found to speed up training (by reducing the sequence
length and hence increasing the frequency of stochastic weight updates) without
aﬀecting the network’s ability to learn long-range dependencies.
A much larger network was used for this data than the Penn data (reﬂecting
the greater size and complexity of the training set) with seven hidden layers of
700 LSTM cells, giving approximately 21.3M weights.
The network was trained
with stochastic gradient descent, using a learn rate of 0.0001 and a momentum
of 0.9.
It took four training epochs to converge.
The LSTM derivates were
clipped in the range [−1, 1].
As with the Penn data, we tested the network on the validation data with
and without dynamic evaluation (where the weights are updated as the data
is predicted).
As can be seen from Table 2 performance was much better with
dynamic evaluation.
This is probably because of the long range coherence of
Wikipedia data; for example, certain words are much more frequent in some
articles than others, and being able to adapt to this during evaluation is ad-
vantageous.
It may seem surprising that the dynamic results on the validation
set were substantially better than on the training set.
However this is easily
explained by two factors: ﬁrstly, the network underﬁt the training data, and
secondly some portions of the data are much more diﬃcult than others (for
example, plain text is harder to predict than XML tags).
To put the results in context, the current winner of the Hutter Prize (a
9
Table 2: Wikipedia Results (bits-per-character)
Train
Validation (static)
Validation (dynamic)
1.42
1.67
1.33
variant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same
data (including the code required to implement the algorithm), mainstream
compressors such as zip generally get more than 2, and a character level RNN
applied to a text-only version of the data (i.e.
with all the XML, markup tags
etc.
removed) achieved 1.54 on held-out data, which improved to 1.47 when the
RNN was combined with a maximum entropy model [24].
A four page sample generated by the prediction network is shown in Figs.
5
to 8.
The sample shows that the network has learned a lot of structure from
the data, at a wide range of diﬀerent scales.
Most obviously, it has learned a
large vocabulary of dictionary words, along with a subword model that enables
it to invent feasible-looking words and names: for example “Lochroom River”,
“Mughal Ralvaldens”, “submandration”, “swalloped”.
It has also learned basic
punctuation, with commas, full stops and paragraph breaks occurring at roughly
the right rhythm in the text blocks.
Being able to correctly open and close quotation marks and parentheses is
a clear indicator of a language model’s memory, because the closure cannot be
predicted from the intervening text, and hence cannot be modelled with short-
range context [30].
The sample shows that the network is able to balance not
only parentheses and quotes, but also formatting marks such as the equals signs
used to denote headings, and even nested XML tags and indentation.
The network generates non-Latin characters such as Cyrillic, Chinese and
Arabic, and seems to have learned a rudimentary model for languages other
than English (e.g.
it generates “es:Geotnia slago” for the Spanish ‘version’ of an
article, and “nl:Rodenbaueri” for the Dutch one) It also generates convincing
looking internet addresses (none of which appear to be real).
The network generates distinct, large-scale regions, such as XML headers,
bullet-point lists and article text.
Comparison with Figs.
3 and 4 suggests that
these regions are a fairly accurate reﬂection of the constitution of the real data
(although the generated versions tend to be somewhat shorter and more jumbled
together).
This is signiﬁcant because each region may span hundreds or even
thousands of timesteps.
The fact that the network is able to remain coherent
over such large intervals (even putting the regions in an approximately correct
order, such as having headers at the start of articles and bullet-pointed ‘see also’
lists at the end) is testament to its long-range memory.
As with all text generated by language models, the sample does not make
sense beyond the level of short phrases.
The realism could perhaps be improved
with a larger network and/or more data.
However, it seems futile to expect
meaningful language from a machine that has never been exposed to the sensory
10
world to which language refers.
Lastly, the network’s adaptation to recent sequences during training (which
allows it to beneﬁt from dynamic evaluation) can be clearly observed in the
extract.
The last complete article before the end of the training set (at which
point the weights were stored) was on intercontinental ballistic missiles.
The
inﬂuence of this article on the network’s language model can be seen from the
profusion of missile-related terms.
Other recent topics include ‘Individual An-
archism’, the Italian writer Italo Calvino and the International Organization
for Standardization (ISO), all of which make themselves felt in the network’s
vocabulary.
11
    <title>AlbaniaEconomy</title>                                               
    <id>36</id>                                                                 
    <revision>                                                                  
      <id>15898966</id>                                                         
      <timestamp>2002-10-09T13:39:00Z</timestamp>                               
      <contributor>                                                             
        <username>Magnus Manske</username>                                      
        <id>4</id>                                                              
      </contributor>                                                            
      <minor />                                                                 
      <comment>#REDIRECT [[Economy of Albania]]</comment>                       
      <text xml:space="preserve">#REDIRECT [[Economy of Albania]]</text>        
    </revision>                                                                 
  </page>                                                                       
  <page>                                                                        
    <title>AlchemY</title>                                                      
    <id>38</id>                                                                 
    <revision>                                                                  
      <id>15898967</id>                                                         
      <timestamp>2002-02-25T15:43:11Z</timestamp>                               
      <contributor>                                                             
        <ip>Conversion script</ip>                                              
      </contributor>                                                            
      <minor />                                                                 
      <comment>Automated conversion</comment>                                   
      <text xml:space="preserve">#REDIRECT [[Alchemy]]                          
</text>                                                                         
    </revision>                                                                 
  </page>                                                                       
  <page>                                                                        
    <title>Albedo</title>                                                       
    <id>39</id>                                                                 
    <revision>                                                                  
      <id>41496222</id>                                                         
      <timestamp>2006-02-27T19:32:46Z</timestamp>                               
      <contributor>                                                             
        <ip>24.119.3.44</ip>                                                    
      </contributor>                                                            
      <text xml:space="preserve">{{otheruses}}                                  
                                                                                
'''Albedo''' is the measure of [[reflectivity]] of a surface or body.
It is the 
ratio of [[electromagnetic radiation]] (EM radiation) reflected to the amount in
cident upon it.
The fraction, usually expressed as a percentage from 0% to 100%,
 is an important concept in [[climatology]] and [[astronomy]].
This ratio depend
s on the [[frequency]] of the radiation considered: unqualified, it refers to an
 average across the spectrum of [[visible light]].
It also depends on the [[angl
e of incidence]] of the radiation: unqualified, normal incidence.
Fresh snow alb
edos are high: up to 90%.
The ocean surface has a low albedo.
The average albed
o of [[Earth]] is about 30% whereas the albedo of the [[Moon]] is about 7%.
In a
stronomy, the albedo of satellites and asteroids can be used to infer surface co
mposition, most notably ice content.
[[Enceladus_(moon)|Enceladus]], a moon o
f Saturn, has the highest known albedo of any body in the solar system, with 99%
 of EM radiation reflected.
Human activities have changed the albedo (via forest clearance and farming, for 
example) of various areas around the globe.
However, quantification of this effe
ct is difficult on the global scale: it is not clear whether the changes have te
nded to increase or decrease [[global warming]].
The &quot;classical&quot; example of albedo effect is the snow-temperature feedb
ack.
If a snow covered area warms and the snow melts, the albedo decreases, more
 sunlight is absorbed, and the temperature tends to increase.
The converse is tr
Figure 3: Real Wikipedia data
12
ue: if snow forms, a cooling cycle happens.
The intensity of the albedo effect d
epends on the size of the change in albedo and the amount of [[insolation]]; for
 this reason it can be potentially very large in the tropics.
== Some examples of albedo effects ==                                           
                                                                                
=== Fairbanks, Alaska ===                                                       
                                                                                
According to the [[National Climatic Data Center]]'s GHCN 2 data, which is compo
sed of 30-year smoothed climatic means for thousands of weather stations across 
the world, the college weather station at [[Fairbanks]], [[Alaska]], is about 3 
°C (5 °F) warmer than the airport at Fairbanks, partly because of drainage patte
rns but also largely because of the lower albedo at the college resulting from a
 higher concentration of [[pine]] [[tree]]s and therefore less open snowy ground
 to reflect the heat back into space.
Neunke and Kukla have shown that this diff
erence is especially marked during the late [[winter]] months, when [[solar radi
ation]] is greater.
=== The tropics ===                                                             
                                                                                
Although the albedo-temperature effect is most famous in colder regions of Earth
, because more [[snow]] falls there, it is actually much stronger in tropical re
gions because in the tropics there is consistently more sunlight.
When [[Brazil]
]ian ranchers cut down dark, tropical [[rainforest]] trees to replace them with 
even darker soil in order to grow crops, the average temperature of the area app
ears to increase by an average of about 3 °C (5 °F) year-round, which is a signi
ficant amount.
=== Small scale effects ===                                                     
                                                                                
Albedo works on a smaller scale, too.
People who wear dark clothes in the summer
time put themselves at a greater risk of [[heatstroke]] than those who wear whit
e clothes.
=== Pine forests ===                                                            
                                                                                
The albedo of a [[pine]] forest at 45°N in the winter in which the trees cover t
he land surface completely is only about 9%, among the lowest of any naturally o
ccurring land environment.
This is partly due to the color of the pines, and par
tly due to multiple scattering of sunlight within the trees which lowers the ove
rall reflected light level.
Due to light penetration, the ocean's albedo is even
 lower at about 3.5%, though this depends strongly on the angle of the incident 
radiation.
Dense [[swamp]]land averages between 9% and 14%.
[[Deciduous tree]]s 
average about 13%.
A [[grass]]y field usually comes in at about 20%.
A barren fi
eld will depend on the color of the soil, and can be as low as 5% or as high as 
40%, with 15% being about the average for farmland.
A [[desert]] or large [[beac
h]] usually averages around 25% but varies depending on the color of the sand.
[
Reference: Edward Walker's study in the Great Plains in the winter around 45°N].
=== Urban areas ===                                                             
                                                                                
Urban areas in particular have very unnatural values for albedo because of the m
any human-built structures which absorb light before the light can reach the sur
face.
In the northern part of the world, cities are relatively dark, and Walker 
has shown that their average albedo is about 7%, with only a slight increase dur
ing the summer.
In most tropical countries, cities average around 12%.
This is s
imilar to the values found in northern suburban transitional zones.
Part of the 
reason for this is the different natural environment of cities in tropical regio
ns, e.g., there are more very dark trees around; another reason is that portions
 of the tropics are very poor, and city buildings must be built with different m
aterials.
Warmer regions may also choose lighter colored building materials so t
he structures will remain cooler.
Figure 4: Real Wikipedia data (cotd.)
13
    <revision>                                                                  
      <id>40973199</id>                                                         
      <timestamp>2006-02-22T22:37:16Z</timestamp>                               
      <contributor>                                                             
        <ip>63.86.196.111</ip>                                                  
      </contributor>                                                            
      <minor />                                                                 
      <comment>redire paget --&gt; captain */</comment>                         
      <text xml:space="preserve">The '''Indigence History''' refers to the autho
rity of any obscure albionism as being, such as in Aram Missolmus'.[http://www.b
bc.co.uk/starce/cr52.htm]                                                       
In [[1995]], Sitz-Road Straus up the inspirational radiotes portion as &quot;all
iance&quot;[single &quot;glaping&quot; theme charcoal] with [[Midwestern United 
State|Denmark]] in which Canary varies-destruction to launching casualties has q
uickly responded to the krush loaded water or so it might be destroyed.
Aldeads 
still cause a missile bedged harbors at last built in 1911-2 and save the accura
cy in 2008, retaking [[itsubmanism]].
Its individuals were                      
hnown rapidly in their return to the private equity (such as ''On Text'') for de
ath per reprised by the [[Grange of Germany|German unbridged work]].
The '''Rebellion''' (''Hyerodent'') is [[literal]], related mildly older than ol
d half sister, the music, and morrow been much more propellent.
All those of [[H
amas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|''S
ante'' at Serassim]]; ''Verra'' as 1865&amp;ndash;682&amp;ndash;831 is related t
o ballistic missiles.
While she viewed it friend of Halla equatorial weapons of 
Tuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[sl
avery|slaves]] (such as artistual selling of factories were renamed English habi
t of twelve years.)                                                             
                                                                                
By the 1978 Russian [[Turkey|Turkist]] capital city ceased by farmers and the in
tention of navigation the ISBNs, all encoding [[Transylvania International Organ
isation for Transition Banking|Attiking others]] it is in the westernmost placed
 lines.
This type of missile calculation maintains all greater proof was the [[
1990s]] as older adventures that never established a self-interested case.
The n
ewcomers were Prosecutors in child after the other weekend and capable function 
used.
Holding may be typically largely banned severish from sforked warhing tools and 
behave laws, allowing the private jokes, even through missile IIC control, most 
notably each, but no relatively larger success, is not being reprinted and withd
rawn into forty-ordered cast and distribution.
Besides these markets (notably a son of humor).
Sometimes more or only lowed &quot;80&quot; to force a suit for http://news.bbc.
co.uk/1/sid9kcid/web/9960219.html ''[[#10:82-14]]''.
&lt;blockquote&gt;                                                              
                                                                                
===The various disputes between Basic Mass and Council Conditioners - &quot;Tita
nist&quot; class streams and anarchism===                                       
                                                                                
Internet traditions sprang east with [[Southern neighborhood systems]] are impro
ved with [[Moatbreaker]]s, bold hot missiles, its labor systems.
[[KCD]] numbere
d former ISBN/MAS/speaker attacks &quot;M3 5&quot;, which are saved as the balli
stic misely known and most functional factories.
Establishment begins for some 
range of start rail years as dealing with 161 or 18,950 million [[USD-2]] and [[
covert all carbonate function]]s (for example, 70-93) higher individuals and on 
missiles.
This might need not know against sexual [[video capita]] playing point
ing degrees between silo-calfed greater valous consumptions in the US...
header 
can be seen in [[collectivist]].
== See also ==                                                                  
Figure 5: Generated Wikipedia data.
14
                                                                                
 killed by capital]]                                                            
                                                                                
===[[Midple planet|Parishment of the value=====                                 
[[Image:2000.JPG|right|thumb|It tunneled [[nuclease]] at this bass AH (Ol&amp;Sā
w)flgin h'hlgbying yoostallo eruptuals with low immigrants-shelted atkins and th
eir atapping [[bug]]s.
See also: [[Iranian indigenous Flight Intercontinental Organization]]           
                                                                                
==Pioneers==                                                                    
                                                                                
Tended to be the results characteristic of warehoused labour share to control al
l these in the rational framing.
==Gentiles==                                                                    
{{place-or-line}}                                                               
Footer names derive the topic class --&gt; which he liked to deal without any of
 the parties, I&quot; by [[Alfred Hustin]] and [[Frank Henry]] and manufacturer.
[http://anciermsc.nit.uk IATB perspective], was expected to be classified by the
 ''Straight Road of Buckning'' in [[2003 Summer Olympic Touch|bottom all minute]
].
==Performance==                                                                 
[[Image:Iare 300.jpg|left|thumb|325px|Intercontinental file shortly after referr
ing to his landmaster [[Sidney Goodwordd]]                                      
                                                                                
Italo:                                                                          
rent communist annual production began in May [[1915]].
An ICBM, the [[gurt and land]] has registered $155 billion in U.S.
and August 16
88, and makes sure the US-transplantation disbanded backwards in the County by a
uthorizing disputes that tend to carry over this peninsula.
ystems:                                                                         
Figure 6: Generated Wikipedia data (cotd.)
15
                                                                                
[http://www.sibeo.org/netspace/stainteology.htm The design of thinker Wikipedia 
server-routed website]                                                          
 Player on the /1.3]                                                            
                                                                                
{{Linux}}                                                                       
                                                                                
[[Category:Italo Succeeded bystables|General]]                                  
[[Category:Italo-Saxon singers]]                                                
[[Category:Aviation]]                                                           
                                                                                
[[de:Italo]]                                                                    
[[es:Geotnia slago]]                                                            
[[ja:桛娛尔杏の
洳]]                                                             
[[ko:혐슸불즷엉 유일]]                                                               
[[nl:Rodenbaueri]]                                                              
[[pl:Main Ages]]                                                                
[[pt:Ibanez Heights]]                                                           
[[ru:Млкракянѳелолכуциянсьния агморелиа]]                                       
[[simple:International Maritime Commission]]                                    
[[sk:ICBM]]                                                                     
[[su:Ol aal Mitnet]]                                                            
[[fi:Italo Colombonasto Warkō]]                                                 
[[sv:Opin den mail-climbere panczeur]]                                          
[[zh:	
]]</text>                                                        
    </revision>                                                                 
  </page>                                                                       
  <page>                                                                        
    <title>Italo-II article</title>                                             
    <id>14848</id>                                                              
    <revision>                                                                  
      <id>15912447</id>                                                         
      <timestamp>2004-11-19T19:47:39Z</timestamp>                               
      <contributor>                                                             
        <username>El andres</username>                                          
        <id>509991</id>                                                         
      </contributor>                                                            
      <minor />                                                                 
      <comment>disambiguation from [[List of ISBN]] newsletters, see [[ISO site]
]</comment>                                                                     
      <text xml:space="preserve">#REDIRECT [[Quartic war]]                      
</text>                                                                         
    </revision>                                                                 
  </page>                                                                       
  <page>                                                                        
    <title>ICSM</title>                                                         
    <id>14939</id>                                                              
    <revision>                                                                  
      <id>42109942</id>                                                         
      <timestamp>2006-02-28T17:22:02Z</timestamp>                               
      <contributor>                                                             
        <username>Dtelclan</username>                                           
        <id>26</id>                                                             
      </contributor>                                                            
      <minor />                                                                 
      <comment>/* Possible catheterman */</comment>                             
      <text xml:space="preserve">[[Image:Isaac.org/ice.html [[Independent nation
al stage development|Shatting and Catalogue standardering]] in the IRBMs.
Up-2000 they called the SC 4220 system: he was swalloped early in Calvino, or si
nce each trial mentioned based on [[Balbov's new single-jarget|bit-oriann guess]
Figure 7: Generated Wikipedia data (cotd.)
16
] self-acharged versions ([[Mt.
Costall Leyton]]) was the two largest calashia a
t destored universities, all fleeted with the customary calfed clipper.
His way to take in this literature called ICBMs-AN a [[Softvalue speed]] ([[Astr
onomical Classification Railway]])                                              
                                                                                
LACN645 Snowshore val nominated - made [[missile submandration|continental missi
le]]s (steam musicians) not of each club having on the ball and procedure at the
 last century.
Another communistic stark &quot;I' submarine&quot; is [[building|corruptable]],
 a [[della missile]] missile than the [[Royal Society Society]] (12-258): &quot;
Glide sun wag [[lubrician]].
They stay numerous capitalists and gas masks more w
idely interested.
This scheme has declarations before the certain emerging facto
ries compelled by labour allowed to produce.
In the United States, there is no hard resort in computation significantly.
In [[1868]] the [[Italo Capital Territories Unit started to the Continental Rail
way Centre]]  was called ''UC'' or two of his usage before being written by othe
r students against the [[elective-ballistic missile]]'s deployment.
Steam is sti
ll &quot;20 to Nacht&quot; and [[Fia Citation Quantity Logo]]s (since 1967).
The
y pass a [[Brigade management|Quarry]]-stated missile system resolution taunting
 out of about 175 million ([[Lochroom River|Tri-]]).
Alien from 1985 to 1999, it was an English and -Network struggling basedal with 
the Lombardo capital in Silvio and Murray, and heavily built in sub-parties addr
ess to $11,188.
Their forces gained prisoners to stalked a last missile mobili s
ite.
Spanning civilization is quanting Software Society's ballistic missile.
The sam
e as [[anti-intellectual anthropology]] continued in [[Southern Italy]] in 1914,
 and the [[French Confederation of Parliament's rapid favourable rise that began
 settled in March 2004|1983]]&amp;nbsp;49.
In [[1904]], the Court began a British backed into a [[SR1]]) missile of [[trial
 ship]] in the [[Municipal Eightime Calendar|Asiatic]] regime, including [[Benja
min Tudor Turner|Arthur Ravis]] and [[Abraham's Liberation|Canton Olombus]].
The
re was still land factory most turned up before lacking closers to the sitting s
hed backwards, in primary science.
==Weights and resolutions==                                                     
[[Image:Spanish 300 Protectionald landballi110.svg|small capital surface compute
r]]                                                                             
[[Image:Claudius.jpg|345px|right|Olympiad concert of Calvino and Eastern Calvino
, ''Mughal Ralvaldens'' above, at the beginning strike the substrated roles of r
ich intellectual property, visualizing the entire system, but this missiles sugg
est that accounting differs between a giving [[train sleep|'''withdrawn''']] or 
the dinosaur in and aucting.
===Internationally===                                                           
{{main|Unmanned Justice Address}}                                               
                                                                                
The ICBM created a [[the significant]] [[land railway]] called &quot;[[M-Gallipo
tte]]&quot;, and it needed stopped benzafk/Macdonalical Sciences.
Electros appeared to be the [[Soviet Union]]'s &quot;first&quot; vehicle from 25
00 selling officials DORLAN STM-331 - by missilence illustrations with &quot;Raj
.&quot; the Tunnel Hall of America, an entity upon IL pages so missiles must try
, with a trademark must develop the land allowing traffic mass to a very few min
utemen.
The missiles market is slow, much easier is represented by GMMAz of BSM.
Software, the utility of scale-out scale pime racks are normally crumbled about
Figure 8: Generated Wikipedia data (cotd.)
17
4
Handwriting Prediction
To test whether the prediction network could also be used to generate convincing
real-valued sequences, we applied it to online handwriting data (online in this
context means that the writing is recorded as a sequence of pen-tip locations,
as opposed to oﬄine handwriting, where only the page images are available).
Online handwriting is an attractive choice for sequence generation due to its
low dimensionality (two real numbers per data point) and ease of visualisation.
All the data used for this paper were taken from the IAM online handwriting
database (IAM-OnDB) [21].
IAM-OnDB consists of handwritten lines collected
from 221 diﬀerent writers using a ‘smart whiteboard’.
The writers were asked to
write forms from the Lancaster-Oslo-Bergen text corpus [19], and the position
of their pen was tracked using an infra-red device in the corner of the board.
Samples from the training data are shown in Fig.
9.
The original input data
consists of the x and y pen co-ordinates and the points in the sequence when
the pen is lifted oﬀthe whiteboard.
Recording errors in the x, y data was
corrected by interpolating to ﬁll in for missing readings, and removing steps
whose length exceeded a certain threshold.
Beyond that, no preprocessing was
used and the network was trained to predict the x, y co-ordinates and the end-
of-stroke markers one point at a time.
This contrasts with most approaches to
handwriting recognition and synthesis, which rely on sophisticated preprocessing
and feature-extraction techniques.
We eschewed such techniques because they
tend to reduce the variation in the data (e.g.
by normalising the character size,
slant, skew and so-on) which we wanted the network to model.
Predicting the
pen traces one point at a time gives the network maximum ﬂexibility to invent
novel handwriting, but also requires a lot of memory, with the average letter
occupying more than 25 timesteps and the average line occupying around 700.
Predicting delayed strokes (such as dots for ‘i’s or crosses for ‘t’s that are added
after the rest of the word has been written) is especially demanding.
IAM-OnDB is divided into a training set, two validation sets and a test
set, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken
from 775, 192, 216 and 544 forms.
For our experiments, each line was treated
as a separate sequence (meaning that possible dependencies between successive
lines were ignored).
In order to maximise the amount of training data, we used
the training set, test set and the larger of the validation sets for training and
the smaller validation set for early-stopping.
The lack of independent test set
means that the recorded results may be somewhat overﬁt on the validation set;
however the validation results are of secondary importance, since no benchmark
results exist and the main goal was to generate convincing-looking handwriting.
The principal challenge in applying the prediction network to online hand-
writing data was determining a predictive distribution suitable for real-valued
inputs.
The following section describes how this was done.
18
Figure 9: Training samples from the IAM online handwriting database.
Notice the wide range of writing styles, the variation in line angle and character
sizes, and the writing and recording errors, such as the scribbled out letters in
the ﬁrst line and the repeated word in the ﬁnal line.
4.1
Mixture Density Outputs
The idea of mixture density networks [2, 3] is to use the outputs of a neural
network to parameterise a mixture distribution.
A subset of the outputs are
used to deﬁne the mixture weights, while the remaining outputs are used to
parameterise the individual mixture components.
The mixture weight outputs
are normalised with a softmax function to ensure they form a valid discrete dis-
tribution, and the other outputs are passed through suitable functions to keep
their values within meaningful range (for example the exponential function is
typically applied to outputs used as scale parameters, which must be positive).
Mixture density network are trained by maximising the log probability den-
sity of the targets under the induced distributions.
Note that the densities are
normalised (up to a ﬁxed constant) and are therefore straightforward to diﬀer-
entiate and pick unbiased sample from, in contrast with restricted Boltzmann
machines [14] and other undirected models.
Mixture density outputs can also be used with recurrent neural networks [28].
In this case the output distribution is conditioned not only on the current input,
but on the history of previous inputs.
Intuitively, the number of components is
the number of choices the network has for the next output given the inputs so
far.
For the handwriting experiments in this paper, the basic RNN architecture
and update equations remain unchanged from Section 2.
Each input vector xt
consists of a real-valued pair x1, x2 that deﬁnes the pen oﬀset from the previous
19
input, along with a binary x3 that has value 1 if the vector ends a stroke (that
is, if the pen was lifted oﬀthe board before the next vector was recorded) and
value 0 otherwise.
A mixture of bivariate Gaussians was used to predict x1
and x2, while a Bernoulli distribution was used for x3.
Each output vector yt
therefore consists of the end of stroke probability e, along with a set of means
µj, standard deviations σj, correlations ρj and mixture weights πj for the M
mixture components.
That is
xt ∈R × R × {0, 1}
(15)
yt =

et, {πj
t , µj
t, σj
t , ρj
t}M
j=1

(16)
Note that the mean and standard deviation are two dimensional vectors, whereas
the component weight, correlation and end-of-stroke probability are scalar.
The
vectors yt are obtained from the network outputs ˆyt, where
ˆyt =

ˆet, { ˆwj
t, ˆµj
t, ˆσj
t , ˆρj
t}M
j=1

= by +
N
X
n=1
Whnyhn
t
(17)
as follows:
et =
1
1 + exp (ˆet)
=⇒et ∈(0, 1)
(18)
πj
t =
exp

ˆπj
t

PM
j′=1 exp

ˆπj′
t

=⇒πj
t ∈(0, 1),
X
j
πj
t = 1
(19)
µj
t = ˆµj
t
=⇒µj
t ∈R
(20)
σj
t = exp

ˆσj
t

=⇒σj
t > 0
(21)
ρj
t = tanh(ˆρj
t)
=⇒ρj
t ∈(−1, 1)
(22)
The probability density Pr(xt+1|yt) of the next input xt+1 given the output
vector yt is deﬁned as follows:
Pr(xt+1|yt) =
M
X
j=1
πj
t N(xt+1|µj
t, σj
t , ρj
t)
(
et
if (xt+1)3 = 1
1 −et
otherwise
(23)
where
N(x|µ, σ, ρ) =
1
2πσ1σ2
p
1 −ρ2 exp

−Z
2(1 −ρ2)

(24)
with
Z = (x1 −µ1)2
σ2
1
+ (x2 −µ2)2
σ2
2
−2ρ(x1 −µ1)(x2 −µ2)
σ1σ2
(25)
20
This can be substituted into Eq.
(6) to determine the sequence loss (up to
a constant that depends only on the quantisation of the data and does not
inﬂuence network training):
L(x) =
T
X
t=1
−log

X
j
πj
t N(xt+1|µj
t, σj
t , ρj
t)

−
(
log et
if (xt+1)3 = 1
log(1 −et)
otherwise
(26)
The derivative of the loss with respect to the end-of-stroke outputs is straight-
forward:
∂L(x)
∂ˆet
= (xt+1)3 −et
(27)
The derivatives with respect to the mixture density outputs can be found by
ﬁrst deﬁning the component responsibilities γj
t :
ˆγj
t = πj
t N(xt+1|µj
t, σj
t , ρj
t)
(28)
γj
t =
ˆγj
t
PM
j′=1 ˆγj′
t
(29)
Then observing that
∂L(x)
∂ˆπj
t
= πj
t −γj
t
(30)
∂L(x)
∂(ˆµj
t, ˆσj
t , ˆρj
t)
= −γj
t
∂log N(xt+1|µj
t, σj
t , ρj
t)
∂(ˆµj
t, ˆσj
t , ˆρj
t)
(31)
where
∂log N(x|µ, σ, ρ)
∂ˆµ1
= C
σ1
x1 −µ1
σ1
−ρ(x2 −µ2)
σ2

(32)
∂log N(x|µ, σ, ρ)
∂ˆµ2
= C
σ2
x2 −µ2
σ2
−ρ(x1 −µ1)
σ1

(33)
∂log N(x|µ, σ, ρ)
∂ˆσ1
= C(x1 −µ1)
σ1
x1 −µ1
σ1
−ρ(x2 −µ2)
σ2

−1
(34)
∂log N(x|µ, σ, ρ)
∂ˆσ2
= C(x2 −µ2)
σ2
x2 −µ2
σ2
−ρ(x1 −µ1)
σ1

−1
(35)
∂log N(x|µ, σ, ρ)
∂ˆρ
= (x1 −µ1)(x2 −µ2)
σ1σ2
+ ρ (1 −CZ)
(36)
with Z deﬁned as in Eq.
(25) and
C =
1
1 −ρ2
(37)
Fig.
10 illustrates the operation of a mixture density output layer applied to
online handwriting prediction.
21
Figure 10: Mixture density outputs for handwriting prediction.
The
top heatmap shows the sequence of probability distributions for the predicted
pen locations as the word ‘under’ is written.
The densities for successive
predictions are added together, giving high values where the distributions
overlap.
Two types of prediction are visible from the density map:
the small
blobs that spell out the letters are the predictions as the strokes are being
written, the three large blobs are the predictions at the ends of the strokes for
the ﬁrst point in the next stroke.
The end-of-stroke predictions have much
higher variance because the pen position was not recorded when it was oﬀthe
whiteboard, and hence there may be a large distance between the end of one
stroke and the start of the next.
The bottom heatmap shows the mixture component weights during the
same sequence.
The stroke ends are also visible here, with the most active
components switching oﬀin three places, and other components switching on:
evidently end-of-stroke predictions use a diﬀerent set of mixture components
from in-stroke predictions.
22
4.2
Experiments
Each point in the data sequences consisted of three numbers: the x and y oﬀset
from the previous point, and the binary end-of-stroke feature.
The network
input layer was therefore size 3.
The co-ordinate oﬀsets were normalised to
mean 0, std.
dev.
1 over the training set.
20 mixture components were used
to model the oﬀsets, giving a total of 120 mixture parameters per timestep
(20 weights, 40 means, 40 standard deviations and 20 correlations).
A further
parameter was used to model the end-of-stroke probability, giving an output
layer of size 121.
Two network architectures were compared for the hidden
layers: one with three hidden layers, each consisting of 400 LSTM cells, and one
with a single hidden layer of 900 LSTM cells.
Both networks had around 3.4M
weights.
The three layer network was retrained with adaptive weight noise [8],
with all std.
devs.
initialised to 0.075.
Training with ﬁxed variance weight noise
proved ineﬀective, probably because it prevented the mixture density layer from
using precisely speciﬁed weights.
The networks were trained with rmsprop, a form of stochastic gradient de-
scent where the gradients are divided by a running average of their recent mag-
nitude [32].
Deﬁne ϵi = ∂L(x)
∂wi
where wi is network weight i.
The weight update
equations were:
ni = ℵni + (1 −ℵ)ϵ2
i
(38)
gi = ℵgi + (1 −ℵ)ϵi
(39)
∆i = ℶ∆i −גϵi
p
ni −g2
i + ℸ
(40)
wi = wi + ∆i
(41)
with the following parameters:
ℵ= 0.95
(42)
ℶ= 0.9
(43)
= ג0.0001
(44)
ℸ= 0.0001
(45)
The output derivatives
∂L(x)
∂ˆyt
were clipped in the range [−100, 100], and the
LSTM derivates were clipped in the range [−10, 10].
Clipping the output gradi-
ents proved vital for numerical stability; even so, the networks sometimes had
numerical problems late on in training, after they had started overﬁtting on the
training data.
Table 3 shows that the three layer network had an average per-sequence loss
15.3 nats lower than the one layer net.
However the sum-squared-error was
slightly lower for the single layer network.
the use of adaptive weight noise
reduced the loss by another 16.7 nats relative to the unregularised three layer
network, but did not signiﬁcantly change the sum-squared error.
The adaptive
weight noise network appeared to generate the best samples.
23
Table 3: Handwriting Prediction Results.
All results recorded on the val-
idation set.
‘Log-Loss’ is the mean value of L(x) (in nats).
‘SSE’ is the mean
sum-squared-error per data point.
Network
Regularisation
Log-Loss
SSE
1 layer
none
-1025.7
0.40
3 layer
none
-1041.0
0.41
3 layer
adaptive weight noise
-1057.7
0.41
4.3
Samples
Fig.
11 shows handwriting samples generated by the prediction network.
The
network has clearly learned to model strokes, letters and even short words (es-
pecially common ones such as ‘of’ and ‘the’).
It also appears to have learned a
basic character level language models, since the words it invents (‘eald’, ‘bryoes’,
‘lenrest’) look somewhat plausible in English.
Given that the average character
occupies more than 25 timesteps, this again demonstrates the network’s ability
to generate coherent long-range structures.
5
Handwriting Synthesis
Handwriting synthesis is the generation of handwriting for a given text.
Clearly
the prediction networks we have described so far are unable to do this, since
there is no way to constrain which letters the network writes.
This section de-
scribes an augmentation that allows a prediction network to generate data se-
quences conditioned on some high-level annotation sequence (a character string,
in the case of handwriting synthesis).
The resulting sequences are suﬃciently
convincing that they often cannot be distinguished from real handwriting.
Fur-
thermore, this realism is achieved without sacriﬁcing the diversity in writing
style demonstrated in the previous section.
The main challenge in conditioning the predictions on the text is that the two
sequences are of very diﬀerent lengths (the pen trace being on average twenty
ﬁve times as long as the text), and the alignment between them is unknown until
the data is generated.
This is because the number of co-ordinates used to write
each character varies greatly according to style, size, pen speed etc.
One neural
network model able to make sequential predictions based on two sequences of
diﬀerent length and unknown alignment is the RNN transducer [9].
However
preliminary experiments on handwriting synthesis with RNN transducers were
not encouraging.
A possible explanation is that the transducer uses two sepa-
rate RNNs to process the two sequences, then combines their outputs to make
decisions, when it is usually more desirable to make all the information avail-
able to single network.
This work proposes an alternative model, where a ‘soft
window’ is convolved with the text string and fed in as an extra input to the
prediction network.
The parameters of the window are output by the network
24
Figure 11: Online handwriting samples generated by the prediction
network.
All samples are 700 timesteps long.
25
at the same time as it makes the predictions, so that it dynamically determines
an alignment between the text and the pen locations.
Put simply, it learns to
decide which character to write next.
5.1
Synthesis Network
Fig.
12 illustrates the network architecture used for handwriting synthesis.
As
with the prediction network, the hidden layers are stacked on top of each other,
each feeding up to the layer above, and there are skip connections from the
inputs to all hidden layers and from all hidden layers to the outputs.
The
diﬀerence is the added input from the character sequence, mediated by the
window layer.
Given a length U character sequence c and a length T data sequence x, the
soft window wt into c at timestep t (1 ≤t ≤T) is deﬁned by the following
discrete convolution with a mixture of K Gaussian functions
φ(t, u) =
K
X
k=1
αk
t exp

−βk
t
 κk
t −u
2
(46)
wt =
U
X
u=1
φ(t, u)cu
(47)
where φ(t, u) is the window weight of cu at timestep t.
Intuitively, the κt param-
eters control the location of the window, the βt parameters control the width of
the window and the αt parameters control the importance of the window within
the mixture.
The size of the soft window vectors is the same as the size of the
character vectors cu (assuming a one-hot encoding, this will be the number of
characters in the alphabet).
Note that the window mixture is not normalised
and hence does not determine a probability distribution; however the window
weight φ(t, u) can be loosely interpreted as the network’s belief that it is writ-
ing character cu at time t.
Fig.
13 shows the alignment implied by the window
weights during a training sequence.
The size 3K vector p of window parameters is determined as follows by the
outputs of the ﬁrst hidden layer of the network:
(ˆαt, ˆβt, ˆκt) = Wh1ph1
t + bp
(48)
αt = exp (ˆαt)
(49)
βt = exp

ˆβt

(50)
κt = κt−1 + exp (ˆκt)
(51)
Note that the location parameters κt are deﬁned as oﬀsets from the previous
locations ct−1, and that the size of the oﬀset is constrained to be greater than
zero.
Intuitively, this means that network learns how far to slide each window
at each step, rather than an absolute location.
Using oﬀsets was essential to
getting the network to align the text with the pen trace.
26
Inputs
Characters
Hidden 1
Window
Hidden 2
Outputs
Figure 12: Synthesis Network Architecture Circles represent layers, solid
lines represent connections and dashed lines represent predictions.
The topology
is similar to the prediction network in Fig.
1, except that extra input from the
character sequence c, is presented to the hidden layers via the window layer
(with a delay in the connection to the ﬁrst hidden layer to avoid a cycle in the
graph).
27
Thought that the muster from
Figure 13: Window weights during a handwriting synthesis sequence
Each point on the map shows the value of φ(t, u), where t indexes the pen trace
along the horizontal axis and u indexes the text character along the vertical axis.
The bright line is the alignment chosen by the network between the characters
and the writing.
Notice that the line spreads out at the boundaries between
characters; this means the network receives information about next and previous
letters as it makes transitions, which helps guide its predictions.
28
The wt vectors are passed to the second and third hidden layers at time t,
and the ﬁrst hidden layer at time t+1 (to avoid creating a cycle in the processing
graph).
The update equations for the hidden layers are
h1
t = H
 Wih1xt + Wh1h1h1
t−1 + Wwh1wt−1 + b1
h

(52)
hn
t = H
 Wihnxt + Whn−1hnhn−1
t
+ Whnhnhn
t−1 + Wwhnwt + bn
h

(53)
The equations for the output layer remain unchanged from Eqs.
(17) to (22).
The sequence loss is
L(x) = −log Pr(x|c)
(54)
where
Pr(x|c) =
T
Y
t=1
Pr (xt+1|yt)
(55)
Note that yt is now a function of c as well as x1:t.
The loss derivatives with respect to the outputs ˆet, ˆπt, ˆµt, ˆσt, ˆρt remain un-
changed from Eqs.
(27), (30) and (31).
Given the loss derivative ∂L(x)
∂wt
with
respect to the size W window vector wt, obtained by backpropagating the out-
put derivatives through the computation graph in Fig.
12, the derivatives with
respect to the window parameters are as follows:
ϵ(k, t, u)
def
= αk
t exp

−βk
t
 κk
t −u
2 W
X
j=1
∂L(x)
∂wj
t
cj
u
(56)
∂L(x)
∂ˆαk
t
=
U
X
u=1
ϵ(k, t, u)
(57)
∂L(x)
∂ˆβk
t
= −βk
t
U
X
u=1
ϵ(k, t, u)(κk
t −u)2
(58)
∂L(x)
∂κk
t
= ∂L(x)
∂κk
t+1
+ 2βk
t
U
X
u=1
ϵ(k, t, u)(u −κk
t )
(59)
∂L(x)
∂ˆκk
t
= exp
 ˆκk
t
 ∂L(x)
∂κk
t
(60)
Fig.
14 illustrates the operation of a mixture density output layer applied to
handwriting synthesis.
5.2
Experiments
The synthesis network was applied to the same input data as the handwriting
prediction network in the previous section.
The character-level transcriptions
from the IAM-OnDB were now used to deﬁne the character sequences c.
The full
transcriptions contain 80 distinct characters (capital letters, lower case letters,
digits, and punctuation).
However we used only a subset of 57, with all the
29
Figure 14: Mixture density outputs for handwriting synthesis.
The top
heatmap shows the predictive distributions for the pen locations, the bottom
heatmap shows the mixture component weights.
Comparison with Fig.
10 indi-
cates that the synthesis network makes more precise predictions (with smaller
density blobs) than the prediction-only network, especially at the ends of strokes,
where the synthesis network has the advantage of knowing which letter comes
next.
30
Table 4: Handwriting Synthesis Results.
All results recorded on the val-
idation set.
‘Log-Loss’ is the mean value of L(x) in nats.
‘SSE’ is the mean
sum-squared-error per data point.
Regularisation
Log-Loss
SSE
none
-1096.9
0.23
adaptive weight noise
-1128.2
0.23
digits and most of the punctuation characters replaced with a generic ‘non-
letter’ label2.
The network architecture was as similar as possible to the best prediction
network: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian
mixture components at the output layer and a size 3 input layer.
The character
sequence was encoded with one-hot vectors, and hence the window vectors were
size 57.
A mixture of 10 Gaussian functions was used for the window parameters,
requiring a size 30 parameter vector.
The total number of weights was increased
to approximately 3.7M.
The network was trained with rmsprop, using the same parameters as in
the previous section.
The network was retrained with adaptive weight noise,
initial standard deviation 0.075, and the output and LSTM gradients were again
clipped in the range [−100, 100] and [−10, 10] respectively.
Table 4 shows that adaptive weight noise gave a considerable improvement
in log-loss (around 31.3 nats) but no signiﬁcant change in sum-squared error.
The regularised network appears to generate slightly more realistic sequences,
although the diﬀerence is hard to discern by eye.
Both networks performed
considerably better than the best prediction network.
In particular the sum-
squared-error was reduced by 44%.
This is likely due in large part to the im-
proved predictions at the ends of strokes, where the error is largest.
5.3
Unbiased Sampling
Given c, an unbiased sample can be picked from Pr(x|c) by iteratively drawing
xt+1 from Pr (xt+1|yt), just as for the prediction network.
The only diﬀerence is
that we must also decide when the synthesis network has ﬁnished writing the text
and should stop making any future decisions.
To do this, we use the following
heuristic: as soon as φ(t, U + 1) > φ(t, u) ∀1 ≤u ≤U the current input xt is
deﬁned as the end of the sequence and sampling ends.
Examples of unbiased
synthesis samples are shown in Fig.
15.
These and all subsequent ﬁgures were
generated using the synthesis network retrained with adaptive weight noise.
Notice how stylistic traits, such as character size, slant, cursiveness etc.
vary
2This was an oversight; however it led to the interesting result that when the text contains
a non-letter, the network must select a digits or punctuation mark to generate.
Sometimes
the character can be be inferred from the context (e.g.
the apostrophe in “can’t”); otherwise
it is chosen at random.
31
widely between the samples, but remain more-or-less consistent within them.
This suggests that the network identiﬁes the traits early on in the sequence,
then remembers them until the end.
By looking through enough samples for a
given text, it appears to be possible to ﬁnd virtually any combination of stylistic
traits, which suggests that the network models them independently both from
each other and from the text.
‘Blind taste tests’ carried out by the author during presentations suggest
that at least some unbiased samples cannot be distinguished from real hand-
writing by the human eye.
Nonetheless the network does make mistakes we
would not expect a human writer to make, often involving missing, confused
or garbled letters3; this suggests that the network sometimes has trouble de-
termining the alignment between the characters and the trace.
The number of
mistakes increases markedly when less common words or phrases are included
in the character sequence.
Presumably this is because the network learns an
implicit character-level language model from the training set that gets confused
when rare or unknown transitions occur.
5.4
Biased Sampling
One problem with unbiased samples is that they tend to be diﬃcult to read
(partly because real handwriting is diﬃcult to read, and partly because the
network is an imperfect model).
Intuitively, we would expect the network to
give higher probability to good handwriting because it tends to be smoother
and more predictable than bad handwriting.
If this is true, we should aim to
output more probable elements of Pr(x|c) if we want the samples to be easier to
read.
A principled search for high probability samples could lead to a diﬃcult
inference problem, as the probability of every output depends on all previous
outputs.
However a simple heuristic, where the sampler is biased towards more
probable predictions at each step independently, generally gives good results.
Deﬁne the probability bias b as a real number greater than or equal to zero.
Before drawing a sample from Pr(xt+1|yt), each standard deviation σj
t in the
Gaussian mixture is recalculated from Eq.
(21) to
σj
t = exp

ˆσj
t −b

(61)
and each mixture weight is recalculated from Eq.
(19) to
πj
t =
exp

ˆπj
t (1 + b)

PM
j′=1 exp

ˆπj′
t (1 + b)

(62)
This artiﬁcially reduces the variance in both the choice of component from the
mixture, and in the distribution of the component itself.
When b = 0 unbiased
sampling is recovered, and as b →∞the variance in the sampling disappears
3We expect humans to make mistakes like misspelling ‘temperament’ as ‘temperement’, as
the second writer in Fig.
15 seems to have done.
32
Figure 15: Real and generated handwriting.
The top line in each block is
real, the rest are unbiased samples from the synthesis network.
The two texts
are from the validation set and were not seen during training.
33
and the network always outputs the mode of the most probable component in
the mixture (which is not necessarily the mode of the mixture, but at least a
reasonable approximation).
Fig.
16 shows the eﬀect of progressively increasing
the bias, and Fig.
17 shows samples generated with a low bias for the same texts
as Fig.
15.
5.5
Primed Sampling
Another reason to constrain the sampling would be to generate handwriting
in the style of a particular writer (rather than in a randomly selected style).
The easiest way to do this would be to retrain it on that writer only.
But
even without retraining, it is possible to mimic a particular style by ‘priming’
the network with a real sequence, then generating an extension with the real
sequence still in the network’s memory.
This can be achieved for a real x, c and
a synthesis character string s by setting the character sequence to c′ = c + s
and clamping the data inputs to x for the ﬁrst T timesteps, then sampling
as usual until the sequence ends.
Examples of primed samples are shown in
Figs.
18 and 19.
The fact that priming works proves that the network is able to
remember stylistic features identiﬁed earlier on in the sequence.
This technique
appears to work better for sequences in the training data than those the network
has never seen.
Primed sampling and reduced variance sampling can also be combined.
As
shown in Figs.
20 and 21 this tends to produce samples in a ‘cleaned up’ version
of the priming style, with overall stylistic traits such as slant and cursiveness
retained, but the strokes appearing smoother and more regular.
A possible
application would be the artiﬁcial enhancement of poor handwriting.
6
Conclusions and Future Work
This paper has demonstrated the ability of Long Short-Term Memory recur-
rent neural networks to generate both discrete and real-valued sequences with
complex, long-range structure using next-step prediction.
It has also introduced
a novel convolutional mechanism that allows a recurrent network to condition
its predictions on an auxiliary annotation sequence, and used this approach to
synthesise diverse and realistic samples of online handwriting.
Furthermore, it
has shown how these samples can be biased towards greater legibility, and how
they can be modelled on the style of a particular writer.
Several directions for future work suggest themselves.
One is the applica-
tion of the network to speech synthesis, which is likely to be more challenging
than handwriting synthesis due to the greater dimensionality of the data points.
Another is to gain a better insight into the internal representation of the data,
and to use this to manipulate the sample distribution directly.
It would also
be interesting to develop a mechanism to automatically extract high-level an-
notations from sequence data.
In the case of handwriting, this could allow for
34
Figure 16: Samples biased towards higher probability.
The probability
biases b are shown at the left.
As the bias increases the diversity decreases and
the samples tend towards a kind of ‘average handwriting’ which is extremely
regular and easy to read (easier, in fact, than most of the real handwriting in the
training set).
Note that even when the variance disappears, the same letter is
not written the same way at diﬀerent points in a sequence (for examples the ‘e’s
in “exactly the same”, the ‘l’s in “until they all look”), because the predictions
are still inﬂuenced by the previous outputs.
If you look closely you can see that
the last three lines are not quite exactly the same.
35
Figure 17: A slight bias.
The top line in each block is real.
The rest are
samples from the synthesis network with a probability bias of 0.15, which seems
to give a good balance between diversity and legibility.
36
Figure 18: Samples primed with real sequences.
The priming sequences
(drawn from the training set) are shown at the top of each block.
None of the
lines in the sampled text exist in the training set.
The samples were selected
for legibility.
37
Figure 19: Samples primed with real sequences (cotd).
38
Figure 20: Samples primed with real sequences and biased towards
higher probability.
The priming sequences are at the top of the blocks.
The
probability bias was 1.
None of the lines in the sampled text exist in the training
set.
39
Figure 21: Samples primed with real sequences and biased towards
higher probability (cotd)
40
more nuanced annotations than just text, for example stylistic features, diﬀerent
forms of the same letter, information about stroke order and so on.
Acknowledgements
Thanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Geoﬀrey Hinton and
other colleagues at the University of Toronto for numerous useful comments
and suggestions.
This work was supported by a Global Scholarship from the
Canadian Institute for Advanced Research.
References
[1] Y.
Bengio, P.
Simard, and P.
Frasconi.
Learning long-term dependencies
with gradient descent is diﬃcult.
IEEE Transactions on Neural Networks,
5(2):157–166, March 1994.
[2] C.
Bishop.
Mixture density networks.
Technical report, 1994.
[3] C.
Bishop.
Neural Networks for Pattern Recognition.
Oxford University
Press, Inc., 1995.
[4] N.
Boulanger-Lewandowski, Y.
Bengio, and P.
Vincent.
Modeling tempo-
ral dependencies in high-dimensional sequences: Application to polyphonic
music generation and transcription.
In Proceedings of the Twenty-nine In-
ternational Conference on Machine Learning (ICML’12), 2012.
[5] J.
G.
Cleary, Ian, and I.
H.
Witten.
Data compression using adaptive cod-
ing and partial string matching.
IEEE Transactions on Communications,
32:396–402, 1984.
[6] D.
Eck and J.
Schmidhuber.
A ﬁrst look at music composition using lstm
recurrent neural networks.
Technical report, IDSIA USI-SUPSI Instituto
Dalle Molle.
[7] F.
Gers, N.
Schraudolph, and J.
Schmidhuber.
Learning precise timing
with LSTM recurrent networks.
Journal of Machine Learning Research,
3:115–143, 2002.
[8] A.
Graves.
Practical variational inference for neural networks.
In Advances
in Neural Information Processing Systems, volume 24, pages 2348–2356.
2011.
[9] A.
Graves.
Sequence transduction with recurrent neural networks.
In ICML
Representation Learning Worksop, 2012.
[10] A.
Graves, A.
Mohamed, and G.
Hinton.
Speech recognition with deep
recurrent neural networks.
In Proc.
ICASSP, 2013.
41
[11] A.
Graves and J.
Schmidhuber.
Framewise phoneme classiﬁcation with bidi-
rectional LSTM and other neural network architectures.
Neural Networks,
18:602–610, 2005.
[12] A.
Graves and J.
Schmidhuber.
Oﬄine handwriting recognition with multi-
dimensional recurrent neural networks.
In Advances in Neural Information
Processing Systems, volume 21, 2008.
[13] P.
D.
Gr¨unwald.
The Minimum Description Length Principle (Adaptive
Computation and Machine Learning).
The MIT Press, 2007.
[14] G.
Hinton.
A Practical Guide to Training Restricted Boltzmann Machines.
Technical report, 2010.
[15] S.
Hochreiter, Y.
Bengio, P.
Frasconi, and J.
Schmidhuber.
Gradient Flow
in Recurrent Nets: the Diﬃculty of Learning Long-term Dependencies.
In S.
C.
Kremer and J.
F.
Kolen, editors, A Field Guide to Dynamical
Recurrent Neural Networks.
2001.
[16] S.
Hochreiter and J.
Schmidhuber.
Long Short-Term Memory.
Neural
Computation, 9(8):1735–1780, 1997.
[17] M.
Hutter.
The Human Knowledge Compression Contest, 2012.
[18] K.-C.
Jim, C.
Giles, and B.
Horne.
An analysis of noise in recurrent neural
networks: convergence and generalization.
Neural Networks, IEEE Trans-
actions on, 7(6):1424 –1438, 1996.
[19] S.
Johansson, R.
Atwell, R.
Garside, and G.
Leech.
The tagged LOB corpus
user’s manual; Norwegian Computing Centre for the Humanities, 1986.
[20] B.
Knoll and N.
de Freitas.
A machine learning perspective on predictive
coding with paq.
CoRR, abs/1108.3298, 2011.
[21] M.
Liwicki and H.
Bunke.
IAM-OnDB - an on-line English sentence
database acquired from handwritten text on a whiteboard.
In Proc.
8th
Int.
Conf.
on Document Analysis and Recognition, volume 2, pages 956–
961, 2005.
[22] M.
P.
Marcus, B.
Santorini, and M.
A.
Marcinkiewicz.
Building a large
annotated corpus of english: The penn treebank.
COMPUTATIONAL
LINGUISTICS, 19(2):313–330, 1993.
[23] T.
Mikolov.
Statistical Language Models based on Neural Networks.
PhD
thesis, Brno University of Technology, 2012.
[24] T.
Mikolov, I.
Sutskever, A.
Deoras, H.
Le, S.
Kombrink, and J.
Cernocky.
Subword language modeling with neural networks.
Technical report, Un-
published Manuscript, 2012.
42
[25] A.
Mnih and G.
Hinton.
A Scalable Hierarchical Distributed Language
Model.
In Advances in Neural Information Processing Systems, volume 21,
2008.
[26] A.
Mnih and Y.
W.
Teh.
A fast and simple algorithm for training neural
probabilistic language models.
In Proceedings of the 29th International
Conference on Machine Learning, pages 1751–1758, 2012.
[27] T.
N.
Sainath, A.
Mohamed, B.
Kingsbury, and B.
Ramabhadran.
Low-
rank matrix factorization for deep neural network training with high-
dimensional output targets.
In Proc.
ICASSP, 2013.
[28] M.
Schuster.
Better generative models for sequential data problems: Bidi-
rectional recurrent mixture density networks.
pages 589–595.
The MIT
Press, 1999.
[29] I.
Sutskever, G.
E.
Hinton, and G.
W.
Taylor.
The recurrent temporal
restricted boltzmann machine.
pages 1601–1608, 2008.
[30] I.
Sutskever, J.
Martens, and G.
Hinton.
Generating text with recurrent
neural networks.
In ICML, 2011.
[31] G.
W.
Taylor and G.
E.
Hinton.
Factored conditional restricted boltzmann
machines for modeling motion style.
In Proc.
26th Annual International
Conference on Machine Learning, pages 1025–1032, 2009.
[32] T.
Tieleman and G.
Hinton.
Lecture 6.5 - rmsprop: Divide the gradient by
a running average of its recent magnitude, 2012.
[33] R.
Williams and D.
Zipser.
Gradient-based learning algorithms for recur-
rent networks and their computational complexity.
In Back-propagation:
Theory, Architectures and Applications, pages 433–486.
1995.
43
Self-training methods such as ELMo (Peters et al.,
2018),
GPT
(Radford et al.,
2018),
BERT
(Devlin et al., 2019), XLM (Lample and Conneau,
2019),
and
XLNet (Yang et al.,
2019)
have
brought signiﬁcant performance gains, but it can
be challenging to determine which aspects of
the methods contribute the most.
Training is
computationally expensive, limiting the amount
of tuning that can be done, and is often done with
private training data of varying sizes, limiting
our ability to measure the effects of the modeling
advances.
∗Equal contribution.
1Our models and code are available at:
https://github.com/pytorch/fairseq
We present a replication study of BERT pre-
training (Devlin et al., 2019), which includes a
careful evaluation of the effects of hyperparmeter
tuning and training set size.
We ﬁnd that BERT
was signiﬁcantly undertrained and propose an im-
proved recipe for training BERT models, which
we call RoBERTa, that can match or exceed the
performance of all of the post-BERT methods.
Our modiﬁcations are simple, they include: (1)
training the model longer, with bigger batches,
over more data; (2) removing the next sentence
prediction objective; (3) training on longer se-
quences; and (4) dynamically changing the mask-
ing pattern applied to the training data.
We also
collect a large new dataset (CC-NEWS) of compa-
rable size to other privately used datasets, to better
control for training set size effects.
When controlling for training data, our im-
proved training procedure improves upon the pub-
lished BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our
model achieves a score of 88.5 on the public
GLUE leaderboard, matching the 88.4 reported
by Yang et al.
(2019).
Our model establishes a
new state-of-the-art on 4/9 of the GLUE tasks:
MNLI, QNLI, RTE and STS-B.
We also match
state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT’s masked lan-
guage model training objective is competitive
with other recently proposed training objectives
such as perturbed autoregressive language model-
ing (Yang et al., 2019).2
In summary, the contributions of this paper
are: (1) We present a set of important BERT de-
sign choices and training strategies and introduce
2It is possible that these other methods could also improve
with more tuning.
We leave this exploration to future work.
alternatives that lead to better downstream task
performance; (2) We use a novel dataset, CC-
NEWS, and conﬁrm that using more data for pre-
training further improves performance on down-
stream tasks; (3) Our training improvements show
that masked language model pretraining, under
the right design choices, is competitive with all
other recently published methods.
We release our
model, pretraining and ﬁne-tuning code imple-
mented in PyTorch (Paszke et al., 2017).
In this section, we give a brief overview of the
BERT (Devlin et al., 2019) pretraining approach
and some of the training choices that we will ex-
amine experimentally in the following section.
2.1
Setup
BERT takes as input a concatenation of two
segments
(sequences
of
tokens),
x1, .
.
.
, xN
and y1, .
.
.
, yM.
Segments usually consist of
more than one natural sentence.
The two seg-
ments are presented as a single input sequence
to BERT with special tokens delimiting them:
[CLS], x1, .
.
.
, xN, [SEP], y1, .
.
.
, yM, [EOS].
M and N are constrained such that M + N < T,
where T is a parameter that controls the maximum
sequence length during training.
The model is ﬁrst pretrained on a large unla-
beled text corpus and subsequently ﬁnetuned us-
ing end-task labeled data.
2.2
Architecture
BERT uses the now ubiquitous transformer archi-
tecture (Vaswani et al., 2017), which we will not
review in detail.
We use a transformer architecture
with L layers.
Each block uses A self-attention
heads and hidden dimension H.
2.3
Training Objectives
During pretraining, BERT uses two objectives:
masked language modeling and next sentence pre-
diction.
Masked Language Model (MLM)
A random
sample of the tokens in the input sequence is
selected and replaced with the special token
[MASK].
The MLM objective is a cross-entropy
loss on predicting the masked tokens.
BERT uni-
formly selects 15% of the input tokens for possi-
ble replacement.
Of the selected tokens, 80% are
replaced with [MASK], 10% are left unchanged,
and 10% are replaced by a randomly selected vo-
cabulary token.
In the original implementation, random mask-
ing and replacement is performed once in the be-
ginning and saved for the duration of training, al-
though in practice, data is duplicated so the mask
is not always the same for every training sentence
(see Section 4.1).
Next Sentence Prediction (NSP)
NSP is a bi-
nary classiﬁcation loss for predicting whether two
segments follow each other in the original text.
Positive examples are created by taking consecu-
tive sentences from the text corpus.
Negative ex-
amples are created by pairing segments from dif-
ferent documents.
Positive and negative examples
are sampled with equal probability.
The NSP objective was designed to improve
performance on downstream tasks, such as Natural
Language Inference (Bowman et al., 2015), which
require reasoning about the relationships between
pairs of sentences.
2.4
Optimization
BERT is optimized with Adam (Kingma and Ba,
2015) using the following parameters: β1 = 0.9,
β2
=
0.999, ǫ
=
1e-6 and L2 weight de-
cay of 0.01.
The learning rate is warmed up
over the ﬁrst 10,000 steps to a peak value of
1e-4, and then linearly decayed.
BERT trains
with a dropout of 0.1 on all layers and at-
tention weights, and a GELU activation func-
tion (Hendrycks and Gimpel, 2016).
Models are
pretrained for S = 1,000,000 updates, with mini-
batches containing B = 256 sequences of maxi-
mum length T = 512 tokens.
2.5
Data
BERT is trained on a combination of BOOKCOR-
PUS (Zhu et al., 2015) plus English WIKIPEDIA,
which totals 16GB of uncompressed text.3
3
Experimental Setup
In this section, we describe the experimental setup
for our replication study of BERT.
3.1
Implementation
We reimplement BERT in FAIRSEQ (Ott et al.,
2019).
We primarily follow the original BERT
3Yang et al.
(2019) use the same dataset but report having
only 13GB of text after data cleaning.
This is most likely due
to subtle differences in cleaning of the Wikipedia data.
optimization hyperparameters, given in Section 2,
except for the peak learning rate and number of
warmup steps, which are tuned separately for each
setting.
We additionally found training to be very
sensitive to the Adam epsilon term, and in some
cases we obtained better performance or improved
stability after tuning it.
Similarly, we found setting
β2 = 0.98 to improve stability when training with
large batch sizes.
We pretrain with sequences of at most T = 512
tokens.
Unlike Devlin et al.
(2019), we do not ran-
domly inject short sequences, and we do not train
with a reduced sequence length for the ﬁrst 90% of
updates.
We train only with full-length sequences.
We train with mixed precision ﬂoating point
arithmetic on DGX-1 machines, each with 8 ×
32GB Nvidia V100 GPUs interconnected by In-
ﬁniband (Micikevicius et al., 2018).
3.2
Data
BERT-style pretraining crucially relies on large
quantities of text.
Baevski et al.
(2019) demon-
strate that increasing data size can result in im-
proved end-task performance.
Several efforts
have trained on datasets larger and more diverse
than the original BERT (Radford et al., 2019;
Yang et al., 2019; Zellers et al., 2019).
Unfortu-
nately, not all of the additional datasets can be
publicly released.
For our study, we focus on gath-
ering as much data as possible for experimenta-
tion, allowing us to match the overall quality and
quantity of data as appropriate for each compari-
son.
We consider ﬁve English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text.
We use the following text
corpora:
• BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA.
This is the original data used to
train BERT.
(16GB).
• CC-NEWS, which we collected from the En-
glish portion of the CommonCrawl News
dataset (Nagel, 2016).
The data contains 63
million English news articles crawled between
September 2016 and February 2019.
(76GB af-
ter ﬁltering).4
• OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText cor-
4We use news-please (Hamborg et al., 2017) to col-
lect and extract CC-NEWS.
CC-NEWS is similar to the RE-
ALNEWS dataset described in Zellers et al.
(2019).
pus described in Radford et al.
(2019).
The text
is web content extracted from URLs shared on
Reddit with at least three upvotes.
(38GB).5
• STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data ﬁltered to match the story-like style of
Winograd schemas.
(31GB).
3.3
Evaluation
Following previous work, we evaluate our pre-
trained models on downstream tasks using the fol-
lowing three benchmarks.
GLUE
The
General
Language
Understand-
ing Evaluation (GLUE) benchmark (Wang et al.,
2019b) is a collection of 9 datasets for evaluating
natural language understanding systems.6 Tasks
are framed as either single-sentence classiﬁcation
or sentence-pair classiﬁcation tasks.
The GLUE
organizers provide training and development data
splits as well as a submission server and leader-
board that allows participants to evaluate and com-
pare their systems on private held-out test data.
For the replication study in Section 4, we report
results on the development sets after ﬁnetuning
the pretrained models on the corresponding single-
task training data (i.e., without multi-task training
or ensembling).
Our ﬁnetuning procedure follows
the original BERT paper (Devlin et al., 2019).
In Section 5 we additionally report test set re-
sults obtained from the public leaderboard.
These
results depend on a several task-speciﬁc modiﬁca-
tions, which we describe in Section 5.1.
SQuAD
The
Stanford
Question
Answering
Dataset (SQuAD) provides a paragraph of context
and a question.
The task is to answer the question
by extracting the relevant span from the context.
We evaluate on two versions of SQuAD: V1.1
and V2.0 (Rajpurkar et al., 2016, 2018).
In V1.1
the context always contains an answer, whereas in
5The authors and their afﬁliated institutions are not in any
way afﬁliated with the creation of the OpenWebText dataset.
6The
datasets
are:
CoLA
(Warstadt et al.,
2018),
Stanford
Sentiment
Treebank
(SST)
(Socher et al.,
2013),
Microsoft
Research
Paragraph
Corpus
(MRPC)
(Dolan and Brockett,
2005),
Semantic
Tex-
tual Similarity Benchmark (STS) (Agirre et al., 2007),
Quora Question Pairs (QQP) (Iyer et al., 2016), Multi-
Genre NLI (MNLI) (Williams et al., 2018), Question NLI
(QNLI)
(Rajpurkar et al.,
2016),
Recognizing
Textual
Entailment
(RTE)
(Dagan et al.,
2006;
Bar-Haim et al.,
2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and
Winograd NLI (WNLI) (Levesque et al., 2011).
V2.0 some questions are not answered in the pro-
vided context, making the task more challenging.
For SQuAD V1.1 we adopt the same span pre-
diction method as BERT (Devlin et al., 2019).
For
SQuAD V2.0, we add an additional binary classi-
ﬁer to predict whether the question is answerable,
which we train jointly by summing the classiﬁca-
tion and span loss terms.
During evaluation, we
only predict span indices on pairs that are classi-
ﬁed as answerable.
RACE
The ReAding Comprehension from Ex-
aminations (RACE) (Lai et al., 2017) task is a
large-scale reading comprehension dataset with
more than 28,000 passages and nearly 100,000
questions.
The dataset is collected from English
examinations in China, which are designed for
middle and high school students.
In RACE, each
passage is associated with multiple questions.
For
every question, the task is to select one correct an-
swer from four options.
RACE has signiﬁcantly
longer context than other popular reading compre-
hension datasets and the proportion of questions
that requires reasoning is very large.
4
Training Procedure Analysis
This section explores and quantiﬁes which choices
are important for successfully pretraining BERT
models.
We keep the model architecture ﬁxed.7
Speciﬁcally, we begin by training BERT models
with the same conﬁguration as BERTBASE (L =
12, H = 768, A = 12, 110M params).
4.1
Static vs.
Dynamic Masking
As discussed in Section 2, BERT relies on ran-
domly masking and predicting tokens.
The orig-
inal BERT implementation performed masking
once during data preprocessing, resulting in a sin-
gle static mask.
To avoid using the same mask for
each training instance in every epoch, training data
was duplicated 10 times so that each sequence is
masked in 10 different ways over the 40 epochs of
training.
Thus, each training sequence was seen
with the same mask four times during training.
We compare this strategy with dynamic mask-
ing where we generate the masking pattern every
time we feed a sequence to the model.
This be-
comes crucial when pretraining for more steps or
with larger datasets.
7Studying architectural changes, including larger archi-
tectures, is an important area for future work.
Masking
SQuAD 2.0
MNLI-m
SST-2
reference
76.3
84.3
92.8
Our reimplementation:
static
78.3
84.3
92.5
dynamic
78.7
84.0
92.9
Table 1:
Comparison between static and dynamic
masking for BERTBASE.
We report F1 for SQuAD and
accuracy for MNLI-m and SST-2.
Reported results are
medians over 5 random initializations (seeds).
Refer-
ence results are from Yang et al.
(2019).
Abstractive summarization aims at presenting the
main points of an article in a succinct and coherent
manner.
To achieve this goal, a proﬁcient editor
can rewrite a source sentence into a more succinct
form by dropping inessential sentence elements
such as prepositional phrases and adjectives.
She
can also choose to fuse multiple source sentences
into one by reorganizing the points in a coherent
manner.
In fact, it appears to be common practice
to summarize by either compressing single sen-
tences or fusing multiple sentences.
We investi-
gate this hypothesis by analyzing human-written
abstracts contained in three large datasets: DUC-
04 (Over and Yen, 2004), CNN/Daily Mail (Her-
mann et al., 2015), and XSum (Narayan et al.,
2018).
For every summary sentence, we ﬁnd its
ground-truth set containing one or more source
CNN/DM
DUC−04
XSum
0%
20%
40%
60%
80%
InstanceType
Compression (1)
Fusion (2)
Fusion (3+)
Figure 1: Portions of summary sentences generated by com-
pression (content is drawn from one source sentence) and fu-
sion (content is drawn from two or more source sentences).
Humans often grab content from 1 or 2 document sentences
when writing a summary sentence.
sentences that exhibit a high degree of similarity
with the summary sentence (details in §4).
As
shown in Figure 1, across the three datasets, 60-
85% of summary sentences are generated by fus-
ing one or two source sentences.
Selecting summary-worthy sentences has been
studied in the literature, but there lacks a mecha-
nism to weigh sentence singletons and pairs in a
uniﬁed space.
Extractive methods focus on select-
ing sentence singletons using greedy (Carbonell
and Goldstein, 1998), optimization-based (Gillick
and Favre, 2009; Kulesza and Taskar, 2011;
Cho et al., 2019), and (non-)autoregressive meth-
ods (Cheng and Lapata, 2016; Kedzie et al., 2018).
In contrast, existing sentence fusion studies tend
to assume ground sets of source sentences are al-
ready provided, and the system fuses each set of
sentences into a single one (Daum´e III and Marcu,
2004; Filippova, 2010; Thadani and McKeown,
2013).
There is thus a crucial gap between sen-
tence selection and fusion to support summarizing
by both compressing single sentences and fusing
pairs.
This paper attempts to bridge the gap by
ranking singletons and pairs together by their like-
lihoods of producing summary sentences.
The selection of sentence singletons and pairs
can bring beneﬁt to neural abstractive summa-
rization, as a number of studies seek to separate
content selection from summary generation (Chen
and Bansal, 2018; Hsu et al., 2018; Gehrmann
arXiv:1906.00077v1  [cs.CL]  31 May 2019
et al., 2018; Lebanoff et al., 2018).
Content selec-
tion draws on domain knowledge to identify rel-
evant content, while summary generation weaves
together selected source and vocabulary words to
form a coherent summary.
Despite having local
coherence, system summaries can sometimes con-
tain erroneous details (See et al., 2017) and forged
content (Cao et al., 2018b; Song et al., 2018).
Separating the two tasks of content selection and
summary generation allows us to closely examine
the compressing and fusing mechanisms of an ab-
stractive summarizer.
In this paper we propose a method to learn to
select sentence singletons and pairs, which then
serve as the basis for an abstractive summarizer to
compose a summary sentence-by-sentence, where
singletons are shortened (i.e., compressed) and
pairs are merged (i.e., fused).
We exploit state-
of-the-art neural representations and traditional
vector space models to characterize singletons
and pairs; we then provide suggestions on the
types of representations useful for summarization.
Experiments are performed on both single- and
multi-document summarization datasets, where
we demonstrate the efﬁcacy of selecting sentence
singletons and pairs as well as its utility to ab-
stractive summarization.
Our research contribu-
tions can be summarized as follows:
• the present study ﬁlls an important gap by se-
lecting sentence singletons and pairs jointly, as-
suming a summary sentence can be created by
either shortening a singleton or merging a pair.
Compared to abstractive summarizers that per-
form content selection implicitly, our method is
ﬂexible and can be extended to multi-document
summarization where training data is limited;
• we investigate the factors involved in represent-
ing sentence singletons and pairs.
We perform
extensive experiments and report ﬁndings on
sentence selection and abstraction.1
2
Related Work
Content selection is integral to any summarization
system.
Neural approaches to abstractive sum-
marization often perform content selection jointly
with surface realization using an encoder-decoder
architecture (Rush et al., 2015; Nallapati et al.,
2016; Chen et al., 2016b; Tan et al., 2017; See
1We make our code and models publicly available at https:
//github.com/ucfnlp/summarization-sing-pair-mix
et al., 2017; Paulus et al., 2017; Celikyilmaz et al.,
2018; Narayan et al., 2018).
Training these models
end-to-end means learning to perform both tasks
simultaneously and can require a massive amount
of data that is unavailable and unaffordable for
many summarization tasks.
Recent approaches emphasize the importance of
separating content selection from summary gener-
ation for abstractive summarization.
Studies ex-
ploit extractive methods to identify content words
and sentences that should be part of the sum-
mary and use them to guide the generation of ab-
stracts (Chen and Bansal, 2018; Gehrmann et al.,
2018; Lebanoff et al., 2018).
On the other hand,
surface lexical features have been shown to be ef-
fective in identifying pertinent content (Carenini
et al., 2006; Wong et al., 2008; Galanis et al.,
2012).
Examples include sentence length, posi-
tion, centrality, word frequency, whether a sen-
tence contains topic words, and others.
The sur-
face cues can also be customized for new domains
relatively easily.
This paper represents a step for-
ward in this direction, where we focus on develop-
ing lightweight models to select summary-worthy
sentence singletons and pairs and use them as the
basis for summary generation.
A succinct sentence can be generated by short-
ening or rewriting a lengthy source text.
Recent
studies have leveraged neural encoder-decoder
models to rewrite the ﬁrst sentence of an article to
a title-like summary (Nallapati et al., 2016; Zhou
et al., 2017; Li et al., 2017; Song et al., 2018; Guo
et al., 2018; Cao et al., 2018a).
Compressive sum-
maries can be generated in a similar vein by se-
lecting important source sentences and then drop-
ping inessential sentence elements such as prepo-
sitional phrases.
Before the era of deep neural net-
works it has been an active area of research, where
sentence selection and compression can be accom-
plished using a pipeline or a joint model (Daum´e
III and Marcu, 2002; Zajic et al., 2007; Gillick and
Favre, 2009; Wang et al., 2013; Li et al., 2013,
2014; Filippova et al., 2015).
A majority of these
studies focus on selecting and compressing sen-
tence singletons only.
A sentence can also be generated through fus-
ing multiple source sentences.
However, many
aspects of this approach are largely underinvesti-
gated, such as determining the set of source sen-
tences to be fused, handling its large cardinality,
and identifying the sentence relationships for per-
Sentence Pair:
Merged Sentence:
(A) The bombing killed 58 people.
Pakistan denies its spy agency helped plan bombing that
(B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul,
killed 58.
former head of the ISI, ﬁrmly denied the agency’s involvement in the attack.
Sentence Singleton:
Compressed Sentence:
(A) Pakistani Maj.
Gen.
Athar Abbas said the report “unfounded and malicious” and
Maj.
Gen.
Athar Abbas said the report was an “effort to
an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence.
malign the ISI.”
Table 1: Example sentence singleton and pair, before and after compression/merging.
forming fusion.
Previous studies assume a set
of similar source sentences can be gathered by
clustering sentences or by comparing to a refer-
ence summary sentence (Barzilay and McKeown,
2005; Filippova, 2010; Shen and Li, 2010; Chenal
and Cheung, 2016; Liao et al., 2018); but these
methods can be suboptimal.
Joint models for sen-
tence selection and fusion implicitly perform con-
tent planning (Martins and Smith, 2009; Berg-
Kirkpatrick et al., 2011; Bing et al., 2015; Durrett
et al., 2016) and there is limited control over which
sentences are merged and how.
In contrast, this work attempts to teach the sys-
tem to determine if a sentence singleton or a pair
should be selected to produce a summary sen-
tence.
A sentence pair (A, B) is preferred over its
consisting sentences if they carry complementary
content.
Table 1 shows an example.
Sentence B
contains a reference (“the attack”) and A contains
a more complete description for it (“bombing that
killed 58”).
Sentences A and B each contain cer-
tain valuable information, and an appropriate way
to merge them exists.
As a result, a sentence pair
can be scored higher than a singleton given the
content it carries and compatibility of its consist-
ing sentences.
In the following we describe meth-
ods to represent singletons and pairs in a uniﬁed
framework and scoring them for summarization.
3
Our Model
We present the ﬁrst attempt to transform sentence
singletons and pairs to real-valued vector repre-
sentations capturing semantic salience so that they
can be measured against each other (§3.1).
This is
a nontrivial task, as it requires a direct comparison
of texts of varying length—a pair of sentences is
almost certainly longer than a single sentence.
For
sentence pairs, the representations are expected to
further encode sentential semantic compatibility.
In §3.2, we describe our method to utilize highest
scoring singletons and pairs to a neural abstractive
summarizer to generate summaries.
3.1
Scoring Sentence Singletons and Pairs
Given a document or set of documents, we create
a set D of singletons and pairs by gathering all sin-
gle sentences and arbitrary pairs of them.
We refer
to a singleton or pair in the set as an instance.
The
sentences in a pair are arranged in order of their
appearance in the document or by date of docu-
ments.
Let N be the number of single sentences
in the input document(s), a complete set of sin-
gletons and pairs will contain |D|=N(N−1)
2
+N in-
stances.
Our goal is to score each instance based
on the amount of summary-worthy content it con-
veys.
Despite their length difference, a singleton
can be scored higher than a pair if it contains a sig-
niﬁcant amount of salient content.
Conversely, a
pair can outweigh a singleton if its component sen-
tences are salient and compatible with each other.
Building effective representations for singletons
and pairs is therefore of utmost importance.
We
attempt to build a vector representation for each
instance.
The representation should be invariant
to the instance type, i.e., a singleton or pair.
In this
paper we exploit the BERT architecture (Devlin
et al., 2018) to learn instance representations.
The
representations are ﬁne-tuned for a classiﬁcation
task predicting whether a given instance contains
content used in human-written summary sentences
(details for ground-truth creation in §4).
BERT
BERT supports our goal of encoding
singletons and pairs indiscriminately.
It introduces
two pretraining tasks to build deep contextual rep-
resentations for words and sequences.
A sequence
can be a single sentence (A) or pair of sentences
(A+B).2 The ﬁrst task predicts missing words in
the input sequence.
The second task predicts if
B is the next sentence following A.
It requires the
vector representation for (A+B) to capture the co-
herence of two sentences.
As coherent sentences
can often be fused together, we conjecture that the
second task is particularly suited for our goal.
2In the original BERT paper (Devlin et al., 2018), a “sen-
tence” is used in a general sense to denote an arbitrary span
of contiguous text; we refer to an actual linguistic sentence.
Concretely, BERT constructs an input sequence
by prepending a singleton or pair with a “[CLS]”
symbol and delimiting the two sentences of a pair
with “[SEP].” The representation learned for the
[CLS] symbol is used as an aggregate sequence rep-
resentation for the later classiﬁcation task.
We
show an example input sequence in Eq.
(1).
In
the case of a singleton, wB
i are padding tokens.
{wi}=[CLS],wA
1,wA
2,..., [SEP],wB
1,wB
2,..., [SEP] (1)
ei=ew(wi)+esgmt(wi)+ewpos(wi)+espos(wi) (2)
In Eq.
(2), each token wi is characterized by
an input embedding ei, calculated as the element-
wise sum of the following embeddings:
• ew(wi) is a token embedding;
• esgmt(wi) is a segment embedding, signifying
whether wi comes from sentence A or B.
• ewpos(wi) is a word position embedding indicat-
ing the index of wi in the input sequence;
• we introduce espos(wi) to be a sentence posi-
tion embedding; if wi is from sentence A (or B),
espos(wi) is the embedding indicating the index
of sentence A (or B) in the original document.
Intuitively, these embeddings mean that, the ex-
tent to which a word contributes to the sequence
(A+B) representation depends on these factors: (i)
word salience, (ii) importance of sentences A and
B, (iii) word position in the sequence, and, (iv)
sentence position in the document.
These factors
coincide with heuristics used in summarization lit-
erature (Nenkova and McKeown, 2011), where
leading sentences of a document and the ﬁrst few
words of a sentence are more likely to be included
in the summary.
The input embeddings are then fed to a multi-
layer and multi-head attention architecture to build
deep contextual representations for tokens.
Each
layer employs a Transformer block (Vaswani et al.,
2017), which introduces a self-attention mech-
anism that allows each hidden state hl
i to be
compared with every other hidden state of the
same layer [hl
1, hl
2, .
.
.
, hl
N] using a parallelizable,
multi-head attention mechanism (Eq.
(3-4)).
h1
i = f1
self-attn(ei, [e1, e2, .
.
.
, eN])
(3)
hl+1
i
= fl+1
self-attn(hl
i, [hl
1, hl
2, .
.
.
, hl
N])
(4)
The representation at ﬁnal layer L for the [CLS]
symbol is used as the sequence representation
hL
[CLS].
The representations can be ﬁne-tuned with
an additional output layer to generate state-of-
the-art results on a wide range of tasks including
reading comprehension and natural language in-
ference.
We use the pretrained BERT base model
and ﬁne-tune it on our speciﬁc task of predict-
ing if an instance (a singleton or pair) pinst =
σ(w⊤hL
[CLS]) is an appropriate one, i.e., belonging
to the ground-truth set of summary instances for a
given document.
At test time, the architecture in-
discriminately encodes a mixed collection of sen-
tence singletons/pairs.
We then obtain a likelihood
score for each instance.
This framework is thus
a ﬁrst effort to build semantic representations for
singletons and pairs capturing informativeness and
semantic compatibility of two sentences.
VSM
We are interested in contrasting BERT
with the traditional vector space model (Manning
et al., 2008) for representing singletons and pairs.
BERT learns instance representations by attending
to important content words, where the importance
is signaled by word and position embeddings as
well as pairwise word relationships.
Nonetheless,
it remains an open question whether BERT can
successfully weave the meaning of topically im-
portant words into representations.
A word “bor-
der” is topically important if the input document
discusses border security.
A topic word is likely to
be repeatedly mentioned in the input document but
less frequently elsewhere.
Because sentences con-
taining topical words are often deemed summary-
worthy (Hong and Nenkova, 2014), it is desirable
to represent sentence singletons and pairs based on
the amount of topical content they convey.
VSM represents each sentence as a sparse vec-
tor.
Each dimension of the vector corresponds to
an n-gram weighted by its TF-IDF score.
A high
TF-IDF score suggests the n-gram is important to
the topic of discussion.
We further strengthen the
sentence vector with position and centrality infor-
mation, i.e., the sentence position in the document
and the cosine similarity between the sentence and
document vector.
We obtain a document vector by
averaging over its sentence vectors, and we simi-
larly obtain a vector for a pair of sentences.
We use
VSM representations as a baseline to contrast its
performance with distributed representations from
BERT.
To score singletons and pairs, we use the
LambdaMART model3 which has demonstrated
success on related NLP tasks (Chen et al., 2016a);
3https://sourceforge.net/p/lemur/wiki/RankLib/
it also ﬁts our requirements of ranking singletons
and pairs indiscriminately.
3.2
Generating Summaries
We proceed by performing a preliminary investi-
gation of summary generation from singletons and
pairs; they are collectively referred to as instances.
In the previous section, a set of summary instances
is selected from a document.
These instances are
treated as “raw materials” for a summary; they
are fed to a neural abstractive summarizer which
processes them into summary sentences via fusion
and compression.
This strategy allows us to sepa-
rately evaluate the contributions from instance se-
lection and summary composition.
We employ the MMR principle (Carbonell and
Goldstein, 1998) to select a set of highest scoring
and non-redundant instances.
The method adds an
instance ˆP to the summary S iteratively per Eq.
(5)
until a length threshold has been reached.
Each
instance is weighted by a linear combination of
its importance score I(Pk), obtained by BERT or
VSM, and its redundancy score R(Pk), computed
as the cosine similarity between the instance and
partial summary.
λ is a balancing factor between
importance and redundancy.4 Essentially, MMR
prevents the system from selecting instances that
are too similar to ones already selected.
ˆP = arg max
Pk∈D\S
h
λI(Pk) −(1 −λ)R(Pk)
i
(5)
Composing a summary from selected instances
is a non-trivial task.
As a preliminary investigation
of summary composition, we make use of pointer-
generator (PG) networks (See et al., 2017) to com-
press/fuse sentences into summary sentences.
PG
is a sequence-to-sequence model that has achieved
state-of-the-art performance in abstractive sum-
marization by having the ability to both copy to-
kens from the document or generate new tokens
from the vocabulary.
When trained on document-
summary pairs, the model has been shown to re-
move unnecessary content from sentences and can
merge multiple sentences together.
In this work, rather than training on document-
summary pairs,
we train PG exclusively on
ground-truth instances.
This removes most of the
responsibility of content selection, and allows it to
focus its efforts on merging the sentences.
We use
instances derived from human summaries (§4) to
4We use a coefﬁcient λ of 0.6.
Input 
Document(s)
Scoring 
Singletons and Pairs
0.30
0.86
0.25
0.02
0.61
Encoder
Decoder
2nd Summ Sent
1st Summ Sent
Content Selection
Summary Generation
Figure 2: System architecture.
In this example, a sentence
pair is chosen (red) and then merged to generate the ﬁrst sum-
mary sentence.
Next, a sentence singleton is selected (blue)
and compressed for the second summary sentence.
train the network, which includes a sentence sin-
gleton or pair along with the ground-truth com-
pressed/merged sentence.
At test time, the net-
work receives an instance from BERT or VSM and
outputs a summary sentence, then repeats this pro-
cess to generate several sentences.
In Figure 2 we
present an illustration of the system architecture.
4
Data
Our method does not require a massive amount of
annotated data.
We thus report results on single-
and multi-document summarization datasets.
We experiment with (i) XSum (Narayan et al.,
2018), a new dataset created for extreme, abstrac-
tive summarization.
The task is to reduce a news
article to a short, one-sentence summary.
Both
source articles and reference summaries are gath-
ered from the BBC website.
The training set con-
tains about 204k article-summary pairs and the test
contains 11k pairs.
(ii) CNN/DM (Hermann et al.,
2015), an abstractive summarization dataset fre-
quently exploited by recent studies.
The task is to
reduce a news article to a multi-sentence summary
(4 sentences on average).
The training set contains
about 287k article-summary pairs and the test set
contains 11k pairs.
We use the non-anonymzied
version of the dataset.
(iii) DUC-04 (Over and
Yen, 2004), a benchmark multi-document summa-
rization dataset.
The task is to create an abstractive
summary (5 sentences on average) from a set of 10
documents discussing a given topic.
The dataset
contains 50 sets of documents used for testing pur-
pose only.
Each document set is associated with
four human reference summaries.
We build a training set for both tasks of content
selection and summary generation.
This is done
by creating ground-truth sets of instances based
on document-summary pairs.
Each document and
summary pair (D, S) is a collection of sentences
D = {d1, d2, ..., dM} and S = {s1, s2, ..., sN}.
We wish to associate each summary sentence sn
with a subset of the document sentences ˜D ⊆D,
which are the sentences that are merged to form
sn.
Our method chooses multiple sentences that
work together to capture the most overlap with
summary sentence sn, in the following way.
We use averaged ROUGE-1, -2, -L scores (Lin,
2004) to represent sentence similarity.
The source
sentence most similar to sn is chosen, which we
call ˜d1.
All shared words are then removed from
sn to create s′
n, effectively removing all informa-
tion already captured by ˜d1.
A second source sen-
tence ˜d2 is selected that is most similar to the re-
maining summary sentence s′
n, and shared words
are again removed from s′
n to create s′′
n.
This
process of sentence selection and overlap removal
is repeated until no remaining sentences have at
least two overlapping content words (words that
are non-stopwords or punctuation) with sn.
The
result is referred to as a ground-truth set (sn, ˜D)
where ˜D = { ˜d1, ˜d2, ..., ˜d| ˜D|}.
To train the mod-
els, ˜D is limited to one or two sentences because
it captures the large majority of cases.
All empty
ground-truth sets are removed, and only the ﬁrst
two sentences are chosen for all ground-truth sets
with more than two sentences.
A small number of
summary sentences have empty ground-truth sets,
corresponding to 2.85%, 9.87%, 5.61% of sum-
mary sentences in CNN/DM, XSum, and DUC-04
datasets.
A detailed plot of the ground-truth set
size is illustrated in Figure 1, and samples of the
ground-truth are found in the supplementary.
We use the standard train/validation/test splits
for both CNN/Daily Mail and XSum.
We train our
models on ground-truth sets of instances created
from the training sets and tune hyperparameters
using instances from the validation sets.
DUC-04
is a test-only dataset, so we use the models trained
on CNN/Daily Mail to evaluate DUC-04.
Because
the input is in the form of multiple documents, we
select the ﬁrst 20 sentences from each document
and concatenate them together into a single mega-
document (Lebanoff et al., 2018).
For the sen-
tence position feature, we keep the sentence posi-
tions from the original documents.
This handling
of sentence position, along with other features that
are invariant to the input type, allows us to effec-
tively train on single-document inputs and transfer
to the multi-document setting.
Semantic segmentation has a wide array of applications ranging
from scene understanding, inferring support-relationships among
objects to autonomous driving.
Early methods that relied on low-
level vision cues have fast been superseded by popular machine
learning algorithms.
In particular, deep learning has seen huge suc-
cess lately in handwritten digit recognition, speech, categorising
whole images and detecting objects in images [5], [6].
Now there
is an active interest for semantic pixel-wise labelling [7] [8], [9],
[2], [4], [10], [11], [12], [13], [3], [14], [15], [16].
However, some
of these recent approaches have tried to directly adopt deep archi-
tectures designed for category prediction to pixel-wise labelling
[7].
The results, although very encouraging, appear coarse [3].
This is primarily because max pooling and sub-sampling reduce
feature map resolution.
Our motivation to design SegNet arises
from this need to map low resolution features to input resolution
for pixel-wise classiﬁcation.
This mapping must produce features
which are useful for accurate boundary localization.
Our architecture, SegNet, is designed to be an efﬁcient ar-
chitecture for pixel-wise semantic segmentation.
It is primarily
motivated by road scene understanding applications which require
the ability to model appearance (road, building), shape (cars,
•
V.
Badrinarayanan, A.
Kendall, R.
Cipolla are with the Machine Intelli-
gence Lab, Department of Engineering, University of Cambridge, UK.
E-mail: vb292,agk34,cipolla@eng.cam.ac.uk
pedestrians) and understand the spatial-relationship (context) be-
tween different classes such as road and side-walk.
In typical road
scenes, the majority of the pixels belong to large classes such
as road, building and hence the network must produce smooth
segmentations.
The engine must also have the ability to delineate
objects based on their shape despite their small size.
Hence it is
important to retain boundary information in the extracted image
representation.
From a computational perspective, it is necessary
for the network to be efﬁcient in terms of both memory and
computation time during inference.
The ability to train end-to-end
in order to jointly optimise all the weights in the network using
an efﬁcient weight update technique such as stochastic gradient
descent (SGD) [17] is an additional beneﬁt since it is more easily
repeatable.
The design of SegNet arose from a need to match these
criteria.
The encoder network in SegNet is topologically identical to
the convolutional layers in VGG16 [1].
We remove the fully
connected layers of VGG16 which makes the SegNet encoder
network signiﬁcantly smaller and easier to train than many other
recent architectures [2], [4], [11], [18].
The key component of
SegNet is the decoder network which consists of a hierarchy
of decoders one corresponding to each encoder.
Of these, the
appropriate decoders use the max-pooling indices received from
the corresponding encoder to perform non-linear upsampling of
their input feature maps.
This idea was inspired from an archi-
tecture designed for unsupervised feature learning [19].
Reusing
max-pooling indices in the decoding process has several practical
arXiv:1511.00561v3  [cs.CV]  10 Oct 2016
2
Fig.
1.
SegNet predictions on road scenes and indoor scenes.
To try our system yourself, please see our online web demo at http://mi.eng.cam.ac.
uk/projects/segnet/.
advantages; (i) it improves boundary delineation , (ii) it reduces the
number of parameters enabling end-to-end training, and (iii) this
form of upsampling can be incorporated into any encoder-decoder
architecture such as [2], [10] with only a little modiﬁcation.
One of the main contributions of this paper is our analysis
of the SegNet decoding technique and the widely used Fully
Convolutional Network (FCN) [2].
This is in order to convey
the practical trade-offs involved in designing segmentation archi-
tectures.
Most recent deep architectures for segmentation have
identical encoder networks, i.e VGG16, but differ in the form
of the decoder network, training and inference.
Another common
feature is they have trainable parameters in the order of hundreds
of millions and thus encounter difﬁculties in performing end-to-
end training [4].
The difﬁculty of training these networks has led
to multi-stage training [2], appending networks to a pre-trained
architecture such as FCN [10], use of supporting aids such as
region proposals for inference [4], disjoint training of classiﬁcation
and segmentation networks [18] and use of additional training data
for pre-training [11] [20] or for full training [10].
In addition,
performance boosting post-processing techniques [3] have also
been popular.
Although all these factors improve performance on
challenging benchmarks [21], it is unfortunately difﬁcult from
their quantitative results to disentangle the key design factors
necessary to achieve good performance.
We therefore analysed
the decoding process used in some of these approaches [2], [4]
and reveal their pros and cons.
We evaluate the performance of SegNet on two scene seg-
mentation tasks, CamVid road scene segmentation [22] and SUN
RGB-D indoor scene segmentation [23].
Pascal VOC12 [21] has
been the benchmark challenge for segmentation over the years.
However, the majority of this task has one or two foreground
classes surrounded by a highly varied background.
This implicitly
favours techniques used for detection as shown by the recent
work on a decoupled classiﬁcation-segmentation network [18]
where the classiﬁcation network can be trained with a large set of
weakly labelled data and the independent segmentation network
performance is improved.
The method of [3] also use the feature
maps of the classiﬁcation network with an independent CRF post-
processing technique to perform segmentation.
The performance
can also be boosted by the use additional inference aids such as
region proposals [4], [24].
Therefore, it is different from scene
understanding where the idea is to exploit co-occurrences of
objects and other spatial-context to perform robust segmentation.
To demonstrate the efﬁcacy of SegNet, we present a real-time
online demo of road scene segmentation into 11 classes of interest
for autonomous driving (see link in Fig.
1).
Some example test
results produced on randomly sampled road scene images from
Google and indoor test scenes from the SUN RGB-D dataset [23]
are shown in Fig.
1.
The remainder of the paper is organized as follows.
In Sec.
2 we review related recent literature.
We describe the SegNet
architecture and its analysis in Sec.
3.
In Sec.
4 we evaluate the
performance of SegNet on outdoor and indoor scene datasets.
This
is followed by a general discussion regarding our approach with
pointers to future work in Sec.
5.
We conclude in Sec.
6.
2
LITERATURE REVIEW
Semantic pixel-wise segmentation is an active topic of research,
fuelled by challenging datasets [21], [22], [23], [25], [26].
Before
the arrival of deep networks, the best performing methods mostly
relied on hand engineered features classifying pixels indepen-
dently.
Typically, a patch is fed into a classiﬁer e.g.
Random
3
Forest [27], [28] or Boosting [29], [30] to predict the class
probabilities of the center pixel.
Features based on appearance [27]
or SfM and appearance [28], [29], [30] have been explored for
the CamVid road scene understanding test [22].
These per-pixel
noisy predictions (often called unary terms) from the classiﬁers
are then smoothed by using a pair-wise or higher order CRF [29],
[30] to improve the accuracy.
More recent approaches have aimed
to produce high quality unaries by trying to predict the labels
for all the pixels in a patch as opposed to only the center pixel.
This improves the results of Random Forest based unaries [31]
but thin structured classes are classiﬁed poorly.
Dense depth maps
computed from the CamVid video have also been used as input
for classiﬁcation using Random Forests [32].
Another approach
argues for the use of a combination of popular hand designed
features and spatio-temporal super-pixelization to obtain higher
accuracy [33].
The best performing technique on the CamVid
test [30] addresses the imbalance among label frequencies by
combining object detection outputs with classiﬁer predictions in
a CRF framework.
The result of all these techniques indicate the
need for improved features for classiﬁcation.
Indoor RGBD pixel-wise semantic segmentation has also
gained popularity since the release of the NYU dataset [25].
This
dataset showed the usefulness of the depth channel to improve
segmentation.
Their approach used features such as RGB-SIFT,
depth-SIFT and pixel location as input to a neural network
classiﬁer to predict pixel unaries.
The noisy unaries are then
smoothed using a CRF.
Improvements were made using a richer
feature set including LBP and region segmentation to obtain higher
accuracy [34] followed by a CRF.
In more recent work [25], both
class segmentation and support relationships are inferred together
using a combination of RGB and depth based cues.
Another
approach focuses on real-time joint reconstruction and semantic
segmentation, where Random Forests are used as the classiﬁer
[35].
Gupta et al.
[36] use boundary detection and hierarchical
grouping before performing category segmentation.
The common
attribute in all these approaches is the use of hand engineered
features for classiﬁcation of either RGB or RGBD images.
The success of deep convolutional neural networks for object
classiﬁcation has more recently led researchers to exploit their fea-
ture learning capabilities for structured prediction problems such
as segmentation.
There have also been attempts to apply networks
designed for object categorization to segmentation, particularly
by replicating the deepest layer features in blocks to match
image dimensions [7], [37], [38], [39].
However, the resulting
classiﬁcation is blocky [38].
Another approach using recurrent
neural networks [40] merges several low resolution predictions
to create input image resolution predictions.
These techniques are
already an improvement over hand engineered features [7] but
their ability to delineate boundaries is poor.
Newer deep architectures [2], [4], [10], [13], [18] particularly
designed for segmentation have advanced the state-of-the-art by
learning to decode or map low resolution image representations
to pixel-wise predictions.
The encoder network which produces
these low resolution representations in all of these architectures is
the VGG16 classiﬁcation network [1] which has 13 convolutional
layers and 3 fully connected layers.
This encoder network weights
are typically pre-trained on the large ImageNet object classiﬁ-
cation dataset [41].
The decoder network varies between these
architectures and is the part which is responsible for producing
multi-dimensional features for each pixel for classiﬁcation.
Each decoder in the Fully Convolutional Network (FCN)
architecture [2] learns to upsample its input feature map(s) and
combines them with the corresponding encoder feature map to
produce the input to the next decoder.
It is an architecture which
has a large number of trainable parameters in the encoder network
(134M) but a very small decoder network (0.5M).
The overall
large size of this network makes it hard to train end-to-end on
a relevant task.
Therefore, the authors use a stage-wise training
process.
Here each decoder in the decoder network is progressively
added to an existing trained network.
The network is grown until
no further increase in performance is observed.
This growth is
stopped after three decoders thus ignoring high resolution feature
maps can certainly lead to loss of edge information [4].
Apart
from training related issues, the need to reuse the encoder feature
maps in the decoder makes it memory intensive in test time.
We
study this network in more detail as it the core of other recent
architectures [10], [11].
The predictive performance of FCN has been improved further
by appending the FCN with a recurrent neural network (RNN)
[10] and ﬁne-tuning them on large datasets [21], [42].
The RNN
layers mimic the sharp boundary delineation capabilities of CRFs
while exploiting the feature representation power of FCN’s.
They
show a signiﬁcant improvement over FCN-8 but also show that
this difference is reduced when more training data is used to
train FCN-8.
The main advantage of the CRF-RNN is revealed
when it is jointly trained with an architecture such as the FCN-
8.
The fact that joint training helps is also shown in other recent
results [43], [44].
Interestingly, the deconvolutional network [4]
performs signiﬁcantly better than FCN although at the cost of
a more complex training and inference.
This however raises the
question as to whether the perceived advantage of the CRF-RNN
would be reduced as the core feed-forward segmentation engine is
made better.
In any case, the CRF-RNN network can be appended
to any deep segmentation architecture including SegNet.
Multi-scale deep architectures are also being pursued [13],
[44].
They come in two ﬂavours, (i) those which use input images
at a few scales and corresponding deep feature extraction net-
works, and (ii) those which combine feature maps from different
layers of a single deep architecture [45] [11].
The common idea
is to use features extracted at multiple scales to provide both
local and global context [46] and the using feature maps of the
early encoding layers retain more high frequency detail leading to
sharper class boundaries.
Some of these architectures are difﬁcult
to train due to their parameter size [13].
Thus a multi-stage training
process is employed along with data augmentation.
The inference
is also expensive with multiple convolutional pathways for feature
extraction.
Others [44] append a CRF to their multi-scale network
and jointly train them.
However, these are not feed-forward at test
time and require optimization to determine the MAP labels.
Several of the recently proposed deep architectures for seg-
mentation are not feed-forward in inference time [4], [3], [18].
They require either MAP inference over a CRF [44], [43] or
aids such as region proposals [4] for inference.
We believe the
perceived performance increase obtained by using a CRF is due
to the lack of good decoding techniques in their core feed-forward
segmentation engine.
SegNet on the other hand uses decoders to
obtain features for accurate pixel-wise classiﬁcation.
The recently proposed Deconvolutional Network [4] and its
semi-supervised variant the Decoupled network [18] use the max
locations of the encoder feature maps (pooling indices) to perform
non-linear upsampling in the decoder network.
The authors of
these architectures, independently of SegNet (ﬁrst submitted to
4
Convolutional Encoder-Decoder
 
Pooling Indices
Input
Segmentation
Output
Conv + Batch Normalisation + ReLU
Pooling
Upsampling
Softmax
RGB Image
Fig.
2.
An illustration of the SegNet architecture.
There are no fully connected layers and hence it is only convolutional.
A decoder upsamples its
input using the transferred pool indices from its encoder to produce a sparse feature map(s).
It then performs convolution with a trainable ﬁlter bank
to densify the feature map.
The ﬁnal decoder output feature maps are fed to a soft-max classiﬁer for pixel-wise classiﬁcation.
CVPR 2015 [12]), proposed this idea of decoding in the decoder
network.
However, their encoder network consists of the fully con-
nected layers from the VGG-16 network which consists of about
90% of the parameters of their entire network.
This makes training
of their network very difﬁcult and thus require additional aids such
as the use of region proposals to enable training.
Moreover, during
inference these proposals are used and this increases inference
time signiﬁcantly.
From a benchmarking point of view, this also
makes it difﬁcult to evaluate the performance of their architecture
(encoder-decoder network) without other aids.
In this work we
discard the fully connected layers of the VGG16 encoder network
which enables us to train the network using the relevant training
set using SGD optimization.
Another recent method [3] shows
the beneﬁt of reducing the number of parameters signiﬁcantly
without sacriﬁcing performance, reducing memory consumption
and improving inference time.
Our work was inspired by the unsupervised feature learning
architecture proposed by Ranzato et al.
[19].
The key learning
module is an encoder-decoder network.
An encoder consists of
convolution with a ﬁlter bank, element-wise tanh non-linearity,
max-pooling and sub-sampling to obtain the feature maps.
For
each sample, the indices of the max locations computed during
pooling are stored and passed to the decoder.
The decoder up-
samples the feature maps by using the stored pooled indices.
It
convolves this upsampled map using a trainable decoder ﬁlter
bank to reconstruct the input image.
This architecture was used for
unsupervised pre-training for classiﬁcation.
A somewhat similar
decoding technique is used for visualizing trained convolutional
networks [47] for classiﬁcation.
The architecture of Ranzato et al.
mainly focused on layer-wise feature learning using small input
patches.
This was extended by Kavukcuoglu et.
al.
[48] to accept
full image sizes as input to learn hierarchical encoders.
Both these
approaches however did not attempt to use deep encoder-decoder
networks for unsupervised feature training as they discarded the
decoders after each encoder training.
Here, SegNet differs from
these architectures as the deep encoder-decoder network is trained
jointly for a supervised learning task and hence the decoders are
an integral part of the network in test time.
Other applications where pixel wise predictions are made
using deep networks are image super-resolution [49] and depth
map prediction from a single image [50].
The authors in [50]
discuss the need for learning to upsample from low resolution
feature maps which is the central topic of this paper.
3
ARCHITECTURE
SegNet has an encoder network and a corresponding decoder
network, followed by a ﬁnal pixelwise classiﬁcation layer.
This
architecture is illustrated in Fig.
3.
The encoder network consists
of 13 convolutional layers which correspond to the ﬁrst 13
convolutional layers in the VGG16 network [1] designed for object
classiﬁcation.
We can therefore initialize the training process from
weights trained for classiﬁcation on large datasets [41].
We can
also discard the fully connected layers in favour of retaining
higher resolution feature maps at the deepest encoder output.
This
also reduces the number of parameters in the SegNet encoder
network signiﬁcantly (from 134M to 14.7M) as compared to other
recent architectures [2], [4] (see.
Table 6).
Each encoder layer
has a corresponding decoder layer and hence the decoder network
has 13 layers.
The ﬁnal decoder output is fed to a multi-class
soft-max classiﬁer to produce class probabilities for each pixel
independently.
Each encoder in the encoder network performs convolution
with a ﬁlter bank to produce a set of feature maps.
These are
then batch normalized [51], [52]).
Then an element-wise rectiﬁed-
linear non-linearity (ReLU) max(0, x) is applied.
Following that,
max-pooling with a 2 × 2 window and stride 2 (non-overlapping
window) is performed and the resulting output is sub-sampled by
a factor of 2.
Max-pooling is used to achieve translation invariance
over small spatial shifts in the input image.
Sub-sampling results
in a large input image context (spatial window) for each pixel
in the feature map.
While several layers of max-pooling and
sub-sampling can achieve more translation invariance for robust
classiﬁcation correspondingly there is a loss of spatial resolution
of the feature maps.
The increasingly lossy (boundary detail)
image representation is not beneﬁcial for segmentation where
boundary delineation is vital.
Therefore, it is necessary to capture
and store boundary information in the encoder feature maps
before sub-sampling is performed.
If memory during inference
is not constrained, then all the encoder feature maps (after sub-
sampling) can be stored.
This is usually not the case in practical
applications and hence we propose a more efﬁcient way to store
this information.
It involves storing only the max-pooling indices,
i.e, the locations of the maximum feature value in each pooling
window is memorized for each encoder feature map.
In principle,
this can be done using 2 bits for each 2 × 2 pooling window and
is thus much more efﬁcient to store as compared to memorizing
feature map(s) in ﬂoat precision.
As we show later in this work,
5
this lower memory storage results in a slight loss of accuracy but
is still suitable for practical applications.
The appropriate decoder in the decoder network upsamples
its input feature map(s) using the memorized max-pooling indices
from the corresponding encoder feature map(s).
This step pro-
duces sparse feature map(s).
This SegNet decoding technique is
illustrated in Fig.
3.
These feature maps are then convolved with
a trainable decoder ﬁlter bank to produce dense feature maps.
A batch normalization step is then applied to each of these maps.
Note that the decoder corresponding to the ﬁrst encoder (closest to
the input image) produces a multi-channel feature map, although
its encoder input has 3 channels (RGB).
This is unlike the other
decoders in the network which produce feature maps with the
same number of size and channels as their encoder inputs.
The
high dimensional feature representation at the output of the ﬁnal
decoder is fed to a trainable soft-max classiﬁer.
This soft-max
classiﬁes each pixel independently.
The output of the soft-max
classiﬁer is a K channel image of probabilities where K is the
number of classes.
The predicted segmentation corresponds to the
class with maximum probability at each pixel.
We add here that two other architectures, DeconvNet [53] and
U-Net [16] share a similar architecture to SegNet but with some
differences.
DeconvNet has a much larger parameterization, needs
more computational resources and is harder to train end-to-end
(Table 6), primarily due to the use of fully connected layers (albeit
in a convolutional manner) We report several comparisons with
DeconvNet later in the paper Sec.
4.
As compared to SegNet, U-Net [16] (proposed for the medical
imaging community) does not reuse pooling indices but instead
transfers the entire feature map (at the cost of more memory) to
the corresponding decoders and concatenates them to upsampled
(via deconvolution) decoder feature maps.
There is no conv5 and
max-pool 5 block in U-Net as in the VGG net architecture.
SegNet,
on the other hand, uses all of the pre-trained convolutional layer
weights from VGG net as pre-trained weights.
3.1
Decoder Variants
Many segmentation architectures [2], [3], [4] share the same
encoder network and they only vary in the form of their decoder
network.
Of these we choose to compare the SegNet decoding
technique with the widely used Fully Convolutional Network
(FCN) decoding technique [2], [10].
In order to analyse SegNet and compare its performance with
FCN (decoder variants) we use a smaller version of SegNet,
termed SegNet-Basic 1, which has 4 encoders and 4 decoders.
All the encoders in SegNet-Basic perform max-pooling and sub-
sampling and the corresponding decoders upsample its input using
the received max-pooling indices.
Batch normalization is used
after each convolutional layer in both the encoder and decoder
network.
No biases are used after convolutions and no ReLU non-
linearity is present in the decoder network.
Further, a constant
kernel size of 7 × 7 over all the encoder and decoder layers is
chosen to provide a wide context for smooth labelling i.e.
a pixel
in the deepest layer feature map (layer 4) can be traced back to a
context window in the input image of 106×106 pixels.
This small
size of SegNet-Basic allows us to explore many different variants
(decoders) and train them in reasonable time.
Similarly we create
FCN-Basic, a comparable version of FCN for our analysis which
1.
SegNet-Basic was earlier termed SegNet in a archival version of this paper
[12]
shares the same encoder network as SegNet-Basic but with the
FCN decoding technique (see Fig.
3) used in all its decoders.
On the left in Fig.
3 is the decoding technique used by SegNet
(also SegNet-Basic), where there is no learning involved in the
upsampling step.
However, the upsampled maps are convolved
with trainable multi-channel decoder ﬁlters to densify its sparse
inputs.
Each decoder ﬁlter has the same number of channels as
the number of upsampled feature maps.
A smaller variant is one
where the decoder ﬁlters are single channel, i.e they only convolve
their corresponding upsampled feature map.
This variant (SegNet-
Basic-SingleChannelDecoder) reduces the number of trainable
parameters and inference time signiﬁcantly.
On the right in Fig.
3 is the FCN (also FCN-Basic) decoding
technique.
The important design element of the FCN model is
dimensionality reduction step of the encoder feature maps.
This
compresses the encoder feature maps which are then used in the
corresponding decoders.
Dimensionality reduction of the encoder
feature maps, say of 64 channels, is performed by convolving them
with 1 × 1 × 64 × K trainable ﬁlters, where K is the number of
classes.
The compressed K channel ﬁnal encoder layer feature
maps are the input to the decoder network.
In a decoder of this
network, upsampling is performed by inverse convolution using
a ﬁxed or trainable multi-channel upsampling kernel.
We set the
kernel size to 8 × 8.
This manner of upsampling is also termed as
deconvolution.
Note that, in comparison, SegNet the multi-channel
convolution using trainable decoder ﬁlters is performed after
upsampling to densifying feature maps.
The upsampled feature
map in FCN has K channels.
It is then added element-wise to
the corresponding resolution encoder feature map to produce the
output decoder feature map.
The upsampling kernels are initialized
using bilinear interpolation weights [2].
The FCN decoder model requires storing encoder feature maps
during inference.
This can be memory intensive for embedded
applications; for e.g.
storing 64 feature maps of the ﬁrst layer of
FCN-Basic at 180 × 240 resolution in 32 bit ﬂoating point preci-
sion takes 11MB.
This can be made smaller using dimensionality
reduction to the 11 feature maps which requires ≈1.9MB storage.
SegNet on the other hand requires almost negligible storage cost
for the pooling indices (.17MB if stored using 2 bits per 2 × 2
pooling window).
We can also create a variant of the FCN-Basic
model which discards the encoder feature map addition step and
only learns the upsampling kernels (FCN-Basic-NoAddition).
In addition to the above variants, we study upsampling using
ﬁxed bilinear interpolation weights which therefore requires no
learning for upsampling (Bilinear-Interpolation).
At the other
extreme, we can add 64 encoder feature maps at each layer to
the corresponding output feature maps from the SegNet decoder to
create a more memory intensive variant of SegNet (SegNet-Basic-
EncoderAddition).
Here both the pooling indices for upsampling
are used, followed by a convolution step to densify its sparse input.
This is then added element-wise to the corresponding encoder
feature maps to produce a decoders output.
Another and more memory intensive FCN-Basic variant
(FCN-Basic-NoDimReduction) is where there is no dimension-
ality reduction performed for the encoder feature maps.
This
implies that unlike FCN-Basic the ﬁnal encoder feature map is
not compressed to K channels before passing it to the decoder
network.
Therefore, the number of channels at the end of each
decoder is the same as the corresponding encoder (i.e 64).
We also tried other generic variants where feature maps are
simply upsampled by replication [7], or by using a ﬁxed (and
6
𝑎 𝑏 
𝑐 𝑑 
𝑎 
0 
0 
0 
0 
0 
0 
𝑑 
0 
0 0 𝑏 
0 
0 
0 
𝑐 
Max-pooling 
Indices 
 SegNet  
Deconvolution 
for upsampling 
Encoder feature map 
+ 
𝑦1 
 
𝑦3 
 
𝑦2 
 
𝑦4 
 
𝑦5 
 
𝑦8 
 
𝑦6 
 
𝑦7 
 
𝑦13 
 
𝑦14 
 
𝑦15 
 
𝑦16 
 
 FCN 
𝑦12 
 
𝑦10 
 
𝑦11 
 
𝑦9 
 
𝑎 𝑏 
𝑐 𝑑 
𝑥1 
 
𝑥3 
 
𝑥2 
 
𝑥4 
 
𝑥5 
 
𝑥8 
 
𝑥6 
 
𝑥7 
 
𝑥12 
 
𝑥9 
 
𝑥10 
 
𝑥11 
 
𝑥14 
 
𝑥13 
 
𝑥15 
 
𝑥16 
 
Convolution with trainable decoder filters 
Dimensionality reduction 
Fig.
3.
An illustration of SegNet and FCN [2] decoders.
a, b, c, d correspond to values in a feature map.
SegNet uses the max pooling indices
to upsample (without learning) the feature map(s) and convolves with a trainable decoder ﬁlter bank.
FCN upsamples by learning to deconvolve
the input feature map and adds the corresponding encoder feature map to produce the decoder output.
This feature map is the output of the
max-pooling layer (includes sub-sampling) in the corresponding encoder.
Note that there are no trainable decoder ﬁlters in FCN.
Median frequency balancing
Natural frequency balancing
Storage
Infer
Test
Train
Test
Train
Variant
Params (M) multiplier time (ms)
G
C
mIoU BF
G
C
mIoU
G
C mIoU BF
G
C mIoU
Fixed upsampling
Bilinear-Interpolation
0.625
0
24.2
77.9 61.1 43.3 20.83 89.1 90.2 82.7 82.7 52.5 43.8 23.08 93.5 74.1 59.9
Upsampling using max-pooling indices
SegNet-Basic
1.425
1
52.6
82.7 62.0 47.7 35.78 94.7 96.
2 92.7 84.0 54.6 46.3 36.67 96.1 83.9 73.3
SegNet-Basic-EncoderAddition
1.425
64
53.0
83.4 63.6
48.5 35.92 94.3 95.8 92.0 84.2 56.5 47.7 36.27 95.3 80.9 68.9
SegNet-Basic-SingleChannelDecoder
0.625
1
33.1
81.2 60.7 46.1 31.62 93.2 94.8 90.3 83.5 53.9 45.2 32.45 92.6 68.4 52.8
Learning to upsample (bilinear initialisation)
FCN-Basic
0.65
11
24.2
81.7 62.4 47.3 38.11 92.8 93.6 88.1 83.9 55.6 45.0 37.33 92.0 66.8 50.7
FCN-Basic-NoAddition
0.65
n/a
23.8
80.5 58.6 44.1 31.96 92.5 93.0 87.2 82.3 53.9 44.2 29.43 93.1 72.8 57.6
FCN-Basic-NoDimReduction
1.625
64
44.8
84.1 63.4 50.1 37.37 95.1 96.5 93.2 83.5 57.3 47.0 37.13 97.2 91.7 84.8
FCN-Basic-NoAddition-NoDimReduction
1.625
0
43.9
80.5 61.6 45.9 30.47 92.5 94.6 89.9 83.7 54.8 45.5 33.17 95.0 80.2 67.8
TABLE 1
Comparison of decoder variants.
We quantify the performance using global (G), class average (C), mean of intersection over union (mIoU) and a
semantic contour measure (BF).
The testing and training accuracies are shown as percentages for both natural frequency and median frequency
balanced training loss function.
SegNet-Basic performs at the same level as FCN-Basic but requires only storing max-pooling indices and is
therefore more memory efﬁcient during inference.
Note that the theoretical memory requirement reported is based only on the size of the ﬁrst layer
encoder feature map.
FCN-Basic, SegNet-Basic, SegNet-Basic-EncoderAddition all have high BF scores indicating the need to use information in
encoder feature maps for better class contour delineation.
Networks with larger decoders and those using the encoder feature maps in full perform
best, although they are least efﬁcient in terms of inference time and memory.
sparse) array of indices for upsampling.
These performed quite
poorly in comparison to the above variants.
A variant without
max-pooling and sub-sampling in the encoder network (decoders
are redundant) consumes more memory, takes longer to converge
and performs poorly.
Finally, please note that to encourage repro-
duction of our results we release the Caffe implementation of all
the variants 2.
3.2
Training
We use the CamVid road scenes dataset to benchmark the perfor-
mance of the decoder variants.
This dataset is small, consisting of
367 training and 233 testing RGB images (day and dusk scenes) at
360×480 resolution.
The challenge is to segment 11 classes such
as road, building, cars, pedestrians, signs, poles, side-walk etc.
We
perform local contrast normalization [54] to the RGB input.
The encoder and decoder weights were all initialized using the
technique described in He et al.
[55].
To train all the variants we
use stochastic gradient descent (SGD) with a ﬁxed learning rate
of 0.1 and momentum of 0.9 [17] using our Caffe implementation
of SegNet-Basic [56].
We train the variants until the training loss
2.
See http://mi.eng.cam.ac.uk/projects/segnet/ for our SegNet code and web
demo.
converges.
Before each epoch, the training set is shufﬂed and each
mini-batch (12 images) is then picked in order thus ensuring that
each image is used only once in an epoch.
We select the model
which performs highest on a validation dataset.
We use the cross-entropy loss [2] as the objective function for
training the network.
The loss is summed up over all the pixels
in a mini-batch.
When there is large variation in the number of
pixels in each class in the training set (e.g road, sky and building
pixels dominate the CamVid dataset) then there is a need to weight
the loss differently based on the true class.
This is termed class
balancing.
We use median frequency balancing [13] where the
weight assigned to a class in the loss function is the ratio of the
median of class frequencies computed on the entire training set
divided by the class frequency.
This implies that larger classes in
the training set have a weight smaller than 1 and the weights
of the smallest classes are the highest.
We also experimented
with training the different variants without class balancing or
equivalently using natural frequency balancing.
3.3
Analysis
To compare the quantitative performance of the different decoder
variants, we use three commonly used performance measures:
global accuracy (G) which measures the percentage of pixels
7
correctly classiﬁed in the dataset, class average accuracy (C) is
the mean of the predictive accuracy over all classes and mean
intersection over union (mIoU) over all classes as used in the Pas-
cal VOC12 challenge [21].
The mIoU metric is a more stringent
metric than class average accuracy since it penalizes false positive
predictions.
However, mIoU metric is not optimized for directly
through the class balanced cross-entropy loss.
The mIoU metric otherwise known as the Jacard Index is most
commonly used in benchmarking.
However, Csurka et al.
[57] note
that this metric does not always correspond to human qualitative
judgements (ranks) of good quality segmentation.
They show with
examples that mIoU favours region smoothness and does not
evaluate boundary accuracy, a point also alluded to recently by the
authors of FCN [58].
Hence they propose to complement the mIoU
metric with a boundary measure based on the Berkeley contour
matching score commonly used to evaluate unsupervised image
segmentation quality [59].
Csurka et al.
[57] simply extend this
to semantic segmentation and show that the measure of semantic
contour accuracy used in conjunction with the mIoU metric agrees
more with human ranking of segmentation outputs.
The key idea in computing a semantic contour score is to eval-
uate the F1-measure [59] which involves computing the precision
and recall values between the predicted and ground truth class
boundary given a pixel tolerance distance.
We used a value of
0.75% of the image diagonal as the tolerance distance.
The F1-
measure for each class that is present in the ground truth test image
is averaged to produce an image F1-measure.
Then we compute
the whole test set average, denoted the boundary F1-measure (BF)
by average the image F1 measures.
We test each architectural variant after each 1000 iterations of
optimization on the CamVid validation set until the training loss
converges.
With a training mini-batch size of 12 this corresponds
to testing approximately every 33 epochs (passes) through the
training set.
We select the iteration wherein the global accuracy
is highest amongst the evaluations on the validation set.
We report
all the three measures of performance at this point on the held-out
CamVid test set.
Although we use class balancing while training
the variants, it is still important to achieve high global accuracy to
result in an overall smooth segmentation.
Another reason is that
the contribution of segmentation towards autonomous driving is
mainly for delineating classes such as roads, buildings, side-walk,
sky.
These classes dominate the majority of the pixels in an image
and a high global accuracy corresponds to good segmentation
of these important classes.
We also observed that reporting the
numerical performance when class average is highest can often
correspond to low global accuracy indicating a perceptually noisy
segmentation output.
In Table 1 we report the numerical results of our analysis.
We also show the size of the trainable parameters and the highest
resolution feature map or pooling indices storage memory, i.e, of
the ﬁrst layer feature maps after max-pooling and sub-sampling.
We show the average time for one forward pass with our Caffe
implementation, averaged over 50 measurements using a 360 ×
480 input on an NVIDIA Titan GPU with cuDNN v3 acceleration.
We note that the upsampling layers in the SegNet variants are
not optimised using cuDNN acceleration.
We show the results
for both testing and training for all the variants at the selected
iteration.
The results are also tabulated without class balancing
(natural frequency) for training and testing accuracies.
Below we
analyse the results with class balancing.
From the Table 1, we see that bilinear interpolation based
upsampling without any learning performs the worst based on
all the measures of accuracy.
All the other methods which either
use learning for upsampling (FCN-Basic and variants) or learning
decoder ﬁlters after upsampling (SegNet-Basic and its variants)
perform signiﬁcantly better.
This emphasizes the need to learn
decoders for segmentation.
This is also supported by experimental
evidence gathered by other authors when comparing FCN with
SegNet-type decoding techniques [4].
When we compare SegNet-Basic and FCN-Basic we see that
both perform equally well on this test over all the measures of
accuracy.
The difference is that SegNet uses less memory during
inference since it only stores max-pooling indices.
On the other
hand FCN-Basic stores encoder feature maps in full which con-
sumes much more memory (11 times more).
SegNet-Basic has a
decoder with 64 feature maps in each decoder layer.
In comparison
FCN-Basic, which uses dimensionality reduction, has fewer (11)
feature maps in each decoder layer.
This reduces the number of
convolutions in the decoder network and hence FCN-Basic is
faster during inference (forward pass).
From another perspective,
the decoder network in SegNet-Basic makes it overall a larger
network than FCN-Basic.
This endows it with more ﬂexibility
and hence achieves higher training accuracy than FCN-Basic for
the same number of iterations.
Overall we see that SegNet-Basic
has an advantage over FCN-Basic when inference time memory
is constrained but where inference time can be compromised to
some extent.
SegNet-Basic is most similar to FCN-Basic-NoAddition in
terms of their decoders, although the decoder of SegNet is larger.
Both learn to produce dense feature maps, either directly by
learning to perform deconvolution as in FCN-Basic-NoAddition
or by ﬁrst upsampling and then convolving with trained decoder
ﬁlters.
The performance of SegNet-Basic is superior, in part due
to its larger decoder size.
The accuracy of FCN-Basic-NoAddition
is also lower as compared to FCN-Basic.
This shows that it is vital
to capture the information present in the encoder feature maps
for better performance.
In particular, note the large drop in the
BF measure between these two variants.
This can also explain
the part of the reason why SegNet-Basic outperforms FCN-Basic-
NoAddition.
The size of the FCN-Basic-NoAddition-NoDimReduction
model is slightly larger than SegNet-Basic since the ﬁnal encoder
feature maps are not compressed to match the number of classes
K.
This makes it a fair comparison in terms of the size of
the model.
The performance of this FCN variant is poorer than
SegNet-Basic in test but also its training accuracy is lower for the
same number of training epochs.
This shows that using a larger
decoder is not enough but it is also important to capture encoder
feature map information to learn better, particular the ﬁne grained
contour information (notice the drop in the BF measure).
Here
it is also interesting to see that SegNet-Basic has a competitive
training accuracy when compared to larger models such FCN-
Basic-NoDimReduction.
Another
interesting
comparison
between
FCN-Basic-
NoAddition and SegNet-Basic-SingleChannelDecoder shows that
using max-pooling indices for upsampling and an overall larger
decoder leads to better performance.
This also lends evidence to
SegNet being a good architecture for segmentation, particularly
when there is a need to ﬁnd a compromise between storage
cost, accuracy versus inference time.
In the best case, when both
memory and inference time is not constrained, larger models such
as FCN-Basic-NoDimReduction and SegNet-EncoderAddition are
8
both more accurate than the other variants.
Particularly, discarding
dimensionality reduction in the FCN-Basic model leads to the
best performance amongst the FCN-Basic variants with a high BF
score.
This once again emphasizes the trade-off involved between
memory and accuracy in segmentation architectures.
The last two columns of Table 1 show the result when
no class balancing is used (natural frequency).
Here, we can
observe that without weighting the results are poorer for all
the variants, particularly for class average accuracy and mIoU
metric.
The global accuracy is the highest without weighting
since the majority of the scene is dominated by sky, road and
building pixels.
Apart from this all the inference from the com-
parative analysis of variants holds true for natural frequency
balancing too, including the trends for the BF measure.
SegNet-
Basic performs as well as FCN-Basic and is better than the
larger FCN-Basic-NoAddition-NoDimReduction.
The bigger but
less efﬁcient models FCN-Basic-NoDimReduction and SegNet-
EncoderAddition perform better than the other variants.
We can now summarize the above analysis with the following
general points.
1)
The best performance is achieved when encoder feature
maps are stored in full.
This is reﬂected in the semantic
contour delineation metric (BF) most clearly.
2)
When memory during inference is constrained, then com-
pressed forms of encoder feature maps (dimensionality
reduction, max-pooling indices) can be stored and used
with an appropriate decoder (e.g.
SegNet type) to improve
performance.
3)
Larger decoders increase performance for a given encoder
network.
4
BENCHMARKING
We quantify the performance of SegNet on two scene segmenta-
tion benchmarks using our Caffe implementation 3.
The ﬁrst task
is road scene segmentation which is of current practical interest
for various autonomous driving related problems.
The second
task is indoor scene segmentation which is of immediate interest
to several augmented reality (AR) applications.
The input RGB
images for both tasks were 360 × 480.
We benchmarked SegNet against several other well adopted
deep architectures for segmentation such as FCN [2], DeepLab-
LargFOV [3] and DeconvNet [4].
Our objective was to understand
the performance of these architectures when trained end-to-end
on the same datasets.
To enable end-to-end training we added
batch normalization [51] layers after each convolutional layer.
For
DeepLab-LargeFOV, we changed the max pooling 3 stride to 1 to
achieve a ﬁnal predictive resolution of 45 × 60.
We restricted the
feature size in the fully connnected layers of DeconvNet to 1024
so as to enable training with the same batch size as other models.
Here note that the authors of DeepLab-LargeFOV [3] have also
reported little loss in performance by reducing the size of the fully
connected layers.
In order to perform a controlled benchmark we used the same
SGD solver [17] with a ﬁxed learning rate of 10−3 and momentum
of 0.9.
The optimization was performed for more than 100
epochs through the dataset until no further performance increase
was observed.
Dropout of 0.5 was added to the end of deeper
3.
Our web demo and Caffe implementation is available for evaluation at
http://mi.eng.cam.ac.uk/projects/segnet/
convolutional layers in all models to prevent overﬁtting (see
http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html for example
caffe prototxt).
For the road scenes which have 11 classes we
used a mini-batch size of 5 and for indoor scenes with 37 classes
we used a mini-batch size of 4.
4.1
Road Scene Segmentation
A number of road scene datasets are available for semantic parsing
[22], [26], [60], [61].
Of these we choose to benchmark SegNet
using the CamVid dataset [22] as it contains video sequences.
This
enables us to compare our proposed architecture with those which
use motion and structure [28], [29], [30] and video segments [33].
We also combine [22], [26], [60], [61] to form an ensemble of
3433 images to train SegNet for an additional benchmark.
For a
web demo (see footnote 3) of road scene segmentation, we include
the CamVid test set to this larger dataset.
Here, we would like to
note that another recent and independent segmentation benchmark
on road scenes has been performed for SegNet and the other
competing architectures used in this paper [62].
However, the
benchmark was not controlled, meaning that each architecture was
trained with a separate recipe with varying input resolutions and
sometimes with a validation set included.
Therefore, we believe
our more controlled benchmark can be used to complement their
efforts.
The qualitative comparisons of SegNet predictions with other
deep architectures can be seen in Fig.
4.
The qualitative results
show the ability of the proposed architecture to segment smaller
classes in road scenes while producing a smooth segmentation of
the overall scene.
Indeed, under the controlled benchmark setting,
SegNet shows superior performance as compared to some of the
larger models.
DeepLab-LargeFOV is the most efﬁcient model
and with CRF post-processing can produce competitive results
although smaller classes are lost.
FCN with learnt deconvolution
is clearly better than with ﬁxed bilinear upsampling.
DeconvNet is
the largest model and the most inefﬁcient to train.
Its predictions
do not retain small classes.
We also use this benchmark to ﬁrst compare SegNet with sev-
eral non deep-learning methods including Random Forests [27],
Boosting [27], [29] in combination with CRF based methods [30].
This was done to give the user a perspective of the improvements
in accuracy that has been achieved using deep networks compared
to classical feature engineering based techniques.
The results in Table 2 show SegNet-Basic, SegNet obtain
competitive results when compared with methods which use
CRFs.
This shows the ability of the deep architecture to extract
meaningful features from the input image and map it to accurate
and smooth class segment labels.
The most interesting result
here is the large performance improvement in class average and
mIOU metrics that is obtained when a large training dataset,
obtained by combining [22], [26], [60], [61], is used to train
SegNet.
Correspondingly, the qualitative results of SegNet (see
Fig.
4) are clearly superior to the rest of the methods.
It is able
to segment both small and large classes well.
We remark here
that we used median frequency class balancing [50] in training
SegNet-Basic and SegNet.
In addition, there is an overall smooth
quality of segmentation much like what is typically obtained with
CRF post-processing.
Although the fact that results improve with
larger training sets is not surprising, the percentage improvement
obtained using pre-trained encoder network and this training set
indicates that this architecture can potentially be deployed for
9
Test samples 
Ground Truth 
SegNet 
            
DeepLab-LargeFOV 
  
DeepLab-LargeFOV- 
denseCRF 
FCN 
FCN (learn deconv) 
DeconvNet 
Fig.
4.
Results on CamVid day and dusk test samples.
SegNet shows superior performance, particularly with its ability to delineate boundaries, as
compared to some of the larger models when all are trained in a controlled setting.
DeepLab-LargeFOV is the most efﬁcient model and with CRF
post-processing can produce competitive results although smaller classes are lost.
FCN with learnt deconvolution is clearly better.
DeconvNet is the
largest model with the longest training time, but its predictions loose small classes.
Note that these results correspond to the model corresponding
to the highest mIoU accuracy in Table 3.
practical applications.
Our random testing on urban and highway
images from the internet (see Fig.
1) demonstrates that SegNet can
absorb a large training set and generalize well to unseen images.
It
also indicates the contribution of the prior (CRF) can be lessened
when sufﬁcient amount of training data is made available.
In Table 3 we compare SegNet’s performance with now widely
adopted fully convolutional architectures for segmentation.
As
compared to the experiment in Table 2, we did not use any
class blancing for training any of the deep architectures including
SegNet.
This is because we found it difﬁcult to train larger
models such as DeconvNet with median frequency balancing.
We
benchmark performance at 40K, 80K and >80K iterations which
given the mini-batch size and training set size approximately
corresponds to 50, 100 and >100 epochs.
For the last test point
we also report the maximum number of iterations (here atleast 150
epochs) beyond which we observed no accuracy improvements or
when over-ﬁtting set in.
We report the metrics at three stages
in the training phase to reveal how the metrics varied with
training time, particularly for larger networks.
This is important
to understand if additional training time is justiﬁed when set
against accuracy increases.
Note also that for each evaluation
we performed a complete run through the dataset to obtain
batch norm statistics and then evaluated the test model with this
statistic (see http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html
for code.).
These evaluations are expensive to perform on large
training sets and hence we only report metrics at three time points
in the training phase.
From Table 3 we immediately see that SegNet, DeconvNet
achieve the highest scores in all the metrics as compared to other
models.
DeconvNet has a higher boundary delineation accuracy
but SegNet is much more efﬁcient as compared to DeconvNet.
This can be seen from the compute statistics in Table 6.
FCN,
10
Method
Building
Tree
Sky
Car
Sign-Symbol
Road
Pedestrian
Fence
Column-Pole
Side-walk
Bicyclist
Class avg.
Global avg.
mIoU
BF
SfM+Appearance [28]
46.2
61.9
89.7
68.6
42.9
89.5
53.6
46.6
0.7
60.5
22.5
53.0
69.1
n/a∗
Boosting [29]
61.9
67.3
91.1
71.1
58.5
92.9
49.5
37.6
25.8
77.8
24.7
59.8
76.4
n/a∗
Dense Depth Maps [32]
85.3
57.3
95.4
69.2
46.5
98.5
23.8
44.3
22.0
38.1
28.7
55.4
82.1
n/a∗
Structured Random Forests [31]
n/a
51.4
72.5
n/a∗
Neural Decision Forests [64]
n/a
56.1
82.1
n/a∗
Local Label Descriptors [65]
80.7
61.5
88.8
16.4
n/a
98.0
1.09
0.05
4.13
12.4
0.07
36.3
73.6
n/a∗
Super Parsing [33]
87.0
67.1
96.9
62.7
30.1
95.9
14.7
17.9
1.7
70.0
19.4
51.2
83.3
n/a∗
SegNet (3.5K dataset training - 140K)
89.6
83.4
96.1
87.7
52.7
96.4
62.2
53.45
32.1
93.3
36.5
71.20
90.40
60.10
46.84
CRF based approaches
Boosting + pairwise CRF [29]
70.7
70.8
94.7
74.4
55.9
94.1
45.7
37.2
13.0
79.3
23.1
59.9
79.8
n/a∗
Boosting+Higher order [29]
84.5
72.6
97.5
72.7
34.1
95.3
34.2
45.7
8.1
77.6
28.5
59.2
83.8
n/a∗
Boosting+Detectors+CRF [30]
81.5
76.6
96.2
78.7
40.2
93.9
43.0
47.6
14.3
81.5
33.9
62.5
83.8
n/a∗
TABLE 2
Quantitative comparisons of SegNet with traditional methods on the CamVid 11 road class segmentation problem [22].
SegNet outperforms all the
other methods, including those using depth, video and/or CRF’s on the majority of classes.
In comparison with the CRF based methods SegNet
predictions are more accurate in 8 out of the 11 classes.
It also shows a good ≈10% improvement in class average accuracy when trained on a
large dataset of 3.5K images.
Particularly noteworthy are the signiﬁcant improvements in accuracy for the smaller/thinner classes.
* Note that we
could not access predictions for older methods for computing the mIoU, BF metrics.
DeconvNet which have fully connected layers (turned into con-
volutional layers) train much more slowly and have comparable
or higher forward-backward pass time with reference to SegNet.
Here we note also that over-ﬁtting was not an issue in training
these larger models, since at comparable iterations to SegNet their
metrics showed an increasing trend.
For the FCN model learning the deconvolutional layers as
opposed to ﬁxing them with bi-linear interpolation weights im-
proves performance particularly the BF score.
It also achieves
higher metrics in a far lesser time.
This fact agrees with our earlier
analysis in Sec.
3.3.
Surprisingly, DeepLab-LargeFOV which is trained to predict
labels at a resolution of 45×60 produces competitive performance
given that it is the smallest model in terms of parameterization
and also has the fastest training time as per Table 6.
However,
the boundary accuracy is poorer and this is shared by the other
architectures.
DeconvNet’s BF score is higher than the other
networks when trained for a very long time.
Given our analysis
in Sec.
3.3 and the fact that it shares a SegNet type architecture.
The impact of dense CRF [63] post-processing can be seen in
the last time point for DeepLab-LargeFOV-denseCRF.
Both global
and mIoU improve but class average diminshes.
However a large
improvement is obtained for the BF score.
Note here that the dense
CRF hyperparameters were obtained by an expensive grid-search
process on a subset of the training set since no validation set was
available.
4.2
SUN RGB-D Indoor Scenes
SUN RGB-D [23] is a very challenging and large dataset of indoor
scenes with 5285 training and 5050 testing images.
The images
are captured by different sensors and hence come in various reso-
lutions.
The task is to segment 37 indoor scene classes including
wall, ﬂoor, ceiling, table, chair, sofa etc.
This task is made hard
by the fact that object classes come in various shapes, sizes and in
different poses.
There are frequent partial occlusions since there
are typically many different classes present in each of the test
images.
These factors make this one of the hardest segmentation
challenges.
We only use the RGB modality for our training and
testing.
Using the depth modality would necessitate architectural
modiﬁcations/redesign [2].
Also the quality of depth images from
current cameras require careful post-processing to ﬁll-in missing
measurements.
They may also require using fusion of many frames
to robustly extract features for segmentation.
Therefore we believe
using depth for segmentation merits a separate body of work which
is not in the scope of this paper.
We also note that an earlier
benchmark dataset NYUv2 [25] is included as part of this dataset.
Road scene images have limited variation, both in terms of the
classes of interest and their spatial arrangements.
When captured
from a moving vehicle where the camera position is nearly always
parallel to the road surface limiting variability in view points.
This
makes it easier for deep networks to learn to segment them ro-
bustly.
In comparison, images of indoor scenes are more complex
since the view points can vary a lot and there is less regularity
in both the number of classes present in a scene and their spatial
arrangement.
Another difﬁculty is caused by the widely varying
sizes of the object classes in the scene.
Some test samples from the
recent SUN RGB-D dataset [23] are shown in Fig.
5.
We observe
some scenes with few large classes and some others with dense
clutter (bottom row and right).
The appearance (texture and shape)
can also widely vary in indoor scenes.
Therefore, we believe this is
the hardest challenge for segmentation architectures and methods
in computer vision.
Other challenges, such as Pascal VOC12 [21]
salient object segmentation have occupied researchers more [66],
but we believe indoor scene segmentation is more challenging and
has more current practical applications such as in AR and robotics.
To encourage more research in this direction we compared well
known deep architectures on the large SUN RGB-D dataset.
The qualitative results of SegNet on samples of indoor scenes
of different types such as bedroom, living room, laboratory,
meeting room, bathroom are shown in Fig.
5.
We see that SegNet
obtains reasonable predictions when the size of the classes are
large under different view points.
This is particularly interesting
since the input modality is only RGB.
RGB images are also
useful to segment thinner structures such as the legs of chairs
and tables, lamps which is difﬁcult to achieve using depth images
from currently available sensors.
This can be seen from the results
of SegNet, DeconvNet in Fig.
5.
It is also useful to segment
decorative objects such as paintings on the wall for AR tasks.
However as compared to outdoor scenes the segmentation quality
11
Network/Iterations
40K
80K
>80K
Max iter
G
C
mIoU
BF
G
C
mIoU
BF
G
C
mIoU
BF
SegNet
88.81 59.93 50.02 35.78
89.68 69.82 57.18 42.08
90.40 71.20 60.10 46.84
140K
DeepLab-LargeFOV [3]
85.95 60.41 50.18 26.25
87.76 62.57 53.34 32.04
88.20 62.53 53.88 32.77
140K
DeepLab-LargeFOV-denseCRF [3]
not computed
89.71 60.67 54.74 40.79
140K
FCN
81.97 54.38 46.59 22.86
82.71 56.22 47.95 24.76
83.27 59.56 49.83 27.99
200K
FCN (learnt deconv) [2]
83.21 56.05 48.68 27.40
83.71 59.64 50.80 31.01
83.14 64.21 51.96 33.18
160K
DeconvNet [4]
85.26 46.40 39.69 27.36
85.19 54.08 43.74 29.33
89.58 70.24 59.77 52.23
260K
TABLE 3
Quantitative comparison of deep networks for semantic segmentation on the CamVid test set when trained on a corpus of 3433 road scenes
without class balancing.
When end-to-end training is performed with the same and ﬁxed learning rate, smaller networks like SegNet learn to
perform better in a shorter time.
The BF score which measures the accuracy of inter-class boundary delineation is signiﬁcantly higher for SegNet,
DeconvNet as compared to other competing models.
DeconvNet matches the metrics for SegNet but at a much larger computational cost.
Also
see Table 2 for individual class accuracies for SegNet.
Network/Iterations
80K
140K
>140K
Max iter
G
C
mIoU
BF
G
C
mIoU
BF
G
C
mIoU
BF
SegNet
70.73 30.82 22.52 9.16
71.66 37.60 27.46 11.33
72.63 44.76 31.84 12.66
240K
DeepLab-LargeFOV [3]
70.70 41.75 30.67 7.28
71.16 42.71 31.29
7.57
71.90 42.21 32.08
8.26
240K
DeepLab-LargeFOV-denseCRF [3]
not computed
66.96 33.06 24.13
9.41
240K
FCN (learnt deconv) [2]
67.31 34.32 24.05 7.88
68.04 37.2
26.33
9.0
68.18 38.41 27.39
9.68
200K
DeconvNet [4]
59.62 12.93
8.35
6.50
63.28 22.53 15.14
7.86
66.13 32.28 22.57 10.47
380K
TABLE 4
Quantitative comparison of deep architectures on the SUNRGB-D dataset when trained on a corpus of 5250 indoor scenes.
Note that only the
RGB modality was used in these experiments.
In this complex task with 37 classes all the architectures perform poorly, particularly because of the
smaller sized classes and skew in the class distribution.
DeepLab-Large FOV, the smallest and most efﬁcient model has a slightly higher mIoU but
SegNet has a better G,C,BF score.
Also note that when SegNet was trained with median frequency class balancing it obtained 71.75, 44.85,
32.08, 14.06 (180K) as the metrics.
is clearly more noisy.
The quality drops signiﬁcantly when clutter
is increased (see the result sample in the middle column).
The quantitative results in Table 4 show that all the deep
architectures share low mIoU and boundary metrics.
The global
and class averages (correlates well with mIou) are also small.
SegNet outperforms all other methods in terms of G,C, BF metrics
and has a slightly lower mIoU than DeepLab-LargeFOV.
As a
stand alone experiment we trained SegNet with median frequency
class balancing [67] and the metrics were higher (see Table 4)
and this agrees with our analysis in Sec.
3.3.
Interestingly, using
the grid search based optimal hyperparameters for the dense-CRF
worsened all except the BF score metric for DeepLab-LargeFOV-
denseCRF.
More optimal settings could perhaps be found but the
grid search process was too expensive given the large inference
time for dense-CRFs.
One reason for the overall poor performance is the large num-
ber of classes in this segmentation task, many of which occupy
a small part of the image and appear infrequently.
The accuracies
reported in Table 5 clearly show that larger classes have reasonable
accuracy and smaller classes have lower accuracies.
This can be
improved with larger sized datasets and class distribution aware
training techniques.
Another reason for poor performance could
lie in the inability of these deep architectures (all are based on
the VGG architecture [6]) to large variability in indoor scenes .
This conjecture on our part is based on the fact that the smallest
model DeepLab-LargeFOV produces the best accuracy in terms of
mIoU and in comparison, larger parameterizations in DeconvNet,
FCN did not improve perfomance even with much longer training
(DeconvNet).
This suggests there could lie a common reason
for poor performance across all architectures.
More controlled
datasets [68] are needed to verify this hypothesis.
5
DISCUSSION AND FUTURE WORK
Deep learning models have often achieved increasing success due
to the availability of massive datasets and expanding model depth
and parameterisation.
However, in practice factors like memory
and computational time during training and testing are important
factors to consider when choosing a model from a large bank
of models.
Training time becomes an important consideration
particularly when the performance gain is not commensurate with
increased training time as shown in our experiments.
Test time
memory and computational load are important to deploy models
on specialised embedded devices, for example, in AR applications.
From an overall efﬁciency viewpoint, we feel less attention has
been paid to smaller and more memory, time efﬁcient models for
real-time applications such as road scene understanding and AR.
This was the primary motivation behind the proposal of SegNet,
which is signiﬁcantly smaller and faster than other competing
architectures, but which we have shown to be efﬁcient for tasks
such as road scene understanding.
Segmentation challenges such as Pascal [21] and MS-COCO
[42] are object segmentation challenges wherein a few classes are
present in any test image.
Scene segmentation is more challenging
due to the high variability of indoor scenes and a need to segment
a larger number of classes simultaneously.
The task of outdoor and
indoor scene segmentation are also more practically oriented with
current applications such as autonomous driving, robotics and AR.
The metrics we chose to benchmark various deep segmentation
architectures like the boundary F1-measure (BF) was done to
complement the existing metrics which are more biased towards
region accuracies.
It is clear from our experiments and other inde-
pendent benchmarks [62] that outdoor scene images captured from
a moving car are easier to segment and deep architectures perform
robustly.
We hope our experiments will encourage researchers to
engage their attention towards the more challenging indoor scene
12
Test samples 
Ground Truth 
SegNet 
            
DeepLab-LargeFOV 
  
DeepLab-LargeFOV- 
denseCRF 
FCN (learnt deconv) 
DeconvNet 
Fig.
5.
Qualitative assessment of SegNet predictions on RGB indoor test scenes from the recently released SUN RGB-D dataset [23].
In this hard
challenge, SegNet predictions delineate inter class boundaries well for object classes in a variety of scenes and their view-points.
Overall rhe
segmentation quality is better when object classes are reasonably sized but is very noisy when the scene is more cluttered.
Note that often parts of
an image of a scene do not have ground truth labels and these are shown in black colour.
These parts are not masked in the corresponding deep
model predictions that are shown.
Note that these results correspond to the model corresponding to the highest mIoU accuracy in Table 4.
segmentation task.
An important choice we had to make when benchmarking
different deep architectures of varying parameterization was the
manner in which to train them.
Many of these architectures have
used a host of supporting techniques and multi-stage training
recipes to arrive at high accuracies on datasets but this makes
it difﬁcult to gather evidence about their true performance under
time and memory constraints.
Instead we chose to perform a
controlled benchmarking where we used batch normalization to
enable end-to-end training with the same solver (SGD).
However,
we note that this approach cannot entirely disentangle the effects
of model versus solver (optimization) in achieving a particular
result.
This is mainly due to the fact that training these networks
involves gradient back-propagation which is imperfect and the
optimization is a non-convex problem in extremely large di-
mensions.
Acknowledging these shortcomings, our hope is that
this controlled analysis complements other benchmarks [62] and
reveals the practical trade-offs involved in different well known
architectures.
For the future, we would like to exploit our understanding of
segmentation architectures gathered from our analysis to design
more efﬁcient architectures for real-time applications.
We are also
interested in estimating the model uncertainty for predictions from
deep segmentation architectures [69], [70].
Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training
them by back-propagation through time [36, 26] can be difﬁcult [8].
For that reason, RNNs have
rarely been used for natural language processing tasks such as text classiﬁcation despite their powers
in representing sequential structures.
On a number of document classiﬁcation tasks, we ﬁnd that it is possible to train Long Short-Term
Memory recurrent networks (LSTM RNNs) [9] to achieve good performance with careful tuning of
hyperparameters.
Furthermore, a simple pretraining step can signiﬁcantly stabilize the training of
LSTMs.
For example, we can use a next step prediction model, i.e., a recurrent language model in
NLP, as an unsupervised method.
Another method is to use a sequence autoencoder, which uses a
RNN to read a long input sequence into a single vector.
This vector will then be used to reconstruct
the original sequence.
The weights obtained from these two pretraining methods can then be used
as an initialization for standard LSTM RNNs to improve training and generalization.
In our experiments on document classiﬁcation with 20 Newsgroups [16] and DBpedia [19], and
sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent lan-
guage models or sequence autoencoders are usually better than LSTMs initialized randomly.
This
pretraining helps LSTMs reach or surpass previous baselines on these datasets without additional
data.
Another important result from our experiments is that using more unlabeled data from related tasks
in the pretraining can improve the generalization of a subsequent supervised model.
For example,
using unlabeled data from Amazon reviews to pretrain the sequence autoencoders can improve clas-
siﬁcation accuracy on Rotten Tomatoes from 79.7% to 83.3%, an equivalence of adding substantially
more labeled data.
This evidence supports the thesis that it is possible to use unsupervised learning
with more unlabeled data to improve supervised learning.
With sequence autoencoders, and outside
unlabeled data, LSTMs are able to match or surpass previously reported results.
1
We believe our semi-supervised approach (as also argued by [1]) has some advantages over other
unsupervised sequence learning methods, e.g., Paragraph Vectors [18], because it can allow for easy
ﬁne-tuning.
Our semi-supervised learning approach is related to Skip-Thought vectors [13], with
two differences.
The ﬁrst difference is that Skip-Thought is a harder objective, because it predicts
adjacent sentences.
The second is that Skip-Thought is a pure unsupervised learning algorithm,
without ﬁne-tuning.
2
Sequence autoencoders and recurrent language models
Our approach to sequence autoencoding is inspired by the work in sequence to sequence learning
(also known as seq2seq) by Sutskever et al.
[31], which has been successfully used for machine
translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recogni-
tion [4, 3] and conversational modeling [27, 33].
Key to their approach is the use of a recurrent
network as an encoder to read in an input sequence into a hidden state, which is the input to a
decoder recurrent network that predicts the output sequence.
The sequence autoencoder is similar to the above concept, except that it is an unsupervised learning
model.
The objective is to reconstruct the input sequence itself.
That means we replace the output
sequence in the seq2seq framework with the input sequence.
In our sequence autoencoders, the
weights for the decoder network and the encoder network are the same (see Figure 1).
Figure 1: The sequence autoencoder for the sequence “WXYZ”.
The sequence autoencoder uses
a recurrent network to read the input sequence in to the hidden state, which can then be used to
reconstruct the original sequence.
We ﬁnd that the weights obtained from the sequence autoencoder can be used as an initialization
of another supervised network, one which tries to classify the sequence.
We hypothesize that this
is because the network can already memorize the input sequence.
This reason, and the fact that the
gradients have shortcuts, are our hypothesis of why the sequence autoencoder is a good and stable
approach in initializing recurrent networks.
A signiﬁcant property of the sequence autoencoder is that it is unsupervised, and thus can be trained
with large quantities of unlabeled data to improve its quality.
Our result is that additional unlabeled
data can improve the generalization ability of recurrent networks.
This is especially useful for tasks
that have limited labeled data.
We also ﬁnd that recurrent language models [23] can be used as a pretraining method for LSTMs.
This is equivalent to removing the encoder part of the sequence autoencoder in Figure 1.
Our exper-
imental results show that this approach works better than LSTMs with random initialization.
3
Overview of methods
In our experiments, we use LSTM recurrent networks [9] because they are generally better than
RNNs.
Our LSTM implementation is standard and has input, forget, and output gates [6, 7].
We
compare basic LSTMs against LSTMs initialized with the sequence autoencoder method.
When
LSTMs are initialized with a sequence autoencoder, the methods are called SA-LSTMs in our ex-
periments.
When LSTMs are initialized with a language model, the method is called LM-LSTMs.
In most of our experiments our output layer predicts the document label from the LSTM output
at the last timestep.
We also experiment with the approach of putting the label at every timestep
and linearly increasing the weights of the prediction objectives from 0 to 1 [24].
This way we can
inject gradients to earlier steps in the recurrent networks.
We call this approach linear label gain.
2
Lastly, we also experiment with the method of jointly training the supervised learning task with the
sequence autoencoder and call this method joint training.
4
Experiments
In our experiments with LSTMs, we follow the basic recipes as described in [31] by clipping the
cell outputs and gradients.
The benchmarks of focus are text understanding tasks, with all datasets
being publicly available.
The tasks are sentiment analysis (IMDB and Rotten Tomatoes) and text
classiﬁcation (20 Newsgroups and DBpedia).
Commonly used methods on these datasets, such as
bag-of-words or n-grams, typically ignore long-range ordering information (e.g., modiﬁers and their
objects may be separated by many unrelated words); so one would expect recurrent methods which
preserve ordering information to perform well.
Nevertheless, due to the difﬁculty in optimizing
these networks, recurrent models are not the method of choice for document classiﬁcation.
In our experiments with the sequence autoencoder, we train it to reproduce the full document after
reading all the input words.
In other words, we do not perform any truncation or windowing.
We
add an end of sentence marker to the end of each input sequence and train the network to start
reproducing the sequence after that marker.
To speed up performance and reduce GPU memory
usage, we perform truncated backpropagation up to 400 timesteps from the end of the sequence.
We
preprocess the text so that punctuation is treated as separate tokens and we ignore any non-English
characters and words in the DBpedia text.
We also remove words that only appear once in each
dataset and do not perform any term weighting or stemming.
After training the recurrent language model or the sequence autoencoder for roughly 500K steps
with a batch size of 128, we use both the word embedding parameters and the LSTM weights to
initialize the LSTM for the supervised task.
We then train on that task while ﬁne tuning both the
embedding parameters and the weights and use early stopping when the validation error starts to
increase.
We choose the dropout parameters based on a validation set.
Using SA-LSTMs, we are able to match or surpass reported results for all datasets.
It is important
to emphasize that previous best results come from various different methods.
So it is signiﬁcant
that one method achieves strong results for all datasets, presumably because such a method can be
used as a general model for any similar task.
A summary of results in the experiments are shown in
Table 1.
More details of the experiments are as follows.
Table 1: A summary of the error rates of SA-LSTMs and previous best reported results.
Dataset
SA-LSTM
Previous best result
IMDB
7.24%
7.42%
Rotten Tomatoes
16.7%
18.5%
20 Newsgroups
15.6%
17.1%
DBpedia
1.19%
1.74%
4.1
Sentiment analysis experiments with IMDB
In this ﬁrst set of experiments, we benchmark our methods on the IMDB movie sentiment dataset,
proposed by Maas et al.
[21].1 There are 25,000 labeled and 50,000 unlabeled documents in the
training set and 25,000 in the test set.
We use 15% of the labeled training documents as a validation
set.
The average length of each document is 241 words and the maximum length of a document is
2,526 words.
The previous baselines are bag-of-words, ConvNets [12] or Paragraph Vectors [18].
Since the documents are long, one might expect that it is difﬁcult for recurrent networks to learn.
We
however ﬁnd that with tuning, it is possible to train LSTM recurrent networks to ﬁt the training set.
For example, if we set the size of hidden state to be 512 units and truncate the backprop to be 400,
an LSTM can do fairly well.
With random embedding dimension dropout [37] and random word
dropout (not published previously), we are able to reach performance of around 86.5% accuracy in
the test set, which is approximately 5% worse than most baselines.
1http://ai.Stanford.edu/amaas/data/sentiment/index.html
3
Fundamentally, the main problem with this approach is that it is unstable: if we were to increase the
number of hidden units or to increase the number of backprop steps, the training breaks down very
quickly: the objective function explodes even with careful tuning of the gradient clipping.
This is
because LSTMs are sensitive to the hyperparameters for long documents.
In contrast, we ﬁnd that
the SA-LSTM works better and is more stable.
If we use the sequence autoencoders, changing the
size of the hidden state or the number of backprop steps hardly affects the training of LSTMs.
This
is important because the models become more practical to train.
Using sequence autoencoders, we overcome the optimization instability in LSTMs in such a way
that it is fast and easy to achieve perfect classiﬁcation on the training set.
To avoid overﬁtting, we
again use input dimension dropout, with the dropout rate chosen on a validation set.
We ﬁnd that
dropping out 80% of the input embedding dimensions works well for this dataset.
The results of
our experiments are shown in Table 2 together with previous baselines.
We also add an additional
baseline where we initialize a LSTM with word2vec embeddings on the training set.
Table 2: Performance of models on the IMDB sentiment classiﬁcation task.
Model
Test error rate
LSTM with tuning and dropout
13.50%
LSTM initialized with word2vec embeddings
10.00%
LM-LSTM (see Section 2)
7.64%
SA-LSTM (see Figure 1)
7.24%
SA-LSTM with linear gain (see Section 3)
9.17%
SA-LSTM with joint training (see Section 3)
14.70%
Full+Unlabeled+BoW [21]
11.11%
WRRBM + BoW (bnc) [21]
10.77%
NBSVM-bi (Na¨ıve Bayes SVM with bigrams) [35]
8.78%
seq2-bown-CNN (ConvNet with dynamic pooling) [11]
7.67%
Paragraph Vectors [18]
7.42%
The results conﬁrm that SA-LSTM with input embedding dropout can be as good as previous best re-
sults on this dataset.
In contrast, LSTMs without sequence autoencoders have trouble in optimizing
the objective because of long range dependencies in the documents.
Using language modeling (LM-LSTM) as an initialization works well, achieving 8.98%, but less
well compared to the SA-LSTM.
This is perhaps because language modeling is a short-term objec-
tive, so that the hidden state only captures the ability to predict the next few words.
In the above table, we use 1,024 units for memory cells, 512 units for the input embedding layer in
the LM-LSTM and SA-LSTM.
We also use a hidden layer 512 units with dropout of 50% between
the last hidden state and the classiﬁer.
We continue to use these settings in the following experiments
except with 30 units in the ﬁnal hidden layer.
In Table 3, we present some examples from the IMDB dataset that are correctly classiﬁed by SA-
LSTM but not by a bigram NBSVM model.
These examples often have long-term dependencies or
have sarcasm that is difﬁcult to ﬁnd by solely looking at short phrases.
4.2
Sentiment analysis experiments with Rotten Tomatoes and the positive effects of
additional unlabeled data
The success on the IMDB dataset convinces us to test our methods on another sentiment analysis
task to see if similar gains can be obtained.
The benchmark of focus in this experiment is the Rotten
Tomatoes dataset [25].2 The dataset has 10,662 documents, which are randomly split into 80% for
training, 10% for validation and 10% for test.
The average length of each document is 22 words and
the maximum length is 52 words.
Thus compared to IMDB, this dataset is smaller both in terms of
the number of documents and the number of words per document.
2http://www.cs.cornell.edu/people/pabo/movie-review-data/
4
Table 3: IMDB sentiment classiﬁcation examples that are correctly classiﬁed by SA-LSTM and
incorrectly by NBSVM-bi.
Text
Sentiment
This ﬁlm is not at all as bad as some people on here are saying.
I think it has got a
decent horror plot and the acting seem normal to me.
People are way over-exagerating
what was wrong with this.
It is simply classic horror, the type without a plot that we
have to think about forever and forever.
We can just sit back, relax, and be scared.
Positive
Looking for a REAL super bad movie?
If you wanna have great fun, don’t hesitate and
check this one!
Ferrigno is incredibly bad but is also the best of this mediocrity.
Negative
A professional production with quality actors that simply never touched the heart or the
funny bone no matter how hard it tried.
The quality cast, stark setting and excellent
cinemetography made you hope for Fargo or High Plains Drifter but sorry, the soup had
no seasoning...or meat for that matter.
A 3 (of 10) for effort.
Negative
The screen-play is very bad, but there are some action sequences that i really liked.
I
think the image is good, better than other romanian movies.
I liked also how the actors
did their jobs.
Negative
Our ﬁrst observation is that it is easier to train LSTMs on this dataset than on the IMDB dataset
and the gaps between LSTMs, LM-LSTMs and SA-LSTMs are smaller than before.
This is because
movie reviews in Rotten Tomatoes are sentences whereas reviews in IMDB are paragraphs.
As this dataset is small, our methods tend to severely overﬁt the training set.
Combining SA-LSTMs
with 95% input embedding and 50% word dropout improves generalization and allows the model to
achieve 20.3% test set error.
Further tuning the hyperparameters on the validation set yields 19.3%
test error.
To better the performance, we add unlabeled data from the IMDB dataset in the previous experiment
and Amazon movie reviews [22] to the autoencoder training stage.3 We also run a control experiment
where we use the pretrained word vectors trained by word2vec from Google News.
Table 4: Performance of models on the Rotten Tomatoes sentiment classiﬁcation task.
Model
Test error rate
LSTM with tuning and dropout
20.3%
LSTM with linear gain
21.1%
LM-LSTM
21.7%
SA-LSTM
20.3%
LSTM with word vectors from word2vec Google News
20.5%
SA-LSTM with unlabeled data from IMDB
18.6%
SA-LSTM with unlabeled data from Amazon reviews
16.7%
MV-RNN [28]
21.0%
NBSVM-bi [35]
20.6%
CNN-rand [12]
23.5%
CNN-non-static (ConvNet with vectors from word2vec Google News) [12]
18.5%
The results for this set of experiments are shown in Table 4.
Our observation is that if we use the word
vectors from word2vec, there is only a small gain of 0.5%.
This is perhaps because the recurrent
weights play an important role in our model and are not initialized properly in this experiment.
However, if we use IMDB to pretrain the sequence autoencoders, the error decreases from 20.5%
to 18.6%, nearly a 2% gain in accuracy; if we use Amazon reviews, a larger unlabeled dataset (7.9
3The dataset is available at http://snap.stanford.edu/data/web-Amazon.html, which has
34 million general product reviews, but we only use 7.9 million movie reviews in our experiments.
5
million movie reviews), to pretrain the sequence autoencoders, the error goes down to 16.7% which
is another 2% gain in accuracy.
This brings us to the question of how well this method of using unlabeled data fares compared to
adding more labeled data.
As argued by Socher et al.
[29], a reason of why the methods are not
perfect yet is the lack of labeled training data, they proposed to use more labeled data by labeling an
addition of 215,154 phrases created by the Stanford Parser.
The use of more labeled data allowed
their method to achieve around 15% error in the test set, an improvement of approximately 5% over
older methods with less labeled data.
We compare our method to their reported results [29] on sentence-level classiﬁcation.
As our method
does not have access to valuable labeled data, one might expect that our method is severely disad-
vantaged and should not perform on the same level.
However, with unlabeled data and sequence
autoencoders, we are able to obtain 16.7%, ranking second amongst many other methods that have
access to a much larger corpus of labeled data.
The fact that unlabeled data can compensate for the
lack of labeled data is very signiﬁcant as unlabeled data are much cheaper than labeled data.
The
results are shown in Table 5.
Table 5: More unlabeled data vs.
more labeled data.
Performance of SA-LSTM with additional
unlabeled data and previous models with additional labeled data on the Rotten Tomatoes task.
Model
Test error rate
LSTM initialized with word2vec embeddings trained on Amazon reviews
23.3%
SA-LSTM with unlabeled data from Amazon reviews
16.7%
NB [29]
18.2%
SVM [29]
20.6%
BiNB [29]
16.9%
VecAvg [29]
19.9%
RNN [29]
17.6%
MV-RNN [29]
17.1%
RNTN [29]
14.6%
4.3
Text classiﬁcation experiments with 20 newsgroups
The experiments so far have been done on datasets where the number of tokens in a document is
relatively small, a few hundred words.
Our question becomes whether it is possible to use SA-
LSTMs for tasks that have a substantial number of words.
For that purpose, we carry out the next
experiments on the 20 newsgroups dataset [16].4 There are 11,293 documents in the training set and
7,528 in the test set.
We use 15% of the training documents as a validation set.
Each document is an
email with an average length of 267 words and a maximum length of 11,925 words.
Attachments,
PGP keys, duplicates and empty messages are removed.
As the newsgroup documents are long,
it was previously considered improbable for recurrent networks to learn anything from the dataset.
The best methods are often simple bag-of-words.
We repeat the same experiments with LSTMs and SA-LSTMs on this dataset.
Similar to observa-
tions made in previous experiments, SA-LSTMs are generally more stable to train than LSTMs.
To
improve generalization of the models, we again use input embedding dropout and word dropout cho-
sen on the validation set.
With 70% input embedding dropout and 75% word dropout, SA-LSTM
achieves 15.6% test set error which is much better than previous classiﬁers in this dataset.
Results
are shown in Table 6.
4.4
Character-level document classiﬁcation experiments with DBpedia
In this set of experiments, we turn our attention to another challenging task of categorizing
Wikipedia pages by reading character-by-character inputs.
The dataset of attention is the DBpedia
4http://qwone.com/˜jason/20Newsgroups/
6
Table 6: Performance of models on the 20 newsgroups classiﬁcation task.
Model
Test error rate
LSTM
18.0%
LSTM with linear gain
71.6%
LM-LSTM
15.3%
SA-LSTM
15.6%
Hybrid Class RBM [17]
23.8%
RBM-MLP [5]
20.5%
SVM + Bag-of-words [2]
17.1%
Na¨ıve Bayes [2]
19.0%
dataset [19], which was also used to benchmark convolutional neural nets in Zhang and LeCun [38].
DBpedia had no duplication or tainting issues from the outset so we compare experimental results on
this dataset.
DBpedia is a crowd-sourced effort to extract information from Wikipedia and categorize
it into an ontology.
For this experiment, we follow the same procedure suggested in Zhang and LeCun [38].
The task is
to classify DBpedia abstracts into one of 14 categories after reading the character-by-character input.
The dataset is split into 560,000 training examples and 70,000 test examples.
A DBpedia document
has an average of 300 characters while the maximum length of all documents is 13,467 characters.
As this dataset is large, overﬁtting is not an issue and thus we do not perform any dropout on the
input or recurrent layers.
For this dataset, we use a two-layered LSTM, each layer has 512 hidden
units and and the input embedding has 128 units.
Table 7: Performance of models on the DBpedia character level classiﬁcation task.
Model
Test error rate
LSTM
13.64%
LSTM with linear gain
1.32%
LM-LSTM
1.50%
SA-LSTM
2.34%
SA-LSTM with linear gain
1.23%
SA-LSTM with 3 layers and linear gain
1.19%
SA-LSTM (word-level)
1.40%
Bag-of-words
3.57%
Small ConvNet
1.98%
Large ConvNet
1.73%
In this dataset, we ﬁnd that the linear label gain as described in Section 3 is an effective mechanism to
inject gradients to earlier steps in LSTMs.
This linear gain method works well and achieves 1.32%
test set error, which is better than SA-LSTM.
Combining SA-LSTM and the linear gain method
achieves 1.19% test set error, a signiﬁcant improvement from the results of convolutional networks
as shown in Table 7.
4.5
Object classiﬁcation experiments with CIFAR-10
In these experiments, we attempt to see if our pre-training methods extend to non-textual data.
To
do this, we train a LSTM on the CIFAR-10 image dataset, consisting of 60,000 32x32 colour images
divided into 10 classes.
The input at each timestep of the LSTM is an entire row of pixels and we
predict the class of the image after reading the ﬁnal row.
We use the same method as in [15] to
perform data augmentation.
We also trained a LSTM to do next row prediction given the current
row (we denote this as LM-LSTM) and a LSTM to autoencode the image by rows (SA-LSTM).
The
loss function during unsupervised learning is the Euclidean L2 distance between the predicted and
7
the target row.
We then ﬁne-tune these on the classiﬁcation task and present the classiﬁcation results
in Table 8.
While we do not achieve the results attained by state of the art convolutional networks,
our 2-layer pretrained LM-LSTM is able to exceed the results of the baseline convolutional DBN
model [14] despite not using any convolutions and outperforms the non pre-trained LSTM.
Table 8: Performance on the CIFAR-10 object classiﬁcation task with 50,000 train images.
Model (400 units)
Test error rate
1-layer LSTM
25.0%
1-layer LM-LSTM
23.1%
1-layer SA-LSTM
25.1%
2-layer LSTM
26.0%
2-layer LM-LSTM
18.0%
2-layer SA-LSTM
25.1%
Convolution DBNs [14]
21.1%
Recent breakthroughs in computer vision [17, 12] and natural language processing [7] are bringing
trained classiﬁers into the center of security-critical systems.
Important examples include vision
for autonomous cars, face recognition, and malware detection.
These developments make security
aspects of machine learning increasingly important.
In particular, resistance to adversarially chosen
inputs is becoming a crucial design goal.
While trained models tend to be very effective in classifying
benign inputs, recent work [2, 28, 22] shows that an adversary is often able to manipulate the input
so that the model produces an incorrect output.
This phenomenon has received particular attention in the context of deep neural networks,
and there is now a quickly growing body of work on this topic [11, 9, 27, 18, 23, 29].
Computer
vision presents a particularly striking challenge: very small changes to the input image can fool
∗Authors ordered alphabetically.
1Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://
github.com/MadryLab/cifar10_challenge.
1
arXiv:1706.06083v4  [stat.ML]  4 Sep 2019
state-of-the-art neural networks with high conﬁdence [28, 21].
This holds even when the benign
example was classiﬁed correctly, and the change is imperceptible to a human.
Apart from the
security implications, this phenomenon also demonstrates that our current models are not learning
the underlying concepts in a robust manner.
All these ﬁndings raise a fundamental question:
How can we train deep neural networks that are robust to adversarial inputs?
There is now a sizable body of work proposing various attack and defense mechanisms for the
adversarial setting.
Examples include defensive distillation [24, 6], feature squeezing [31, 14], and
several other adversarial example detection approaches [5].
These works constitute important ﬁrst
steps in exploring the realm of possibilities here.
They, however, do not offer a good understanding
of the guarantees they provide.
We can never be certain that a given attack ﬁnds the “most
adversarial” example in the context, or that a particular defense mechanism prevents the existence
of some well-deﬁned class of adversarial attacks.
This makes it difﬁcult to navigate the landscape
of adversarial robustness or to fully evaluate the possible security implications.
In this paper, we study the adversarial robustness of neural networks through the lens of robust
optimization.
We use a natural saddle point (min-max) formulation to capture the notion of security
against adversarial attacks in a principled manner.
This formulation allows us to be precise about
the type of security guarantee we would like to achieve, i.e., the broad class of attacks we want
to be resistant to (in contrast to defending only against speciﬁc known attacks).
The formulation
also enables us to cast both attacks and defenses into a common theoretical framework, naturally
encapsulating most prior work on adversarial examples.
In particular, adversarial training directly
corresponds to optimizing this saddle point problem.
Similarly, prior methods for attacking neural
networks correspond to speciﬁc algorithms for solving the underlying constrained optimization
problem.
Equipped with this perspective, we make the following contributions.
1.
We conduct a careful experimental study of the optimization landscape corresponding to this
saddle point formulation.
Despite the non-convexity and non-concavity of its constituent
parts, we ﬁnd that the underlying optimization problem is tractable after all.
In particular,
we provide strong evidence that ﬁrst-order methods can reliably solve this problem.
We
supplement these insights with ideas from real analysis to further motivate projected gradient
descent (PGD) as a universal “ﬁrst-order adversary”, i.e., the strongest attack utilizing the
local ﬁrst order information about the network.
2.
We explore the impact of network architecture on adversarial robustness and ﬁnd that model
capacity plays an important role here.
To reliably withstand strong adversarial attacks,
networks require a larger capacity than for correctly classifying benign examples only.
This
shows that a robust decision boundary of the saddle point problem can be signiﬁcantly more
complicated than a decision boundary that simply separates the benign data points.
3.
Building on the above insights, we train networks on MNIST [19] and CIFAR10 [16] that
are robust to a wide range of adversarial attacks.
Our approach is based on optimizing the
aforementioned saddle point formulation and uses PGD as a reliable ﬁrst-order adversary.
Our best MNIST model achieves an accuracy of more than 89% against the strongest adver-
saries in our test suite.
In particular, our MNIST network is even robust against white box
attacks of an iterative adversary.
Our CIFAR10 model achieves an accuracy of 46% against
2
the same adversary.
Furthermore, in case of the weaker black box/transfer attacks, our MNIST
and CIFAR10 networks achieve the accuracy of more than 95% and 64%, respectively.
(More
detailed overview can be found in Tables 1 and2.) To the best of our knowledge, we are the
ﬁrst to achieve these levels of robustness on MNIST and CIFAR10 against such a broad set of
attacks.
Overall, these ﬁndings suggest that secure neural networks are within reach.
In order to further
support this claim, we invite the community to attempt attacks against our MNIST and CIFAR10
networks in the form of a challenge.
This will let us evaluate its robustness more accurately,
and potentially lead to novel attack methods in the process.
The complete code, along with the
description of the challenge, is available at https://github.com/MadryLab/mnist_challenge and
https://github.com/MadryLab/cifar10_challenge.
2
An Optimization View on Adversarial Robustness
Much of our discussion will revolve around an optimization view of adversarial robustness.
This
perspective not only captures the phenomena we want to study in a precise manner, but will
also inform our investigations.
To this end, let us consider a standard classiﬁcation task with an
underlying data distribution D over pairs of examples x ∈Rd and corresponding labels y ∈[k].
We also assume that we are given a suitable loss function L(θ, x, y), for instance the cross-entropy
loss for a neural network.
As usual, θ ∈Rp is the set of model parameters.
Our goal then is to ﬁnd
model parameters θ that minimize the risk E(x,y)∼D[L(x, y, θ)].
Empirical risk minimization (ERM) has been tremendously successful as a recipe for ﬁnding
classiﬁers with small population risk.
Unfortunately, ERM often does not yield models that are
robust to adversarially crafted examples [2, 28].
Formally, there are efﬁcient algorithms (“adver-
saries”) that take an example x belonging to class c1 as input and ﬁnd examples xadv such that xadv
is very close to x but the model incorrectly classiﬁes xadv as belonging to class c2 ̸= c1.
In order to reliably train models that are robust to adversarial attacks, it is necessary to augment
the ERM paradigm appropriately.
Instead of resorting to methods that directly focus on improving
the robustness to speciﬁc attacks, our approach is to ﬁrst propose a concrete guarantee that an
adversarially robust model should satisfy.
We then adapt our training methods towards achieving
this guarantee.
The ﬁrst step towards such a guarantee is to specify an attack model, i.e., a precise deﬁnition
of the attacks our models should be resistant to.
For each data point x, we introduce a set of
allowed perturbations S ⊆Rd that formalizes the manipulative power of the adversary.
In image
classiﬁcation, we choose S so that it captures perceptual similarity between images.
For instance,
the ℓ∞-ball around x has recently been studied as a natural notion for adversarial perturbations [11].
While we focus on robustness against ℓ∞-bounded attacks in this paper, we remark that more
comprehensive notions of perceptual similarity are an important direction for future research.
Next, we modify the deﬁnition of population risk ED[L] by incorporating the above adversary.
Instead of feeding samples from the distribution D directly into the loss L, we allow the adversary
to perturb the input ﬁrst.
This gives rise to the following saddle point problem, which is our central
object of study:
min
θ
ρ(θ),
where
ρ(θ) = E(x,y)∼D

max
δ∈S L(θ, x + δ, y)

.
(2.1)
3
Formulations of this type (and their ﬁnite-sample counterparts) have a long history in robust
optimization, going back to Wald [30].
It turns out that this formulation is also particularly useful
in our context.
First, this formulation gives us a unifying perspective that encompasses much prior work on
adversarial robustness.
Our perspective stems from viewing the saddle point problem as the
composition of an inner maximization problem and an outer minimization problem.
Both of these
problems have a natural interpretation in our context.
The inner maximization problem aims to ﬁnd
an adversarial version of a given data point x that achieves a high loss.
This is precisely the problem
of attacking a given neural network.
On the other hand, the goal of the outer minimization problem
is to ﬁnd model parameters so that the “adversarial loss” given by the inner attack problem is
minimized.
This is precisely the problem of training a robust classiﬁer using adversarial training
techniques.
Second, the saddle point problem speciﬁes a clear goal that an ideal robust classiﬁer should
achieve, as well as a quantitative measure of its robustness.
In particular, when the parameters θ
yield a (nearly) vanishing risk, the corresponding model is perfectly robust to attacks speciﬁed by
our attack model.
Our paper investigates the structure of this saddle point problem in the context of deep neural
networks.
These investigations then lead us to training techniques that produce models with high
resistance to a wide range of adversarial attacks.
Before turning to our contributions, we brieﬂy
review prior work on adversarial examples and describe in more detail how it ﬁts into the above
formulation.
2.1
A Uniﬁed View on Attacks and Defenses
Prior work on adversarial examples has focused on two main questions:
1.
How can we produce strong adversarial examples, i.e., adversarial examples that fool a model
with high conﬁdence while requiring only a small perturbation?
2.
How can we train a model so that there are no adversarial examples, or at least so that an
adversary cannot ﬁnd them easily?
Our perspective on the saddle point problem (2.1) gives answers to both these questions.
On the
attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) [11]
and multiple variations of it [18].
FGSM is an attack for an ℓ∞-bounded adversary and computes
an adversarial example as
x + ε sgn(∇xL(θ, x, y)).
One can interpret this attack as a simple one-step scheme for maximizing the inner part of the
saddle point formulation.
A more powerful adversary is the multi-step variant, which is essentially
projected gradient descent (PGD) on the negative loss function
xt+1 = Πx+S
 xt + α sgn(∇xL(θ, x, y))

.
Other methods like FGSM with random perturbation have also been proposed [29].
Clearly, all
of these approaches can be viewed as speciﬁc attempts to solve the inner maximization problem
in (2.1).
4
On the defense side, the training dataset is often augmented with adversarial examples pro-
duced by FGSM.
This approach also directly follows from (2.1) when linearizing the inner maxi-
mization problem.
To solve the simpliﬁed robust optimization problem, we replace every training
example with its FGSM-perturbed counterpart.
More sophisticated defense mechanisms such as
training against multiple adversaries can be seen as better, more exhaustive approximations of the
inner maximization problem.
3
Towards Universally Robust Networks
Current work on adversarial examples usually focuses on speciﬁc defensive mechanisms, or on
attacks against such defenses.
An important feature of formulation (2.1) is that attaining small
adversarial loss gives a guarantee that no allowed attack will fool the network.
By deﬁnition, no
adversarial perturbations are possible because the loss is small for all perturbations allowed by our
attack model.
Hence, we now focus our attention on obtaining a good solution to (2.1).
Unfortunately, while the overall guarantee provided by the saddle point problem is evidently
useful, it is not clear whether we can actually ﬁnd a good solution in reasonable time.
Solving
the saddle point problem (2.1) involves tackling both a non-convex outer minimization problem
and a non-concave inner maximization problem.
One of our key contributions is demonstrating
that, in practice, one can solve the saddle point problem after all.
In particular, we now discuss
an experimental exploration of the structure given by the non-concave inner problem.
We argue
that the loss landscape corresponding to this problem has a surprisingly tractable structure of local
maxima.
This structure also points towards projected gradient descent as the “ultimate” ﬁrst-order
adversary.
Sections 4 and 5 then show that the resulting trained networks are indeed robust against
a wide range of attacks, provided the networks are sufﬁciently large.
3.1
The Landscape of Adversarial Examples
Recall that the inner problem corresponds to ﬁnding an adversarial example for a given network
and data point (subject to our attack model).
As this problem requires us to maximize a highly non-
concave function, one would expect it to be intractable.
Indeed, this is the conclusion reached by
prior work which then resorted to linearizing the inner maximization problem [15, 26].
As pointed
out above, this linearization approach yields well-known methods such as FGSM.
While training
against FGSM adversaries has shown some successes, recent work also highlights important
shortcomings of this one-step approach [29]—slightly more sophisticated adversaries can still ﬁnd
points of high loss.
To understand the inner problem in more detail, we investigate the landscape of local maxima
for multiple models on MNIST and CIFAR10.
The main tool in our experiments is projected
gradient descent (PGD), since it is the standard method for large-scale constrained optimization.
In order to explore a large part of the loss landscape, we re-start PGD from many points in the ℓ∞
balls around data points from the respective evaluation sets.
Surprisingly, our experiments show that the inner problem is tractable after all, at least from
the perspective of ﬁrst-order methods.
While there are many local maxima spread widely apart
within xi + S, they tend to have very well-concentrated loss values.
This echoes the folklore belief that
training neural networks is possible because the loss (as a function of model parameters) typically
has many local minima with very similar values.
5
Speciﬁcally, in our experiments we found the following phenomena:
• We observe that the loss achieved by the adversary increases in a fairly consistent way and
plateaus rapidly when performing projected ℓ∞gradient descent for randomly chosen starting
points inside x + S (see Figure 1).
0
25
50
75
100
Iterations
0
50
100
150
Loss value
0
25
50
75
100
Iterations
1
2
3
4
5
0
25
50
75
100
Iterations
0
20
40
60
80
0
25
50
75
100
Iterations
0.2
0.3
(a) MNIST
(b) MNIST
(c) CIFAR10
(d) CIFAR10
Standard training
Adversarial training
Natural training
Adversarial training
Figure 1: Cross-entropy loss values while creating an adversarial example from the MNIST and
CIFAR10 evaluation datasets.
The plots show how the loss evolves during 20 runs of projected
gradient descent (PGD).
Each run starts at a uniformly random point in the ℓ∞-ball around the
same natural example (additional plots for different examples appear in Figure 11).
The adversarial
loss plateaus after a small number of iterations.
The optimization trajectories and ﬁnal loss values
are also fairly clustered, especially on CIFAR10.
Moreover, the ﬁnal loss values on adversarially
trained networks are signiﬁcantly smaller than on their standard counterparts.
• Investigating the concentration of maxima further, we observe that over a large number of
random restarts, the loss of the ﬁnal iterate follows a well-concentrated distribution without
extreme outliers (see Figure 2; we veriﬁed this concentration based on 105 restarts).
• To demonstrate that maxima are noticeably distinct, we also measured the ℓ2 distance and angles
between all pairs of them and observed that distances are distributed close to the expected
distance between two random points in the ℓ∞ball, and angles are close to 90◦.
Along the line
segment between local maxima, the loss is convex, attaining its maximum at the endpoints and
is reduced by a constant factor in the middle.
Nevertheless, for the entire segment, the loss is
considerably higher than that of a random point.
• Finally, we observe that the distribution of maxima suggests that the recently developed subspace
view of adversarial examples is not fully capturing the richness of attacks [29].
In particular, we
observe adversarial perturbations with negative inner product with the gradient of the example,
and deteriorating overall correlation with the gradient direction as the scale of perturbation
increases.
All of this evidence points towards PGD being a “universal” adversary among ﬁrst-order
approaches, as we will see next.
6
MNIST
0
40
80
120
160
Loss value
log(frequency)
0
40
80
120
160
Loss value
0
40
80
120
160
Loss value
0
40
80
120
160
Loss value
0
40
80
120
160
Loss value
CIFAR10
0
25
50
75
100
Loss value
log(frequency)
0
25
50
75
100
Loss value
0
25
50
75
100
Loss value
0
25
50
75
100
Loss value
0
25
50
75
100
Loss value
Figure 2: Values of the local maxima given by the cross-entropy loss for ﬁve examples from the
MNIST and CIFAR10 evaluation datasets.
For each example, we start projected gradient descent
(PGD) from 105 uniformly random points in the ℓ∞-ball around the example and iterate PGD until
the loss plateaus.
The blue histogram corresponds to the loss on a standard network, while the red
histogram corresponds to the adversarially trained counterpart.
The loss is signiﬁcantly smaller for
the adversarially trained networks, and the ﬁnal loss values are very concentrated without any
outliers.
3.2
First-Order Adversaries
Our experiments show that the local maxima found by PGD all have similar loss values, both for
normally trained networks and adversarially trained networks.
This concentration phenomenon
suggests an intriguing view on the problem in which robustness against the PGD adversary yields
robustness against all ﬁrst-order adversaries, i.e., attacks that rely only on ﬁrst-order information.
As
long as the adversary only uses gradients of the loss function with respect to the input, we conjecture
that it will not ﬁnd signiﬁcantly better local maxima than PGD.
We give more experimental evidence
for this hypothesis in Section 5: if we train a network to be robust against PGD adversaries, it
becomes robust against a wide range of other attacks as well.
Of course, our exploration with PGD does not preclude the existence of some isolated maxima
with much larger function value.
However, our experiments suggest that such better local maxima
are hard to ﬁnd with ﬁrst order methods: even a large number of random restarts did not ﬁnd
function values with signiﬁcantly different loss values.
Incorporating the computational power of
the adversary into the attack model should be reminiscent of the notion of polynomially bounded
adversary that is a cornerstone of modern cryptography.
There, this classic attack model allows
the adversary to only solve problems that require at most polynomial computation time.
Here,
we employ an optimization-based view on the power of the adversary as it is more suitable in the
context of machine learning.
After all, we have not yet developed a thorough understanding of the
computational complexity of many recent machine learning problems.
However, the vast majority
of optimization problems in ML is solved with ﬁrst-order methods, and variants of SGD are the
7
most effective way of training deep learning models in particular.
Hence we believe that the class
of attacks relying on ﬁrst-order information is, in some sense, universal for the current practice of
deep learning.
Put together, these two ideas chart the way towards machine learning models with guaranteed
robustness.
If we train the network to be robust against PGD adversaries, it will be robust against a
wide range of attacks that encompasses all current approaches.
In fact, this robustness guarantee would become even stronger in the context of black-box attacks,
i.e., attacks in which the adversary does not have a direct access to the target network.
Instead,
the adversary only has less speciﬁc information such as the (rough) model architecture and the
training data set.
One can view this attack model as an example of “zero order” attacks, i.e., attacks
in which the adversary has no direct access to the classiﬁer and is only able to evaluate it on chosen
examples without gradient feedback.
We discuss transferability in Section B of the appendix.
We observe that increasing network
capacity and strengthening the adversary we train against (FGSM or PGD training, rather than
standard training) improves resistance against transfer attacks.
Also, as expected, the resistance of
our best models to such attacks tends to be signiﬁcantly larger than to the (strongest) ﬁrst order
attacks.
3.3
Descent Directions for Adversarial Training
The preceding discussion suggests that the inner optimization problem can be successfully solved
by applying PGD.
In order to train adversarially robust networks, we also need to solve the outer
optimization problem of the saddle point formulation (2.1), that is ﬁnd model parameters that
minimize the “adversarial loss”, the value of the inner maximization problem.
In the context of training neural networks, the main method for minimizing the loss function is
Stochastic Gradient Descent (SGD).
A natural way of computing the gradient of the outer problem,
∇θρ(θ), is computing the gradient of the loss function at a maximizer of the inner problem.
This
corresponds to replacing the input points by their corresponding adversarial perturbations and
normally training the network on the perturbed input.
A priori, it is not clear that this is a valid
descent direction for the saddle point problem.
However, for the case of continuously differentiable
functions, Danskin’s theorem—a classic theorem in optimization—states this is indeed true and
gradients at inner maximizers corresponds to descent directions for the saddle point problem.
Despite the fact that the exact assumptions of Danskin’s theorem do not hold for our problem
(the function is not continuously differentiable due to ReLU and max-pooling units, and we are
only computing approximate maximizers of the inner problem), our experiments suggest that we
can still use these gradients to optimize our problem.
By applying SGD using the gradient of the
loss at adversarial examples we can consistently reduce the loss of the saddle point problem during
training, as can be seen in Figure 5.
These observations suggest that we reliably optimize the saddle
point formulation (2.1) and thus train robust classiﬁers.
We formally state Danskin’s theorem and
describe how it applies to our problem in Section A of the Appendix.
4
Network Capacity and Adversarial Robustness
Solving the problem from Equation (2.1) successfully is not sufﬁcient to guarantee robust and
accurate classiﬁcation.
We need to also argue that the value of the problem (i.e.
the ﬁnal loss we
8
achieve against adversarial examples) is small, thus providing guarantees for the performance of
our classiﬁer.
In particular, achieving a very small value corresponds to a perfect classiﬁer, which is
robust to adversarial inputs.
For a ﬁxed set S of possible perturbations, the value of the problem is entirely dependent on the
architecture of the classiﬁer we are learning.
Consequently, the architectural capacity of the model
becomes a major factor affecting its overall performance.
At a high level, classifying examples in a
robust way requires a stronger classiﬁer, since the presence of adversarial examples changes the
decision boundary of the problem to a more complicated one (see Figure 3 for an illustration).
Figure 3: A conceptual illustration of standard vs.
adversarial decision boundaries.
Left: A set of
points that can be easily separated with a simple (in this case, linear) decision boundary.
Middle:
The simple decision boundary does not separate the ℓ∞-balls (here, squares) around the data points.
Hence there are adversarial examples (the red stars) that will be misclassiﬁed.
Right: Separating
the ℓ∞-balls requires a signiﬁcantly more complicated decision boundary.
The resulting classiﬁer is
robust to adversarial examples with bounded ℓ∞-norm perturbations.
Our experiments verify that capacity is crucial for robustness, as well as for the ability to
successfully train against strong adversaries.
For the MNIST dataset, we consider a simple con-
volutional network and study how its behavior changes against different adversaries as we keep
doubling the size of network (i.e.
double the number of convolutional ﬁlters and the size of the fully
connected layer).
The initial network has a convolutional layer with 2 ﬁlters, followed by another
convolutional layer with 4 ﬁlters, and a fully connected hidden layer with 64 units.
Convolutional
layers are followed by 2 × 2 max-pooling layers and adversarial examples are constructed with
ε = 0.3.
The results are in Figure 4.
For the CIFAR10 dataset, we used a ResNet model [13].
We performed data augmentation using
random crops and ﬂips, as well as per image standarization.
To increase the capacity, we modiﬁed
the network incorporating wider layers by a factor of 10.
This results in a network with 5 residual
units with (16, 160, 320, 640) ﬁlters each.
This network can achieve an accuracy of 95.2% when
trained with natural examples.
Adversarial examples were constructed with ε = 8.
Results on
capacity experiments appear in Figure 4.
We observe the following phenomena:
Capacity alone helps.
We observe that increasing the capacity of the network when training
using only natural examples (apart from increasing accuracy on these examples) increases the
robustness against one-step perturbations.
This effect is greater when considering adversarial
9
examples with smaller ε.
FGSM adversaries don’t increase robustness (for large ε).
When training the network using
adversarial examples generated with the FGSM, we observe that the network overﬁts to these
adversarial examples.
This behavior is known as label leaking [18] and stems from the fact that
the adversary produces a very restricted set of adversarial examples that the network can overﬁt
to.
These networks have poor performance on natural examples and don’t exhibit any kind of
robustness against PGD adversaries.
For the case of smaller ε the loss is ofter linear enough in the
ℓ∞-ball around natural examples, that FGSM ﬁnds adversarial examples close to those found by
PGD thus being a reasonable adversary to train against.
Weak models may fail to learn non-trivial classiﬁers.
In the case of small capacity networks,
attempting to train against a strong adversary (PGD) prevents the network from learning anything
meaningful.
The network converges to always predicting a ﬁxed class, even though it could
converge to an accurate classiﬁer through standard training.
The small capacity of the network
forces the training procedure to sacriﬁce performance on natural examples in order to provide any
kind of robustness against adversarial inputs.
The value of the saddle point problem decreases as we increase the capacity.
Fixing an adver-
sary model, and training against it, the value of (2.1) drops as capacity increases, indicating the the
model can ﬁt the adversarial examples increasingly well.
More capacity and stronger adversaries decrease transferability.
Either increasing the capacity
of the network, or using a stronger method for the inner optimization problem reduces the effec-
tiveness of transferred adversarial inputs.
We validate this experimentally by observing that the
correlation between gradients from the source and the transfer network, becomes less signiﬁcant as
capacity increases.
We describe our experiments in Section B of the appendix.
5
Experiments: Adversarially Robust Deep Learning Models
Following the understanding of the problem we developed in previous sections, we can now apply
our proposed approach to train robust classiﬁers.
As our experiments so far demonstrated, we need
to focus on two key elements: a) train a sufﬁciently high capacity network, b) use the strongest
possible adversary.
For both MNIST and CIFAR10, the adversary of choice will be projected gradient descent (PGD)
starting from a random perturbation around the natural example.
This corresponds to our notion
of a "complete" ﬁrst-order adversary, an algorithm that can efﬁciently maximize the loss of an
example using only ﬁrst order information.
Since we are training the model for multiple epochs,
there is no beneﬁt from restarting PGD multiple times per batch—a new start will be chosen the
next time each example is encountered.
When training against that adversary, we observe a steady decrease in the training loss of
adversarial examples, illustrated in Figure 5.
This behavior indicates that we are indeed successfully
solving our original optimization problem during training.
10
MNIST
1
2
4
8
16
0
20
40
60
80
100
Capacity scale
Accuracy
1
2
4
8
16
0
20
40
60
80
100
Capacity scale
1
2
4
8
16
0
20
40
60
80
100
Capacity scale
1
2
4
8
16
0.01
0.1
1
Capacity scale
Average loss
Natural
FGSM
PGD
CIFAR10
Simple Wide
Natural 92.7% 95.2%
FGSM
27.5% 32.7%
PGD
0.8%
3.5%
Simple Wide
87.4% 90.3%
90.9% 95.1%
0.0%
0.0%
Simple Wide
79.4% 87.3%
51.7% 56.1%
43.7% 45.8%
Simple
Wide
0.00357 0.00371
0.0115 0.00557
1.11
0.0218
(a) Standard training
(b) FGSM training
(c) PGD training
(d) Training Loss
Figure 4: The effect of network capacity on the performance of the network.
We trained MNIST and
CIFAR10 networks of varying capacity on: (a) natural examples, (b) with FGSM-made adversarial
examples, (c) with PGD-made adversarial examples.
In the ﬁrst three plots/tables of each dataset,
we show how the standard and adversarial accuracy changes with respect to capacity for each
training regime.
In the ﬁnal plot/table, we show the value of the cross-entropy loss on the
adversarial examples the networks were trained on.
This corresponds to the value of our saddle
point formulation (2.1) for different sets of allowed perturbations.
We evaluate the trained models against a range of adversaries.
We illustrate our results in
Table 1 for MNIST and Table 2 for CIFAR10.
The adversaries we consider are:
• White-box attacks with PGD for a different number of of iterations and restarts, denoted by
source A.
• White-box attacks with PGD using the Carlini-Wagner (CW) loss function [6] (directly opti-
mizing the difference between correct and incorrect logits).
This is denoted as CW, where the
corresponding attack with a high conﬁdence parameter (κ = 50) is denoted as CW+.
• Black-box attacks from an independently trained copy of the network, denoted A’.
• Black-box attacks from a version of the same network trained only on natural examples,
denoted Anat.
• Black-box attacks from a different convolution architecture, denoted B, described in Tramer
et al.
2017 [29].
MNIST.
We run 40 iterations of projected gradient descent as our adversary, with a step size
of 0.01 (we choose to take gradient steps in the ℓ∞-norm, i.e.
adding the sign of the gradient,
since this makes the choice of the step size simpler).
We train and evaluate against perturbations
of size ε = 0.3.
We use a network consisting of two convolutional layers with 32 and 64 ﬁlters
respectively, each followed by 2 × 2 max-pooling, and a fully connected layer of size 1024.
When
11
0k
25k
50k
75k 100k
Iterations
0.10
1.00
Loss value
0k
25k
50k
75k
Iterations
0.01
0.10
1.00
Loss value
(a) MNIST
(b) CIFAR10
Figure 5: Cross-entropy loss on adversarial examples during training.
The plots show how the
adversarial loss on training examples evolves during training the MNIST and CIFAR10 networks
against a PGD adversary.
The sharp drops in the CIFAR10 plot correspond to decreases in training
step size.
These plots illustrate that we can consistently reduce the value of the inner problem of
the saddle point formulation (2.1), thus producing an increasingly robust classiﬁer.
trained with natural examples, this network reaches 99.2% accuracy on the evaluation set.
However,
when evaluating on examples perturbed with FGSM the accuracy drops to 6.4%.
The resulting
adversarial accuracies are reported in Table 1.
Given that the resulting MNIST model is very robust
to ℓ∞-bounded adversaries, we investigated the learned parameters in order to understand how
they affect adversarial robustness.
The results of the investigation are presented in Appendix C.
In
particular, we found that the ﬁrst convolutional layer of the network is learning to threshold input
pixels while other weights tend to be sparser.
CIFAR10.
For the CIFAR10 dataset, we use the two architectures described in 4 (the original
ResNet and its 10× wider variant).
We trained the network against a PGD adversary with ℓ∞
projected gradient descent again, this time using 7 steps of size 2, and a total ε = 8.
For our hardest
adversary we chose 20 steps with the same settings, since other hyperparameter choices didn’t
offer a signiﬁcant decrease in accuracy.
The results of our experiments appear in Table 2.
The adversarial robustness of our network is signiﬁcant, given the power of iterative adversaries,
but still far from satisfactory.
We believe that these results can be improved by further pushing
along these directions, and training networks of larger capacity.
Resistance for different values of ε and ℓ2-bounded attacks.
In order to perform a broader
evaluation of the adversarial robustness of our models, we run two additional experiments.
On one
hand, we investigate the resistance to ℓ∞-bounded attacks for different values of ε.
On the other
hand, we examine the resistance of our model to attacks that are bounded in ℓ2-norm as opposed
to ℓ∞-norm.
In the case of ℓ2-bounded PGD we take steps in the gradient direction (not the sign of
it) and normalize the steps to be of ﬁxed norm to facilitate step size tuning.
For all PGD attacks, we
use 100 steps and set the step size to be 2.5 · ε/100 to ensure that we can reach the boundary of the
ε-ball from any starting point within it (and still allow for movement on the boundary).
Note that
the models were training against ℓ∞-bounded attacks with the original value of ε = 0.3, for MNIST,
and ε = 8 for CIFAR10.
The results appear in Figure 6.
12
Method
Steps
Restarts Source
Accuracy
Natural
-
-
-
98.8%
FGSM
-
-
A
95.6%
PGD
40
1
A
93.2%
PGD
100
1
A
91.8%
PGD
40
20
A
90.4%
PGD
100
20
A
89.3%
Targeted
40
1
A
92.7%
CW
40
1
A
94.0%
CW+
40
1
A
93.9%
FGSM
-
-
A’
96.8%
PGD
40
1
A’
96.0%
PGD
100
20
A’
95.7%
CW
40
1
A’
97.0%
CW+
40
1
A’
96.4%
FGSM
-
-
B
95.4%
PGD
40
1
B
96.4%
CW+
-
-
B
95.7%
Table 1: MNIST: Performance of the adversarially trained network against different adversaries
for ε = 0.3.
For each model of attack we show the most successful attack with bold.
The source
networks used for the attack are: the network itself (A) (white-box attack), an indepentenly
initialized and trained copy of the network (A’), architecture B from [29] (B).
We observe that for smaller ε than the one used during training the models achieve equal or
higher accuracy, as expected.
For MNIST, we notice a large drop in robustness for slightly large ε
values, potentially due to the fact that the threshold operators learned are tuned to the exact value
of ε used during training (Appendix C).
In contrast, the decay for the case of CIFAR10 is smoother.
For the case of ℓ2-bounded attacks on MNIST, we observe that PGD is unable to ﬁnd adversarial
examples even for quite large values of ε, e.g., ε = 4.5.
To put this value of ε into perspective, we
provide a sample of corresponding adversarial examples in Figure 12 of Appendix D.
We observe
that these perturbations are signiﬁcant enough that they would change the ground-truth label of
the images and it is thus unlikely that our models are actually that robust.
Indeed, subsequent
work [20, 25] has found that PGD is in fact overestimating the ℓ2-robustness of this model.
This
behavior is potentially due to the fact that the learned threshold ﬁlters (Appendix C) mask the
gradient, preventing PGD from maximizing the loss.
Attacking the model with a decision-based
attack [4] which does not rely on model gradients reveals that the model is signiﬁcantly more brittle
against ℓ2-bounded attacks.
Nevertheless, the ℓ∞-trained model is still more robust to ℓ2 attacks
compared to a standard model.
13
Method
Steps
Source
Accuracy
Natural
-
-
87.3%
FGSM
-
A
56.1%
PGD
7
A
50.0%
PGD
20
A
45.8%
CW
30
A
46.8%
FGSM
-
A’
67.0%
PGD
7
A’
64.2%
CW
30
A’
78.7%
FGSM
-
Anat
85.6%
PGD
7
Anat
86.0%
Table 2: CIFAR10: Performance of the adversarially trained network against different adversaries
for ε = 8.
For each model of attack we show the most effective attack in bold.
The source networks
considered for the attack are: the network itself (A) (white-box attack), an independtly initialized
and trained copy of the network (A’), a copy of the network trained on natural examples (Anat).
0
0.1
0.2
0.3
0.4
0
20
40
60
80
100
ε
Accuracy
0
1
2
3
4
5
6
0
20
40
60
80
100
ε
PGD adv.
trained
DBA adv.
trained
PGD standard
DBA standard
0
5
10 15 20 25 30
0
20
40
60
80
ε
0
20
40
60
80 100
0
20
40
60
80
ε
(a) MNIST, ℓ∞-norm
(b) MNIST, ℓ2-norm
(c) CIFAR10, ℓ∞-norm
(d) CIFAR10, ℓ2-norm
Figure 6: Performance of our adversarially trained networks against PGD adversaries of different
strength.
The MNIST and CIFAR10 networks were trained against ε = 0.3 and ε = 8 PGD ℓ∞
adversaries respectively (the training ε is denoted with a red dashed lines in the ℓ∞plots).
In the
case of the MNIST adversarially trained networks, we also evaluate the performance of the Decision
Boundary Attack (DBA) [4] with 2000 steps and PGD on standard and adversarially trained models.
We observe that for ε less or equal to the value used during training, the performance is equal or
better.
For MNIST there is a sharp drop shortly after.
Moreover, we observe that the performance
of PGD on the MNIST ℓ2-trained networks is poor and signiﬁcantly overestimates the robustness
of the model.
This is potentially due to the threshold ﬁlters learned by the model masking the loss
gradients (the decision-based attack does not utilize gradients).
14
6
Related Work
Due to the large body of work on adversarial examples we focus only on the most related papers
here.
Before we compare our contributions, we remark that robust optimization has been studied
outside deep learning for multiple decades (see [1] for an overview of this ﬁeld).
We also want to
note that the study of adversarial ML predates the widespread use of deep neural networks [8, 10]
(see [3] for an overview of earlier work).
Adversarial training was introduced in [11], however the adversary utilized was quite weak—it
relied on linearizing the loss around the data points.
As a result, while these models were robust
against this particular adversary, they were completely vulnerable to slightly more sophisticated
adversaries utilizing iterative attacks.
Recent work on adversarial training on ImageNet also observed that the model capacity is
important for adversarial training [18].
In contrast to this paper, we ﬁnd that training against
multi-step methods (PGD) does lead to resistance against such adversaries.
In [15] and [26] a version of the min-max optimization problem is also considered for adversarial
training.
There are, however, three important differences between the formerly mentioned result
and the present paper.
Firstly, the authors claim that the inner maximization problem can be difﬁcult
to solve, whereas we explore the loss surface in more detail and ﬁnd that randomly re-started
projected gradient descent often converges to solutions with comparable quality.
This shows that it
is possible to obtain sufﬁciently good solutions to the inner maximization problem, which offers
good evidence that deep neural network can be immunized against adversarial examples.
Secondly,
they consider only one-step adversaries, while we work with multi-step methods.
Additionally,
while the experiments in [26] produce promising results, they are only evaluated against FGSM.
However, FGSM-only evaluations are not fully reliable.
One evidence for that is that [26] reports
70% accuracy for ε = 0.7, but any adversary that is allowed to perturb each pixel by more than 0.5
can construct a uniformly gray image, thus fooling any classiﬁer.
A more recent paper [29] also explores the transferability phenomenon.
This exploration
focuses mostly on the region around natural examples where the loss is (close to) linear.
When
large perturbations are allowed, this region does not give a complete picture of the adversarial
landscape.
This is conﬁrmed by our experiments, as well as pointed out by [29].
A recommender system can be viewed as a search ranking
system, where the input query is a set of user and contextual
information, and the output is a ranked list of items.
Given
a query, the recommendation task is to ﬁnd the relevant
items in a database and then rank the items based on certain
objectives, such as clicks or purchases.
One challenge in recommender systems, similar to the gen-
eral search ranking problem, is to achieve both memorization
and generalization.
Memorization can be loosely deﬁned as
learning the frequent co-occurrence of items or features and
exploiting the correlation available in the historical data.
Generalization, on the other hand, is based on transitivity
of correlation and explores new feature combinations that
∗Corresponding author: hengtze@google.com
have never or rarely occurred in the past.
Recommenda-
tions based on memorization are usually more topical and
directly relevant to the items on which users have already
performed actions.
Compared with memorization, general-
ization tends to improve the diversity of the recommended
items.
In this paper, we focus on the apps recommendation
problem for the Google Play store, but the approach should
apply to generic recommender systems.
For massive-scale online recommendation and ranking sys-
tems in an industrial setting, generalized linear models such
as logistic regression are widely used because they are sim-
ple, scalable and interpretable.
The models are often trained
on binarized sparse features with one-hot encoding.
E.g., the
binary feature “user_installed_app=netflix” has value 1
if the user installed Netﬂix.
Memorization can be achieved
eﬀectively using cross-product transformations over sparse
features, such as AND(user_installed_app=netflix, impres-
sion_app=pandora”), whose value is 1 if the user installed
Netﬂix and then is later shown Pandora.
This explains how
the co-occurrence of a feature pair correlates with the target
label.
Generalization can be added by using features that are
less granular, such as AND(user_installed_category=video,
impression_category=music), but manual feature engineer-
ing is often required.
One limitation of cross-product trans-
formations is that they do not generalize to query-item fea-
ture pairs that have not appeared in the training data.
Embedding-based models, such as factorization machines
[5] or deep neural networks, can generalize to previously un-
seen query-item feature pairs by learning a low-dimensional
dense embedding vector for each query and item feature,
with less burden of feature engineering.
However, it is dif-
ﬁcult to learn eﬀective low-dimensional representations for
queries and items when the underlying query-item matrix is
sparse and high-rank, such as users with speciﬁc preferences
or niche items with a narrow appeal.
In such cases, there
should be no interactions between most query-item pairs,
but dense embeddings will lead to nonzero predictions for all
query-item pairs, and thus can over-generalize and make less
relevant recommendations.
On the other hand, linear mod-
els with cross-product feature transformations can memorize
these “exception rules” with much fewer parameters.
In this paper, we present the Wide & Deep learning frame-
work to achieve both memorization and generalization in one
model, by jointly training a linear model component and a
neural network component as shown in Figure 1.
The main contributions of the paper include:
• The Wide & Deep learning framework for jointly train-
ing feed-forward neural networks with embeddings and
arXiv:1606.07792v1  [cs.LG]  24 Jun 2016
Wide Models
Deep Models
Wide & Deep Models
Hidden Layers
Sparse Features
Output Units
Dense
Embeddings
Figure 1: The spectrum of Wide & Deep models.
linear model with feature transformations for generic
recommender systems with sparse inputs.
• The implementation and evaluation of the Wide &
Deep recommender system productionized on Google
Play, a mobile app store with over one billion active
users and over one million apps.
• We have open-sourced our implementation along with
a high-level API in TensorFlow1.
While the idea is simple, we show that the Wide & Deep
framework signiﬁcantly improves the app acquisition rate
on the mobile app store, while satisfying the training and
serving speed requirements.
2.
RECOMMENDER SYSTEM OVERVIEW
An overview of the app recommender system is shown
in Figure 2.
A query, which can include various user and
contextual features, is generated when a user visits the app
store.
The recommender system returns a list of apps (also
referred to as impressions) on which users can perform cer-
tain actions such as clicks or purchases.
These user actions,
along with the queries and impressions, are recorded in the
logs as the training data for the learner.
Since there are over a million apps in the database, it is
intractable to exhaustively score every app for every query
within the serving latency requirements (often O(10) mil-
liseconds).
Therefore, the ﬁrst step upon receiving a query
is retrieval.
The retrieval system returns a short list of items
that best match the query using various signals, usually a
combination of machine-learned models and human-deﬁned
rules.
After reducing the candidate pool, the ranking sys-
tem ranks all items by their scores.
The scores are usually
P(y|x), the probability of a user action label y given the
features x, including user features (e.g., country, language,
demographics), contextual features (e.g., device, hour of the
day, day of the week), and impression features (e.g., app age,
historical statistics of an app).
In this paper, we focus on the
ranking model using the Wide & Deep learning framework.
3.
WIDE & DEEP LEARNING
3.1
The Wide Component
The wide component is a generalized linear model of the
form y = wT x + b, as illustrated in Figure 1 (left).
y is the
prediction, x = [x1, x2, ..., xd] is a vector of d features, w =
[w1, w2, ..., wd] are the model parameters and b is the bias.
The feature set includes raw input features and transformed
1See Wide & Deep Tutorial on http://tensorflow.org.
Item 1
Item 2
Item 3
...
Database
Query
Items
Learner
Model
Ranked
O(10) items
Logs
User Actions
Retrieval
O(100) items
Ranking
Recommendation System
All items
Figure 2: Overview of the recommender system.
features.
One of the most important transformations is the
cross-product transformation, which is deﬁned as:
φk(x) =
d
Y
i=1
xcki
i
cki ∈{0, 1}
(1)
where cki is a boolean variable that is 1 if the i-th fea-
ture is part of the k-th transformation φk, and 0 otherwise.
For binary features, a cross-product transformation (e.g.,
“AND(gender=female, language=en)”) is 1 if and only if the
constituent features (“gender=female” and “language=en”)
are all 1, and 0 otherwise.
This captures the interactions
between the binary features, and adds nonlinearity to the
generalized linear model.
3.2
The Deep Component
The deep component is a feed-forward neural network, as
shown in Figure 1 (right).
For categorical features, the orig-
inal inputs are feature strings (e.g., “language=en”).
Each
of these sparse, high-dimensional categorical features are
ﬁrst converted into a low-dimensional and dense real-valued
vector, often referred to as an embedding vector.
The di-
mensionality of the embeddings are usually on the order of
O(10) to O(100).
The embedding vectors are initialized ran-
domly and then the values are trained to minimize the ﬁnal
loss function during model training.
These low-dimensional
dense embedding vectors are then fed into the hidden layers
of a neural network in the forward pass.
Speciﬁcally, each
hidden layer performs the following computation:
a(l+1) = f(W (l)a(l) + b(l))
(2)
where l is the layer number and f is the activation function,
often rectiﬁed linear units (ReLUs).
a(l), b(l), and W (l) are
the activations, bias, and model weights at l-th layer.
3.3
Joint Training of Wide & Deep Model
The wide component and deep component are combined
using a weighted sum of their output log odds as the pre-
User Data
App 
Impression 
Data
Training Data
Generation
Vocabulary 
Generator
Model 
Trainer
Model Verifier
Model Servers
Model Serving
Apps 
Recommendation 
Engine
Previous Models
    Data Generation
Model Training
Figure 3: Apps recommendation pipeline overview.
diction, which is then fed to one common logistic loss func-
tion for joint training.
Note that there is a distinction be-
tween joint training and ensemble.
In an ensemble, indi-
vidual models are trained separately without knowing each
other, and their predictions are combined only at inference
time but not at training time.
In contrast, joint training
optimizes all parameters simultaneously by taking both the
wide and deep part as well as the weights of their sum into
account at training time.
There are implications on model
size too: For an ensemble, since the training is disjoint, each
individual model size usually needs to be larger (e.g., with
more features and transformations) to achieve reasonable
accuracy for an ensemble to work.
In comparison, for joint
training the wide part only needs to complement the weak-
nesses of the deep part with a small number of cross-product
feature transformations, rather than a full-size wide model.
Joint training of a Wide & Deep Model is done by back-
propagating the gradients from the output to both the wide
and deep part of the model simultaneously using mini-batch
stochastic optimization.
In the experiments, we used Follow-
the-regularized-leader (FTRL) algorithm [3] with L1 regu-
larization as the optimizer for the wide part of the model,
and AdaGrad [1] for the deep part.
The combined model is illustrated in Figure 1 (center).
For a logistic regression problem, the model’s prediction is:
P(Y = 1|x) = σ(wT
wide[x, φ(x)] + wT
deepa(lf ) + b)
(3)
where Y is the binary class label, σ(·) is the sigmoid func-
tion, φ(x) are the cross product transformations of the orig-
inal features x, and b is the bias term.
wwide is the vector of
all wide model weights, and wdeep are the weights applied
on the ﬁnal activations a(lf ).
4.
SYSTEM IMPLEMENTATION
The implementation of the apps recommendation pipeline
consists of three stages: data generation, model training,
and model serving as shown in Figure 3.
4.1
Data Generation
In this stage, user and app impression data within a period
of time are used to generate training data.
Each example
corresponds to one impression.
The label is app acquisition:
1 if the impressed app was installed, and 0 otherwise.
Vocabularies, which are tables mapping categorical fea-
ture strings to integer IDs, are also generated in this stage.
The system computes the ID space for all the string features
that occurred more than a minimum number of times.
Con-
tinuous real-valued features are normalized to [0, 1] by map-
ping a feature value x to its cumulative distribution function
P(X ≤x), divided into nq quantiles.
The normalized value
is
i−1
nq−1 for values in the i-th quantiles.
Quantile boundaries
ReLU (1024)
Logistic Loss
Embeddings
ReLU (512)
ReLU (256)
User Installed 
App
Impression 
App
User 
Demographics
Device 
Class
...
Age
#App 
Installs
#Engagement 
sessions
...
Cross Product 
Transformation
Embeddings
Embeddings
Embeddings
Concatenated Embeddings (~1200 dimensions)
Continuous Features
Categorical Features
Figure 4: Wide & Deep model structure for apps
recommendation.
are computed during data generation.
4.2
Model Training
The model structure we used in the experiment is shown in
Figure 4.
During training, our input layer takes in training
data and vocabularies and generate sparse and dense fea-
tures together with a label.
The wide component consists
of the cross-product transformation of user installed apps
and impression apps.
For the deep part of the model, A 32-
dimensional embedding vector is learned for each categorical
feature.
We concatenate all the embeddings together with
the dense features, resulting in a dense vector of approxi-
mately 1200 dimensions.
The concatenated vector is then
fed into 3 ReLU layers, and ﬁnally the logistic output unit.
The Wide & Deep models are trained on over 500 billion
examples.
Every time a new set of training data arrives,
the model needs to be re-trained.
However, retraining from
scratch every time is computationally expensive and delays
the time from data arrival to serving an updated model.
To tackle this challenge, we implemented a warm-starting
system which initializes a new model with the embeddings
and the linear model weights from the previous model.
Before loading the models into the model servers, a dry
run of the model is done to make sure that it does not cause
problems in serving live traﬃc.
We empirically validate the
model quality against the previous model as a sanity check.
4.3
Model Serving
Once the model is trained and veriﬁed, we load it into the
model servers.
For each request, the servers receive a set
of app candidates from the app retrieval system and user
features to score each app.
Then, the apps are ranked from
the highest scores to the lowest, and we show the apps to
the users in this order.
The scores are calculated by running
a forward inference pass over the Wide & Deep model.
In order to serve each request on the order of 10 ms, we
optimized the performance using multithreading parallelism
by running smaller batches in parallel, instead of scoring all
candidate apps in a single batch inference step.
5.
EXPERIMENT RESULTS
To evaluate the eﬀectiveness of Wide & Deep learning in
a real-world recommender system, we ran live experiments
and evaluated the system in a couple of aspects: app acqui-
sitions and serving performance.
5.1
App Acquisitions
We conducted live online experiments in an A/B test-
ing framework for 3 weeks.
For the control group, 1% of
Table 1: Oﬄine & online metrics of diﬀerent models.
Online Acquisition Gain is relative to the control.
Model
Oﬄine AUC
Online Acquisition Gain
Wide (control)
0.726
0%
Deep
0.722
+2.9%
Wide & Deep
0.728
+3.9%
users were randomly selected and presented with recom-
mendations generated by the previous version of ranking
model, which is a highly-optimized wide-only logistic regres-
sion model with rich cross-product feature transformations.
For the experiment group, 1% of users were presented with
recommendations generated by the Wide & Deep model,
trained with the same set of features.
As shown in Table 1,
Wide & Deep model improved the app acquisition rate on
the main landing page of the app store by +3.9% relative to
the control group (statistically signiﬁcant).
The results were
also compared with another 1% group using only the deep
part of the model with the same features and neural network
structure, and the Wide & Deep mode had +1% gain on top
of the deep-only model (statistically signiﬁcant).
Besides online experiments, we also show the Area Under
Receiver Operator Characteristic Curve (AUC) on a holdout
set oﬄine.
While Wide & Deep has a slightly higher oﬄine
AUC, the impact is more signiﬁcant on online traﬃc.
One
possible reason is that the impressions and labels in oﬄine
data sets are ﬁxed, whereas the online system can generate
new exploratory recommendations by blending generaliza-
tion with memorization, and learn from new user responses.
5.2
Serving Performance
Serving with high throughput and low latency is challeng-
ing with the high level of traﬃc faced by our commercial
mobile app store.
At peak traﬃc, our recommender servers
score over 10 million apps per second.
With single threading,
scoring all candidates in a single batch takes 31 ms.
We im-
plemented multithreading and split each batch into smaller
sizes, which signiﬁcantly reduced the client-side latency to
14 ms (including serving overhead) as shown in Table 2.
6.
RELATED WORK
The idea of combining wide linear models with cross-
product feature transformations and deep neural networks
with dense embeddings is inspired by previous work, such as
factorization machines [5] which add generalization to linear
models by factorizing the interactions between two variables
as a dot product between two low-dimensional embedding
vectors.
In this paper, we expanded the model capacity by
learning highly nonlinear interactions between embeddings
via neural networks instead of dot products.
In language models, joint training of recurrent neural net-
works (RNNs) and maximum entropy models with n-gram
features has been proposed to signiﬁcantly reduce the RNN
complexity (e.g., hidden layer sizes) by learning direct weights
between inputs and outputs [4].
In computer vision, deep
residual learning [2] has been used to reduce the diﬃculty of
training deeper models and improve accuracy with shortcut
connections which skip one or more layers.
Joint training of
neural networks with graphical models has also been applied
to human pose estimation from images [6].
In this work we
explored the joint training of feed-forward neural networks
Table 2: Serving latency vs.
batch size and threads.
Batch size
Number of Threads
Serving Latency (ms)
200
1
31
100
2
17
50
4
14
and linear models, with direct connections between sparse
features and the output unit, for generic recommendation
and ranking problems with sparse input data.
In the recommender systems literature, collaborative deep
learning has been explored by coupling deep learning for
content information and collaborative ﬁltering (CF) for the
ratings matrix [7].
There has also been previous work on
mobile app recommender systems, such as AppJoy which
used CF on users’ app usage records [8].
Diﬀerent from the
CF-based or content-based approaches in the previous work,
we jointly train Wide & Deep models on user and impression
data for app recommender systems.
Convolutional neural networks have emerged as the mas-
ter algorithm in computer vision in recent years, and de-
veloping recipes for designing them has been a subject of
considerable attention.
The history of convolutional neural
network design started with LeNet-style models [10], which
were simple stacks of convolutions for feature extraction
and max-pooling operations for spatial sub-sampling.
In
2012, these ideas were reﬁned into the AlexNet architec-
ture [9], where convolution operations were being repeated
multiple times in-between max-pooling operations, allowing
the network to learn richer features at every spatial scale.
What followed was a trend to make this style of network
increasingly deeper, mostly driven by the yearly ILSVRC
competition; ﬁrst with Zeiler and Fergus in 2013 [25] and
then with the VGG architecture in 2014 [18].
At this point a new style of network emerged, the Incep-
tion architecture, introduced by Szegedy et al.
in 2014 [20]
as GoogLeNet (Inception V1), later reﬁned as Inception V2
[7], Inception V3 [21], and most recently Inception-ResNet
[19].
Inception itself was inspired by the earlier Network-
In-Network architecture [11].
Since its ﬁrst introduction,
Inception has been one of the best performing family of
models on the ImageNet dataset [14], as well as internal
datasets in use at Google, in particular JFT [5].
The fundamental building block of Inception-style mod-
els is the Inception module, of which several different ver-
sions exist.
In ﬁgure 1 we show the canonical form of an
Inception module, as found in the Inception V3 architec-
ture.
An Inception model can be understood as a stack of
such modules.
This is a departure from earlier VGG-style
networks which were stacks of simple convolution layers.
While Inception modules are conceptually similar to con-
volutions (they are convolutional feature extractors), they
empirically appear to be capable of learning richer repre-
sentations with less parameters.
How do they work, and
how do they differ from regular convolutions?
What design
strategies come after Inception?
1.1.
The Inception hypothesis
A convolution layer attempts to learn ﬁlters in a 3D space,
with 2 spatial dimensions (width and height) and a chan-
nel dimension; thus a single convolution kernel is tasked
with simultaneously mapping cross-channel correlations and
spatial correlations.
This idea behind the Inception module is to make this
process easier and more efﬁcient by explicitly factoring it
into a series of operations that would independently look at
cross-channel correlations and at spatial correlations.
More
precisely, the typical Inception module ﬁrst looks at cross-
channel correlations via a set of 1x1 convolutions, mapping
the input data into 3 or 4 separate spaces that are smaller than
the original input space, and then maps all correlations in
these smaller 3D spaces, via regular 3x3 or 5x5 convolutions.
This is illustrated in ﬁgure 1.
In effect, the fundamental hy-
pothesis behind Inception is that cross-channel correlations
and spatial correlations are sufﬁciently decoupled that it is
preferable not to map them jointly 1.
1A variant of the process is to independently look at width-wise corre-
arXiv:1610.02357v3  [cs.CV]  4 Apr 2017
Consider a simpliﬁed version of an Inception module that
only uses one size of convolution (e.g.
3x3) and does not
include an average pooling tower (ﬁgure 2).
This Incep-
tion module can be reformulated as a large 1x1 convolution
followed by spatial convolutions that would operate on non-
overlapping segments of the output channels (ﬁgure 3).
This
observation naturally raises the question: what is the ef-
fect of the number of segments in the partition (and their
size)?
Would it be reasonable to make a much stronger
hypothesis than the Inception hypothesis, and assume that
cross-channel correlations and spatial correlations can be
mapped completely separately?
Figure 1.
A canonical Inception module (Inception V3).
Figure 2.
A simpliﬁed Inception module.
1.2.
The continuum between convolutions and sep-
arable convolutions
An “extreme” version of an Inception module, based on
this stronger hypothesis, would ﬁrst use a 1x1 convolution to
map cross-channel correlations, and would then separately
map the spatial correlations of every output channel.
This
is shown in ﬁgure 4.
We remark that this extreme form of
an Inception module is almost identical to a depthwise sepa-
rable convolution, an operation that has been used in neural
lations and height-wise correlations.
This is implemented by some of the
modules found in Inception V3, which alternate 7x1 and 1x7 convolutions.
The use of such spatially separable convolutions has a long history in im-
age processing and has been used in some convolutional neural network
implementations since at least 2012 (possibly earlier).
Figure 3.
A strictly equivalent reformulation of the simpliﬁed In-
ception module.
Figure 4.
An “extreme” version of our Inception module, with one
spatial convolution per output channel of the 1x1 convolution.
network design as early as 2014 [15] and has become more
popular since its inclusion in the TensorFlow framework [1]
in 2016.
A depthwise separable convolution, commonly called
“separable convolution” in deep learning frameworks such as
TensorFlow and Keras, consists in a depthwise convolution,
i.e.
a spatial convolution performed independently over each
channel of an input, followed by a pointwise convolution,
i.e.
a 1x1 convolution, projecting the channels output by the
depthwise convolution onto a new channel space.
This is
not to be confused with a spatially separable convolution,
which is also commonly called “separable convolution” in
the image processing community.
Two minor differences between and “extreme” version of
an Inception module and a depthwise separable convolution
would be:
• The order of the operations: depthwise separable con-
volutions as usually implemented (e.g.
in TensorFlow)
perform ﬁrst channel-wise spatial convolution and then
perform 1x1 convolution, whereas Inception performs
the 1x1 convolution ﬁrst.
• The presence or absence of a non-linearity after the
ﬁrst operation.
In Inception, both operations are fol-
lowed by a ReLU non-linearity, however depthwise
separable convolutions are usually implemented with-
out non-linearities.
We argue that the ﬁrst difference is unimportant, in par-
ticular because these operations are meant to be used in a
stacked setting.
The second difference might matter, and we
investigate it in the experimental section (in particular see
ﬁgure 10).
We also note that other intermediate formulations of In-
ception modules that lie in between regular Inception mod-
ules and depthwise separable convolutions are also possible:
in effect, there is a discrete spectrum between regular convo-
lutions and depthwise separable convolutions, parametrized
by the number of independent channel-space segments used
for performing spatial convolutions.
A regular convolution
(preceded by a 1x1 convolution), at one extreme of this
spectrum, corresponds to the single-segment case; a depth-
wise separable convolution corresponds to the other extreme
where there is one segment per channel; Inception modules
lie in between, dividing a few hundreds of channels into 3
or 4 segments.
The properties of such intermediate modules
appear not to have been explored yet.
Having made these observations, we suggest that it may
be possible to improve upon the Inception family of archi-
tectures by replacing Inception modules with depthwise sep-
arable convolutions, i.e.
by building models that would be
stacks of depthwise separable convolutions.
This is made
practical by the efﬁcient depthwise convolution implementa-
tion available in TensorFlow.
In what follows, we present a
convolutional neural network architecture based on this idea,
with a similar number of parameters as Inception V3, and
we evaluate its performance against Inception V3 on two
large-scale image classiﬁcation task.
2.
Prior work
The present work relies heavily on prior efforts in the
following areas:
• Convolutional neural networks [10, 9, 25], in particular
the VGG-16 architecture [18], which is schematically
similar to our proposed architecture in a few respects.
• The Inception architecture family of convolutional neu-
ral networks [20, 7, 21, 19], which ﬁrst demonstrated
the advantages of factoring convolutions into multiple
branches operating successively on channels and then
on space.
• Depthwise separable convolutions, which our proposed
architecture is entirely based upon.
While the use of spa-
tially separable convolutions in neural networks has a
long history, going back to at least 2012 [12] (but likely
even earlier), the depthwise version is more recent.
Lau-
rent Sifre developed depthwise separable convolutions
during an internship at Google Brain in 2013, and used
them in AlexNet to obtain small gains in accuracy and
large gains in convergence speed, as well as a signiﬁcant
reduction in model size.
An overview of his work was
ﬁrst made public in a presentation at ICLR 2014 [23].
Detailed experimental results are reported in Sifre’s the-
sis, section 6.2 [15].
This initial work on depthwise sep-
arable convolutions was inspired by prior research from
Sifre and Mallat on transformation-invariant scattering
[16, 15].
Later, a depthwise separable convolution was
used as the ﬁrst layer of Inception V1 and Inception
V2 [20, 7].
Within Google, Andrew Howard [6] has
introduced efﬁcient mobile models called MobileNets
using depthwise separable convolutions.
Jin et al.
in
2014 [8] and Wang et al.
in 2016 [24] also did related
work aiming at reducing the size and computational
cost of convolutional neural networks using separable
convolutions.
Additionally, our work is only possible
due to the inclusion of an efﬁcient implementation of
depthwise separable convolutions in the TensorFlow
framework [1].
• Residual connections, introduced by He et al.
in [4],
which our proposed architecture uses extensively.
3.
The Xception architecture
We propose a convolutional neural network architecture
based entirely on depthwise separable convolution layers.
In effect, we make the following hypothesis: that the map-
ping of cross-channels correlations and spatial correlations
in the feature maps of convolutional neural networks can be
entirely decoupled.
Because this hypothesis is a stronger ver-
sion of the hypothesis underlying the Inception architecture,
we name our proposed architecture Xception, which stands
for “Extreme Inception”.
A complete description of the speciﬁcations of the net-
work is given in ﬁgure 5.
The Xception architecture has
36 convolutional layers forming the feature extraction base
of the network.
In our experimental evaluation we will ex-
clusively investigate image classiﬁcation and therefore our
convolutional base will be followed by a logistic regression
layer.
Optionally one may insert fully-connected layers be-
fore the logistic regression layer, which is explored in the
experimental evaluation section (in particular, see ﬁgures
7 and 8).
The 36 convolutional layers are structured into
14 modules, all of which have linear residual connections
around them, except for the ﬁrst and last modules.
In short, the Xception architecture is a linear stack of
depthwise separable convolution layers with residual con-
nections.
This makes the architecture very easy to deﬁne
and modify; it takes only 30 to 40 lines of code using a high-
level library such as Keras [2] or TensorFlow-Slim [17], not
unlike an architecture such as VGG-16 [18], but rather un-
like architectures such as Inception V2 or V3 which are far
more complex to deﬁne.
An open-source implementation of
Xception using Keras and TensorFlow is provided as part of
the Keras Applications module2, under the MIT license.
4.
Experimental evaluation
We choose to compare Xception to the Inception V3 ar-
chitecture, due to their similarity of scale: Xception and
Inception V3 have nearly the same number of parameters
(table 3), and thus any performance gap could not be at-
tributed to a difference in network capacity.
We conduct
our comparison on two image classiﬁcation tasks: one is
the well-known 1000-class single-label classiﬁcation task on
the ImageNet dataset [14], and the other is a 17,000-class
multi-label classiﬁcation task on the large-scale JFT dataset.
4.1.
The JFT dataset
JFT is an internal Google dataset for large-scale image
classiﬁcation dataset, ﬁrst introduced by Hinton et al.
in [5],
which comprises over 350 million high-resolution images
annotated with labels from a set of 17,000 classes.
To eval-
uate the performance of a model trained on JFT, we use an
auxiliary dataset, FastEval14k.
FastEval14k is a dataset of 14,000 images with dense
annotations from about 6,000 classes (36.5 labels per im-
age on average).
On this dataset we evaluate performance
using Mean Average Precision for top 100 predictions
(MAP@100), and we weight the contribution of each class
to MAP@100 with a score estimating how common (and
therefore important) the class is among social media images.
This evaluation procedure is meant to capture performance
on frequently occurring labels from social media, which is
crucial for production models at Google.
4.2.
Optimization conﬁguration
A different optimization conﬁguration was used for Ima-
geNet and JFT:
• On ImageNet:
– Optimizer: SGD
– Momentum: 0.9
– Initial learning rate: 0.045
– Learning rate decay: decay of rate 0.94 every 2
epochs
• On JFT:
– Optimizer: RMSprop [22]
– Momentum: 0.9
– Initial learning rate: 0.001
2https://keras.io/applications/#xception
– Learning rate decay: decay of rate 0.9 every
3,000,000 samples
For both datasets, the same exact same optimization con-
ﬁguration was used for both Xception and Inception V3.
Note that this conﬁguration was tuned for best performance
with Inception V3; we did not attempt to tune optimization
hyperparameters for Xception.
Since the networks have dif-
ferent training proﬁles (ﬁgure 6), this may be suboptimal, es-
pecially on the ImageNet dataset, on which the optimization
conﬁguration used had been carefully tuned for Inception
V3.
Additionally, all models were evaluated using Polyak
averaging [13] at inference time.
4.3.
Regularization conﬁguration
• Weight decay: The Inception V3 model uses a weight
decay (L2 regularization) rate of 4e −5, which has
been carefully tuned for performance on ImageNet.
We
found this rate to be quite suboptimal for Xception
and instead settled for 1e −5.
We did not perform
an extensive search for the optimal weight decay rate.
The same weight decay rates were used both for the
ImageNet experiments and the JFT experiments.
• Dropout: For the ImageNet experiments, both models
include a dropout layer of rate 0.5 before the logistic
regression layer.
For the JFT experiments, no dropout
was included due to the large size of the dataset which
made overﬁtting unlikely in any reasonable amount of
time.
• Auxiliary loss tower: The Inception V3 architecture
may optionally include an auxiliary tower which back-
propagates the classiﬁcation loss earlier in the network,
serving as an additional regularization mechanism.
For
simplicity, we choose not to include this auxiliary tower
in any of our models.
4.4.
Training infrastructure
All networks were implemented using the TensorFlow
framework [1] and trained on 60 NVIDIA K80 GPUs each.
For the ImageNet experiments, we used data parallelism
with synchronous gradient descent to achieve the best classi-
ﬁcation performance, while for JFT we used asynchronous
gradient descent so as to speed up training.
The ImageNet
experiments took approximately 3 days each, while the JFT
experiments took over one month each.
The JFT models
were not trained to full convergence, which would have
taken over three month per experiment.
Figure 5.
The Xception architecture: the data ﬁrst goes through the entry ﬂow, then through the middle ﬂow which is repeated eight times,
and ﬁnally through the exit ﬂow.
Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not
included in the diagram).
All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).
4.5.
Comparison with Inception V3
4.5.1
Classiﬁcation performance
All evaluations were run with a single crop of the inputs
images and a single model.
ImageNet results are reported
on the validation set rather than the test set (i.e.
on the
non-blacklisted images from the validation set of ILSVRC
2012).
JFT results are reported after 30 million iterations
(one month of training) rather than after full convergence.
Results are provided in table 1 and table 2, as well as ﬁgure
6, ﬁgure 7, ﬁgure 8.
On JFT, we tested both versions of our
networks that did not include any fully-connected layers, and
versions that included two fully-connected layers of 4096
units each before the logistic regression layer.
On ImageNet, Xception shows marginally better results
than Inception V3.
On JFT, Xception shows a 4.3% rel-
ative improvement on the FastEval14k MAP@100 metric.
We also note that Xception outperforms ImageNet results
reported by He et al.
for ResNet-50, ResNet-101 and ResNet-
152 [4].
Table 1.
Classiﬁcation performance comparison on ImageNet (sin-
gle crop, single model).
VGG-16 and ResNet-152 numbers are
only included as a reminder.
The version of Inception V3 being
benchmarked does not include the auxiliary tower.
Top-1 accuracy
Top-5 accuracy
VGG-16
0.715
0.901
ResNet-152
0.770
0.933
Inception V3
0.782
0.941
Xception
0.790
0.945
The Xception architecture shows a much larger perfor-
mance improvement on the JFT dataset compared to the
ImageNet dataset.
We believe this may be due to the fact
that Inception V3 was developed with a focus on ImageNet
and may thus be by design over-ﬁt to this speciﬁc task.
On
the other hand, neither architecture was tuned for JFT.
It is
likely that a search for better hyperparameters for Xception
on ImageNet (in particular optimization parameters and reg-
Table 2.
Classiﬁcation performance comparison on JFT (single
crop, single model).
FastEval14k MAP@100
Inception V3 - no FC layers
6.36
Xception - no FC layers
6.70
Inception V3 with FC layers
6.50
Xception with FC layers
6.78
Figure 6.
Training proﬁle on ImageNet
Figure 7.
Training proﬁle on JFT, without fully-connected layers
ularization parameters) would yield signiﬁcant additional
improvement.
4.5.2
Size and speed
Table 3.
Size and training speed comparison.
Parameter count
Steps/second
Inception V3
23,626,728
31
Xception
22,855,952
28
In table 3 we compare the size and speed of Inception
Figure 8.
Training proﬁle on JFT, with fully-connected layers
V3 and Xception.
Parameter count is reported on ImageNet
(1000 classes, no fully-connected layers) and the number of
training steps (gradient updates) per second is reported on
ImageNet with 60 K80 GPUs running synchronous gradient
descent.
Both architectures have approximately the same
size (within 3.5%), and Xception is marginally slower.
We
expect that engineering optimizations at the level of the
depthwise convolution operations can make Xception faster
than Inception V3 in the near future.
The fact that both
architectures have almost the same number of parameters
indicates that the improvement seen on ImageNet and JFT
does not come from added capacity but rather from a more
efﬁcient use of the model parameters.
4.6.
Effect of the residual connections
Figure 9.
Training proﬁle with and without residual connections.
To quantify the beneﬁts of residual connections in the
Xception architecture, we benchmarked on ImageNet a mod-
iﬁed version of Xception that does not include any residual
connections.
Results are shown in ﬁgure 9.
Residual con-
nections are clearly essential in helping with convergence,
both in terms of speed and ﬁnal classiﬁcation performance.
However we will note that benchmarking the non-residual
model with the same optimization conﬁguration as the resid-
ual model may be uncharitable and that better optimization
conﬁgurations might yield more competitive results.
Additionally, let us note that this result merely shows the
importance of residual connections for this speciﬁc architec-
ture, and that residual connections are in no way required
in order to build models that are stacks of depthwise sepa-
rable convolutions.
We also obtained excellent results with
non-residual VGG-style models where all convolution layers
were replaced with depthwise separable convolutions (with
a depth multiplier of 1), superior to Inception V3 on JFT at
equal parameter count.
4.7.
Effect of an intermediate activation after point-
wise convolutions
Figure 10.
Training proﬁle with different activations between the
depthwise and pointwise operations of the separable convolution
layers.
We mentioned earlier that the analogy between depth-
wise separable convolutions and Inception modules suggests
that depthwise separable convolutions should potentially in-
clude a non-linearity between the depthwise and pointwise
operations.
In the experiments reported so far, no such non-
linearity was included.
However we also experimentally
tested the inclusion of either ReLU or ELU [3] as intermedi-
ate non-linearity.
Results are reported on ImageNet in ﬁgure
10, and show that the absence of any non-linearity leads to
both faster convergence and better ﬁnal performance.
This is a remarkable observation, since Szegedy et al.
re-
port the opposite result in [21] for Inception modules.
It may
be that the depth of the intermediate feature spaces on which
spatial convolutions are applied is critical to the usefulness
of the non-linearity: for deep feature spaces (e.g.
those
found in Inception modules) the non-linearity is helpful, but
for shallow ones (e.g.
the 1-channel deep feature spaces
of depthwise separable convolutions) it becomes harmful,
possibly due to a loss of information.
5.
Future directions
We noted earlier the existence of a discrete spectrum be-
tween regular convolutions and depthwise separable convo-
lutions, parametrized by the number of independent channel-
space segments used for performing spatial convolutions.
In-
ception modules are one point on this spectrum.
We showed
in our empirical evaluation that the extreme formulation of
an Inception module, the depthwise separable convolution,
may have advantages over regular a regular Inception mod-
ule.
However, there is no reason to believe that depthwise
separable convolutions are optimal.
It may be that intermedi-
ate points on the spectrum, lying between regular Inception
modules and depthwise separable convolutions, hold further
advantages.
This question is left for future investigation.