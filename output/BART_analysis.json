{
  "paper_title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural",
  "sections": {
    "abstract": {
      "content": "BART: Denoising Sequence-to-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad,\nAbdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer\nFacebook AI\n{mikelewis,yinhanliu,naman}@fb.com\nAbstract\nWe present BART, a denoising autoencoder\nfor pretraining sequence-to-sequence models.\nBART is trained by (1) corrupting text with an\narbitrary noising function, and (2) learning a\nmodel to reconstruct the original text. It uses\na standard Tranformer-based neural machine\ntranslation architecture which, despite its sim-\nplicity, can be seen as generalizing BERT (due\nto the bidirectional encoder), GPT (with the\nleft-to-right decoder), and many other more re-\ncent pretraining schemes. We evaluate a num-\nber of noising approaches, \ufb01nding the best per-\nformance by both randomly shuf\ufb02ing the or-\nder of the original sentences and using a novel\nin-\ufb01lling scheme, where spans of text are re-\nplaced with a single mask token.\nBART is\nparticularly effective when \ufb01ne tuned for text\ngeneration but also works well for compre-\nhension tasks. It matches the performance of\nRoBERTa with comparable training resources\non GLUE and SQuAD, achieves new state-\nof-the-art results on a range of abstractive di-\nalogue, question answering, and summariza-\ntion tasks, with gains of up to 6 ROUGE.\nBART also provides a 1.1 BLEU increase over\na back-translation system for machine transla-\ntion, with only target language pretraining. We\nalso report ablation experiments that replicate\nother pretraining schemes within the BART\nframework, to better measure which factors\nmost in\ufb02uence end-task performance.\n1\nIntroduction\nSelf-supervised methods have achieved remarkable\nsuccess in a wide range of NLP tasks (Mikolov et al.,\n2013; Peters et al., 2018; Devlin et al., 2019; Joshi\net al., 2019; Yang et al., 2019; Liu et al., 2019).\nThe most successful approaches have been variants of\nmasked language models, which are denoising autoen-\ncoders that are trained to reconstruct text where a ran-\ndom subset of the words has been masked out. Recent\nwork has shown gains by improving the distribution of\nmasked tokens (Joshi et al., 2019), the order in which\nmasked tokens are predicted (Yang et al., 2019), and the\navailable context for replacing masked tokens (Dong\net al., 2019). However, these methods typically focus\non particular types of end tasks (e.g. span prediction,\ngeneration, etc.), limiting their applicability.\nIn this paper, we present BART, which pre-trains\na model combining Bidirectional and Auto-Regressive\nTransformers. BART is a denoising autoencoder built\nwith a sequence-to-sequence model that is applicable\nto a very wide range of end tasks.\nPretraining has\ntwo stages (1) text is corrupted with an arbitrary nois-\ning function, and (2) a sequence-to-sequence model is\nlearned to reconstruct the original text. BART uses a\nstandard Tranformer-based neural machine translation\narchitecture which, despite its simplicity, can be seen as\ngeneralizing BERT (due to the bidirectional encoder),\nGPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes (see Figure 1).\nA key advantage of this setup is the noising \ufb02exibil-\nity; arbitrary transformations can be applied to the orig-\ninal text, including changing its length. We evaluate\na number of noising approaches, \ufb01nding the best per-\nformance by both randomly shuf\ufb02ing the order of the\noriginal sentences and using a novel in-\ufb01lling scheme,\nwhere arbitrary length spans of text (including zero\nlength) are replaced with a single mask token. This ap-\nproach generalizes the original word masking and next\nsentence prediction objectives in BERT by forcing the\nmodel to reason more about overall sentence length and\nmake longer range transformations to the input.\nBART is particularly effective when \ufb01ne tuned for\ntext generation but also works well for comprehen-\nsion tasks. It matches the performance of RoBERTa\n(Liu et al., 2019) with comparable training resources\non GLUE (Wang et al., 2018) and SQuAD (Rajpurkar\net al., 2016), and achieves new state-of-the-art results\non a range of abstractive dialogue, question answer-\ning, and summarization tasks.\nFor example, it im-\nproves performance by 6 ROUGE over previous work\non XSum (Narayan et al., 2018).\nBART also opens up new ways of thinking about \ufb01ne\ntuning. We present a new scheme for machine transla-\ntion where a BART model is stacked above a few ad-\nditional transformer layers. These layers are trained\nto essentially translate the foreign language to noised\narXiv:1910.13461v1  [cs.CL]  29 Oct 2019\n\nBidirectional \nEncoder\nA  _  C  _  E \nB       D    \n(a) BERT: Random tokens are replaced with masks, and\nthe document is encoded bidirectionally. Missing tokens\nare predicted independently, so BERT cannot easily be\nused for generation.\nAutoregressive \nDecoder\nA  B  C  D  E\n<s> A  B  C  D  \n(b) GPT: Tokens are predicted auto-regressively, meaning\nGPT can be used for generation. However words can only\ncondition on leftward context, so it cannot learn bidirec-\ntional interactions.\nAutoregressive \nDecoder\nBidirectional \nEncoder\nA  B  C  D  E\nA  _  B  _  E      \n<s> A  B  C  D  \n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a\ndocument has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with\na bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\nFor \ufb01ne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the \ufb01nal\nhidden state of the decoder.\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\nEnglish, by propagation through BART, thereby us-\ning BART as a pre-trained target-side language model.\nThis approach improves performance over a strong\nback-translation MT baseline by 1.1 BLEU on the\nWMT Romanian-English benchmark.\nTo better understand these effects, we also report\nan ablation analysis that replicates other recently pro-\nposed training objectives. This study allows us to care-\nfully control for a number of factors, including data\nand optimization parameters, which have been shown\nto be as important for overall performance as the se-\nlection of training objectives (Liu et al., 2019). We \ufb01nd\nthat BART exhibits the most consistently strong perfor-\nmance across the full range of tasks we consider.\n2\nModel\nBART is a denoising autoencoder that maps a corrupted\ndocument to the original document it was derived from.\nIt is implemented as a sequence-to-sequence model\nwith a bidirectional encoder over corrupted text and a\nleft-to-right autoregressive decoder. For pre-training,\nwe optimize the negative log likelihood of the original\ndocument.\n2.1\nArchitecture\nBART uses the standard sequence-to-sequence Trans-\nformer architecture from (Vaswani et al., 2017), ex-\ncept, following GPT, that we modify ReLU activa-\ntion functions to GeLUs (Hendrycks & Gimpel, 2016)\nand initialise parameters from N(0, 0.02).\nFor our\nbase model, we use 6 layers in the encoder and de-\ncoder, and for our large model we use 12 layers in\neach. The architecture is closely related to that used in\nBERT, with the following differences: (1) each layer of\nthe decoder additionally performs cross-attention over\nthe \ufb01nal hidden layer of the encoder (as in the trans-\nformer sequence-to-sequence model); and (2) BERT\nuses an additional feed-forward network before word-\nprediction, which BART does not. In total, BART con-\ntains roughly 10% more parameters than the equiva-\nlently sized BERT model.\n2.2\nPre-training BART\nBART is trained by corrupting documents and then op-\ntimizing a reconstruction loss\u2014the cross-entropy be-\ntween the decoder\u2019s output and the original document.\nUnlike existing denoising autoencoders, which are tai-\nlored to speci\ufb01c noising schemes, BART allows us to\napply any type of document corruption. In the extreme\ncase, where all information about the source is lost,\nBART is equivalent to a language model.\nWe experiment with several previously proposed and\nnovel transformations, but we believe there is a sig-\nni\ufb01cant potential for development of other new alter-\nnatives. The transformations we used are summarized\nbelow, and examples are shown in Figure 2.\nToken Masking\nFollowing BERT (Devlin et al.,\n2019), random tokens are sampled and replaced with\n[MASK] elements.\nToken Deletion\nRandom tokens are deleted from the\ninput. In contrast to token masking, the model must\ndecide which positions are missing inputs.\n\nA B C . D E .\nA . C . E .\nA _ . D _ E .\nA _C . _ E .\nC . D E . A B\nDocument Rotation\nToken Masking\nToken Deletion\nText In\ufb01lling\nD E . A B C .\nSentence Permutation\nFigure 2: Transformations for noising the input that we experiment with. These transformations can be composed.\nText In\ufb01lling\nA number of text spans are sampled,\nwith span lengths drawn from a Poisson distribution\n(\u03bb = 3). Each span is replaced with a single [MASK]\ntoken. 0-length spans correspond to the insertion of\n[MASK] tokens.\nText in\ufb01lling is inspired by Span-\nBERT (Joshi et al., 2019), but SpanBERT samples\nspan lengths from a different (clamped geometric) dis-\ntribution, and replaces each span with a sequence of\n[MASK] tokens of exactly the same length. Text in\ufb01ll-\ning teaches the model to predict how many tokens are\nmissing from a span.\nSentence Permutation\nA document is divided into\nsentences based on full stops, and these sentences are\nshuf\ufb02ed in a random order.\nDocument Rotation\nA token is chosen uniformly at\nrandom, and the document is rotated so that it begins\nwith that token. This task trains the model to identify\nthe start of the document.\n3\nFine-tuning BART\nThe representations produced by BART can be used in\nseveral ways for downstream applications.\n3.1\nSequence Classi\ufb01cation Tasks\nFor sequence classi\ufb01cation tasks, the same input is fed\ninto the encoder and decoder, and the \ufb01nal hidden state\nof the \ufb01nal decoder token is fed into new multi-class\nlinear classi\ufb01er. This approach is related to the CLS\ntoken in BERT; however we add the additional token\nto the end so that representation for the token in the\ndecoder can attend to decoder states from the complete\ninput (Figure 3a).\n3.2\nToken Classi\ufb01cation Tasks\nFor token classi\ufb01cation tasks, such as answer endpoint\nclassi\ufb01cation for SQuAD, we feed the complete doc-\nument into the encoder and decoder, and use the top\nhidden state of the decoder as a representation for each\nword. This representation is used to classify the token.\n3.3\nSequence Generation Tasks\nBecause BART has an autoregressive decoder, it can be\ndirectly \ufb01ne tuned for sequence generation tasks such\nas abstractive question answering and summarization.\nIn both of these tasks, information is copied from the\ninput but manipulated, which is closely related to the\ndenoising pre-training objective. Here, the encoder in-\nput is the input sequence, and the decoder generates\noutputs autoregressively.\n3.4\nMachine Translation\nWe also explore using BART to improve machine trans-\nlation decoders for translating into English. Previous\nwork Edunov et al. (2019) has shown that models can\nbe improved by incorporating pre-trained encoders, but\ngains from using pre-trained language models in de-\ncoders have been limited. We show that it is possible\nto use the entire BART model (both encoder and de-\ncoder) as a single pretrained decoder for machine trans-\nlation, by adding a new set of encoder parameters that\nare learned from bitext (see Figure 3b).\nMore precisely, we replace BART\u2019s encoder embed-\nding layer with a new randomly initialized encoder.\nThe model is trained end-to-end, which trains the new\nencoder to map foreign words into an input that BART\ncan de-noise to English. The new encoder can use a\nseparate vocabulary from the original BART model.\nWe train the source encoder in two steps, in both\ncases backpropagating the cross-entropy loss from the\noutput of the BART model. In the \ufb01rst step, we freeze\nmost of BART parameters and only update the ran-\ndomly initialized source encoder, the BART positional\nembeddings, and the self-attention input projection ma-\ntrix of BART\u2019s encoder \ufb01rst layer. In the second step,\nwe train all model parameters for a small number of\niterations.\n4\nComparing Pre-training Objectives\nBART supports a much wider range of noising schemes\nduring pre-training than previous work. We compare a\nrange of options using base-size models (6 encoder and\n6 decoder layers, with a hidden size of 768), evaluated\non a representative subset of the tasks we will consider\nfor the full large scale experiments in \u00a75.\n4.1\nComparison Objectives\nWhile many pre-training objectives have been pro-\nposed, fair comparisons between these have been dif-\n\ufb01cult to perform, at least in part due to differences in\ntraining data, training resources, architectural differ-\nences between models, and \ufb01ne-tuning procedures. We\n\nPre-trained \nDecoder\nPre-trained \nEncoder\nlabel\nA  B  C  D  E \n<s> A  B  C  D  E\n(a) To use BART for classi\ufb01cation problems, the same\ninput is fed into the encoder and decoder, and the repre-\nsentation from the \ufb01nal output is used.\nRandomly \nInitialized Encoder\n    \u03b1   \u03b2   \u03b3   \u03b4   \u03b5\nPre-trained  \nDecoder\nPre-trained \nEncoder\nA  B  C  D  E\n<s> A  B  C  D  \n(b) For machine translation, we learn a small additional\nencoder that replaces the word embeddings in BART. The\nnew encoder can use a disjoint vocabulary.\nFigure 3: Fine tuning BART for classi\ufb01cation and translation.\nre-implement strong pre-training approaches recently\nproposed for discriminative and generation tasks. We\naim, as much as possible, to control for differences un-\nrelated to the pre-training objective. However, we do\nmake minor changes to the learning rate and usage of\nlayer normalisation in order to improve performance\n(tuning these separately for each objective). For refer-\nence, we compare our implementations with published\nnumbers from BERT, which was also trained for 1M\nsteps on a combination of books and Wikipedia data.\nWe compare the following approaches:\nLanguage Model\nSimilarly to GPT (Radford et al.,\n2018), we train a left-to-right Transformer language\nmodel. This model is equivalent to the BART decoder,\nwithout cross-attention.\nPermuted Language Model\nBased on XLNet (Yang\net al., 2019), we sample 1/6 of the tokens, and gener-\nate them in a random order autoregressively. For con-\nsistency with other models, we do not implement the\nrelative positional embeddings or attention across seg-\nments from XLNet.\nMasked Language Model\nFollowing BERT (Devlin\net al., 2019), we replace 15% of tokens with [MASK]\nsymbols, and train the model to independently predict\nthe original tokens.\nMultitask Masked Language Model\nAs in UniLM\n(Dong et al., 2019), we train a Masked Language\nModel with additional self-attention masks. Self at-\ntention masks are chosen randomly in with the follow\nproportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 un-\nmasked, and 1/3 with the \ufb01rst 50% of tokens unmasked\nand a left-to-right mask for the remainder.\nMasked Seq-to-Seq\nInspired by MASS (Song et al.,\n2019), we mask a span containing 50% of tokens,\nand train a sequence to sequence model to predict the\nmasked tokens.\nFor the Permuted LM, Masked LM and Multitask\nMasked LM, we use two-stream attention (Yang et al.,\n2019) to ef\ufb01ciently compute likelihoods of the output\npart of the sequence (using a diagonal self-attention\nmask on the output to predict words left-to-right).\nWe experiment with (1) treating the task as a stan-\ndard sequence-to-sequence problem, where the source\ninput to the encoder and the target is the decoder out-\nput, or (2) adding the source as pre\ufb01x to the target in\nthe decoder, with a loss only on the target part of the\nsequence. We \ufb01nd the former works better for BART\nmodels, and the latter for other models.\nTo most directly compare our models on their ability\nto model their \ufb01ne-tuning objective (the log likelihood\nof the human text), we report perplexity in Table 1.\n4.2\nTasks\nSQuAD\n(Rajpurkar et al., 2016)a an extractive ques-\ntion answering task on Wikipedia paragraphs. Answers\nare text spans extracted from a given document context.\nSimilar to BERT (Devlin et al., 2019), we use concate-\nnated question and context as input to the encoder of\nBART, and additionally pass them to the decoder. The\nmodel includes classi\ufb01ers to predict the start and end\nindices of each token.\nMNLI\n(Williams et al., 2017), a bitext classi\ufb01cation\ntask to predict whether one sentence entails another.\nThe \ufb01ne-tuned model concatenates the two sentences\nwith appended an EOS token, and passes them to both\nthe BART encoder and decoder. In contrast to BERT,\nthe representation of the EOS token is used to classify\nthe sentences relations.\nELI5\n(Fan et al., 2019), a long-form abstractive ques-\ntion answering dataset. Models generate answers con-\nditioned on the concatenation of a question and sup-\nporting documents.\nXSum\n(Narayan et al., 2018), a news summarization\ndataset with highly abstractive summaries.\nConvAI2\n(Dinan et al., 2019), a dialogue response\ngeneration task, conditioned on context and a persona.\nCNN/DM\n(Hermann et al., 2015), a news summa-\nrization dataset. Summaries here are typically closely\nrelated to source sentences.\n4.3\nResults\nResults are shown in Table 1. Several trends are clear:\n\nModel\nSQuAD 1.1\nMNLI\nELI5\nXSum\nConvAI2\nCNN/DM\nF1\nAcc\nPPL\nPPL\nPPL\nPPL\nBERT Base (Devlin et al., 2019)\n88.5\n84.3\n-\n-\n-\n-\nMasked Language Model\n90.0\n83.5\n24.77\n7.87\n12.59\n7.06\nMasked Seq2seq\n87.0\n82.1\n23.40\n6.80\n11.43\n6.19\nLanguage Model\n76.7\n80.1\n21.40\n7.00\n11.51\n6.56\nPermuted Language Model\n89.1\n83.7\n24.03\n7.69\n12.23\n6.96\nMultitask Masked Language Model\n89.2\n82.4\n23.73\n7.50\n12.39\n6.74\nBART Base\nw/ Token Masking\n90.4\n84.1\n25.05\n7.08\n11.73\n6.10\nw/ Token Deletion\n90.4\n84.1\n24.61\n6.90\n11.46\n5.87\nw/ Text In\ufb01lling\n90.8\n84.0\n24.26\n6.61\n11.05\n5.83\nw/ Document Rotation\n77.2\n75.3\n53.69\n17.14\n19.87\n10.59\nw/ Sentence Shuf\ufb02ing\n85.4\n81.5\n41.87\n10.93\n16.67\n7.89\nw/ Text In\ufb01lling + Sentence Shuf\ufb02ing\n90.8\n83.8\n24.17\n6.62\n11.12\n5.41\nTable 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps\non a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data\nusing the same code-base, and \ufb01ne-tuned with the same procedures. Entries in the second block are inspired by\npre-training objectives proposed in previous work, but have been simpli\ufb01ed to focus on evaluation objectives (see\n\u00a74.1). Performance varies considerably across tasks, but the BART models with text in\ufb01lling demonstrate the most\nconsistently strong performance.\nPerformance of pre-training methods varies signi\ufb01-\ncantly across tasks\nThe effectiveness of pre-training\nmethods is highly dependent on the task. For exam-\nple, a simple language model achieves the best ELI5\nperformance, but the worst SQUAD results.\nToken masking is crucial\nPre-training objectives\nbased on rotating documents or permuting sentences\nperform poorly in isolation. The successful methods\neither use token deletion or masking, or self-attention\nmasks.\nDeletion appears to outperform masking on\ngeneration tasks.\nLeft-to-right\npre-training\nimproves\ngeneration\nThe Masked Language Model and the Permuted\nLanguage Model perform less well than others on\ngeneration, and are the only models we consider that\ndo not include left-to-right auto-regressive language\nmodelling during pre-training.\nBidirectional encoders are crucial for SQuAD\nAs\nnoted in previous work (Devlin et al., 2019), just\nleft-to-right decoder performs poorly on SQuAD, be-\ncause future context is crucial in classi\ufb01cation deci-\nsions. However, BART achieves similar performance\nwith only half the number of bidirectional layers.\nThe pre-training objective is not the only important\nfactor\nOur Permuted Language Model performs less\nwell than XLNet (Yang et al., 2019). Some of this dif-\nference is likely due to not including other architectural\nimprovements, such as relative-position embeddings or\nsegment-level recurrence.\nPure language models perform best on ELI5\nThe\nELI5 dataset is an outlier, with much higher perplex-\nities than other tasks, and is the only generation task\nwhere other models outperform BART. A pure lan-\nguage model performs best, suggesting that BART is\nless effective when the output is only loosely con-\nstrained by the input.\nBART achieves the most consistently strong perfor-\nmance.\nWith the exception of ELI5, BART models\nusing text-in\ufb01lling perform well on all tasks.\n5\nLarge-scale Pre-training Experiments\nRecent work has shown that downstream performance\ncan dramatically improve when pre-training is scaled\nto large batch sizes (Yang et al., 2019; Liu et al., 2019)\nand corpora. To test how well BART performs in this\nregime, and to create a useful model for downstream\ntasks, we trained BART using the same scale as the\nRoBERTa model.\n5.1\nExperimental Setup\nWe pre-train a large model with 12 layers in each of the\nencoder and decoder, and a hidden size of 1024. Fol-\nlowing RoBERTa (Liu et al., 2019), we use a batch size\nof 8000, and train the model for 500000 steps. Docu-\nments are tokenized with the same byte-pair encoding\nas GPT-2 (Radford et al., 2019). Based on the results in\nSection \u00a74, we use a combination of text in\ufb01lling and\nsentence permutation. We mask 30% of tokens in each\ndocument, and permute all sentences. Although sen-\ntence permutation only shows signi\ufb01cant additive gains\n\nSQuAD 1.1\nSQuAD 2.0\nMNLI\nSST\nQQP\nQNLI\nSTS-B\nRTE\nMRPC\nCoLA\nEM/F1\nEM/F1\nm/mm\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nMcc\nBERT\n84.1/90.9\n79.0/81.8\n86.6/-\n93.2\n91.3\n92.3\n90.0\n70.4\n88.0\n60.6\nUniLM\n-/-\n80.5/83.4\n87.0/85.9\n94.5\n-\n92.7\n-\n70.9\n-\n61.1\nXLNet\n89.0/94.5\n86.1/88.8\n89.8/-\n95.6\n91.8\n93.9\n91.8\n83.8\n89.2\n63.6\nRoBERTa\n88.9/94.6\n86.5/89.4\n90.2/90.2\n96.4\n92.2\n94.7\n92.4\n86.6\n90.9\n68.0\nBART\n88.8/94.6\n86.1/89.2\n89.9/90.1\n96.6\n92.5\n94.9\n91.2\n87.0\n90.4\n62.8\nTable 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and\nXLNet, suggesting that BART\u2019s uni-directional decoder layers do not reduce performance on discriminative tasks.\nCNN/DailyMail\nXSum\nR1\nR2\nRL\nR1\nR2\nRL\nLead-3\n40.42\n17.62\n36.67\n16.30\n1.60\n11.95\nPTGEN (See et al., 2017)\n36.44\n15.66\n33.42\n29.70\n9.21\n23.24\nPTGEN+COV (See et al., 2017)\n39.53\n17.28\n36.38\n28.10\n8.02\n21.72\nUniLM\n43.33\n20.21\n40.51\n-\n-\n-\nBERTSUMABS (Liu & Lapata, 2019)\n41.72\n19.39\n38.76\n38.76\n16.33\n31.15\nBERTSUMEXTABS (Liu & Lapata, 2019)\n42.13\n19.60\n39.18\n38.81\n16.50\n31.27\nBART\n44.16\n21.28\n40.90\n45.14\n22.27\n37.25\nTable 3: Results on two standard summarization datasets. BART outperforms previous work on summarization on\ntwo tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\non the CNN/DM summarization dataset, we hypothe-\nsised that larger pre-trained models may be better able\nto learn from this task. To help the model better \ufb01t the\ndata, we disabled dropout for the \ufb01nal 10% of training\nsteps. We use the same pre-training data as Liu et al.\n(2019), consisting of 160Gb of news, books, stories,\nand web text.\n5.2\nDiscriminative Tasks\nTable 2 compares the performance of BART with sev-\neral recent approaches on the well-studied SQuAD and\nGLUE tasks (Warstadt et al., 2018; Socher et al., 2013;\nDolan & Brockett, 2005; Agirre et al., 2007; Williams\net al., 2018; Dagan et al., 2006; Levesque et al., 2011).\nThe most directly comparable baseline is RoBERTa,\nwhich was pre-trained with the same resources, but\na different objective. Overall, BART performs simi-\nlarly, with only small differences between the models\non most tasks. suggesting that BART\u2019s improvements\non generation tasks do not come at the expense of clas-\nsi\ufb01cation performance.\n5.3\nGeneration Tasks\nWe also experiment with several text generation tasks.\nBART is \ufb01ne-tuned as a standard sequence-to-sequence\nmodel from the input to the output text. During \ufb01ne-\ntuning we use a label smoothed cross entropy loss\n(Pereyra et al., 2017), with the smoothing parameter\nset to 0.1. During generation, we set beam size as 5,\nremove duplicated trigrams in beam search, and tuned\nthe model with min-len, max-len, length penalty on the\nvalidation set (Fan et al., 2017).\nConvAI2\nValid F1\nValid PPL\nSeq2Seq + Attention\n16.02\n35.07\nBest System\n19.09\n17.51\nBART\n20.72\n11.85\nTable 4: BART outperforms previous work on conver-\nsational response generation.\nPerplexities are renor-\nmalized based on of\ufb01cial tokenizer for ConvAI2.\nSummarization\nTo provide a comparison with the\nstate-of-the-art in summarization, we present results\non two summarization datasets, CNN/DailyMail and\nXSum, which have distinct properties.\nSummaries in the CNN/DailyMail tend to resemble\nsource sentences. Extractive models do well here, and\neven the baseline of the \ufb01rst-three source sentences is\nhighly competitive. Nevertheless, BART outperforms\nall existing work.\nIn contrast, XSum is highly abstractive, and extrac-\ntive models perform poorly. BART outperforms the\nbest previous work, which leverages BERT, by roughly\n6.0 points on all ROUGE metrics\u2014representing a sig-\nni\ufb01cant advance in performance on this problem. Qual-\nitatively, sample quality is high (see \u00a76).\nDialogue\nWe evaluate dialogue response generation\non CONVAI2 (Dinan et al., 2019), in which agents\nmust generate responses conditioned on both the pre-\nvious context and a textually-speci\ufb01ed persona. BART\noutperforms previous work on two automated metrics.\n\nELI5\nR1\nR2\nRL\nBest Extractive\n23.5\n3.1\n17.5\nLanguage Model\n27.8\n4.7\n23.1\nSeq2Seq\n28.3\n5.1\n22.8\nSeq2Seq Multitask\n28.9\n5.4\n23.1\nBART\n30.6\n6.2\n24.3\nTable 5:\nBART achieves state-of-the-art results on\nthe challenging ELI5 abstractive question answering\ndataset. Comparison models are from Fan et al. (2019).\nRO-EN\nBaseline\n36.80\nFixed BART\n36.29\nTuned BART\n37.96\nTable 6: The performance (BLEU) of baseline and\nBART on WMT\u201916 RO-EN augmented with back-\ntranslation data. BART improves over a strong back-\ntranslation (BT) baseline by using monolingual English\npre-training.\nAbstractive QA\nWe use the recently proposed ELI5\ndataset to test the model\u2019s ability to generate long free-\nform answers. We \ufb01nd BART outperforms the best pre-\nvious work by 1.2 ROUGE-L, but the dataset remains\na challenging, because answers are only weakly speci-\n\ufb01ed by the question.\n5.4\nTranslation\nWe also evaluated performance on WMT16 Romanian-\nEnglish,\naugmented\nwith\nback-translation\ndata\nfrom Sennrich et al. (2016).\nWe use a 6-layer\ntransformer source encoder to map Romanian into\na representation that BART is able to de-noise into\nEnglish, following the approach introduced in \u00a73.4.\nExperiment results are presented in Table 6.\nWe\ncompare our results against a baseline Transformer\narchitecture (Vaswani et al., 2017) with Transformer-\nlarge settings (the baseline row).\nWe show the\nperformance of both steps of our model in the \ufb01xed\nBART and tuned BART rows.\nFor each row we\nexperiment on the original WMT16 Romanian-English\naugmented with back-translation data.\nWe use a\nbeam width of 5 and a length penalty of \u03b1 = 1.\nPreliminary results suggested that our approach was\nless effective without back-translation data, and prone\nto over\ufb01tting\u2014future work should explore additional\nregularization techniques.\n6\nQualitative Analysis\nBART shows large improvements on summarization\nmetrics, of up to 6 points over the prior state-of-the-art.\nTo understand BART\u2019s performance beyond automated\nmetrics, we analyse its generations qualitatively.\nTable 7 shows example summaries generated by\nBART. Examples are taken from WikiNews articles\npublished after the creation of the pre-training corpus,\nto eliminate the possibility of the events described be-\ning present in the model\u2019s training data.\nFollowing\nNarayan et al. (2018), we remove the \ufb01rst sentence of\nthe article prior to summarizing it, so there is no easy\nextractive summary of the document.\nUnsurprisingly, model output is \ufb02uent and grammat-\nical English. However, model output is also highly ab-\nstractive, with few phrases copied from the input. The\noutput is also generally factually accurate, and inte-\ngrates supporting evidence from across the input doc-\nument with background knowledge (for example, cor-\nrectly completing names, or inferring that PG&E oper-\nates in California). In the \ufb01rst example, inferring that\n\ufb01sh are protecting reefs from global warming requires\nnon-trivial inference from the text. However, the claim\nthat the work was published in Science is not supported\nby the source.\nThese samples demonstrate that the BART pretrain-\ning has learned a strong combination of natural lan-\nguage understanding and generation.\n7\nRelated Work\nEarly methods for pretraining were based on language\nmodels. GPT (Radford et al., 2018) only models left-\nward context, which is problematic for some tasks.\nELMo (Peters et al., 2018) concatenates left-only and\nright-only representations, but does not pre-train inter-\nactions between these features. Radford et al. (2019)\ndemonstrated that very large language models can act\nas unsupervised multitask models.\nBERT (Devlin et al., 2019) introduced masked lan-\nguage modelling, which allows pre-training to learn in-\nteractions between left and right context words. Re-\ncent work has shown that very strong performance can\nbe achieved by training for longer (Liu et al., 2019),\nby tying parameters across layers (Lan et al., 2019),\nand by masking spans instead of words (Joshi et al.,\n2019). Predictions are not made auto-regressively, re-\nducing the effectiveness of BERT for generation tasks.\nUniLM (Dong et al., 2019) \ufb01ne-tunes BERT with an\nensemble of masks, some of which allow only leftward\ncontext. Like BART, this allows UniLM to be used for\nboth generative and discriminative tasks. A difference\nis that UniLM predictions are conditionally indepen-\ndent, whereas BART\u2019s are autoregressive. BART re-\nduces the mismatch between pre-training and genera-\ntion tasks, because the decoder is always trained on un-\ncorrupted context.\nMASS (Song et al., 2019) is perhaps the most similar\nmodel to BART. An input sequence where a contiguous\nspan of tokens is masked is mapped to a sequence con-\nsisting of the missing tokens. MASS is less effective\nfor discriminative tasks, because disjoint sets of tokens\nare fed into the encoder and decoder.\nXL-Net (Yang et al., 2019) extends BERT by pre-\n\nSource Document (abbreviated)\nBART Summary\nThe researchers examined three types of coral in reefs off the\ncoast of Fiji ... The researchers found when \ufb01sh were plentiful,\nthey would eat algae and seaweed off the corals, which appeared\nto leave them more resistant to the bacterium Vibrio coralliilyti-\ncus, a bacterium associated with bleaching. The researchers sug-\ngested the algae, like warming temperatures, might render the\ncorals\u2019 chemical defenses less effective, and the \ufb01sh were pro-\ntecting the coral by removing the algae.\nFisheries off the coast of Fiji are protect-\ning coral reefs from the effects of global\nwarming, according to a study in the jour-\nnal Science.\nSacoolas, who has immunity as a diplomat\u2019s wife, was involved\nin a traf\ufb01c collision ... Prime Minister Johnson was questioned\nabout the case while speaking to the press at a hospital in Wat-\nford. He said, \u201cI hope that Anne Sacoolas will come back ...\nif we can\u2019t resolve it then of course I will be raising it myself\npersonally with the White House.\u201d\nBoris Johnson has said he will raise the is-\nsue of US diplomat Anne Sacoolas\u2019 diplo-\nmatic immunity with the White House.\nAccording to Syrian state media, government forces began de-\nploying into previously SDF controlled territory yesterday. ...\nOn October 6, US President Donald Trump and Turkish Presi-\ndent Recep Tayyip Erdoan spoke on the phone. Then both na-\ntions issued statements speaking of an imminent incursion into\nnortheast Syria ... . On Wednesday, Turkey began a military\noffensive with airstrikes followed by a ground invasion.\nSyrian government forces have entered\nterritory held by the US-backed Syrian\nDemocratic Forces (SDF) in response to\nTurkey\u2019s incursion into the region.\nThis is the \ufb01rst time anyone has been recorded to run a full\nmarathon of 42.195 kilometers (approximately 26 miles) under\nthis pursued landmark time. It was not, however, an of\ufb01cially\nsanctioned world record, as it was not an \u201dopen race\u201d of the\nIAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge\nran in Vienna, Austria. It was an event speci\ufb01cally designed to\nhelp Kipchoge break the two hour barrier.\nKenyan runner Eliud Kipchoge has run a\nmarathon in less than two hours.\nPG&E stated it scheduled the blackouts in response to forecasts\nfor high winds amid dry conditions. The aim is to reduce the risk\nof wild\ufb01res. Nearly 800 thousand customers were scheduled to\nbe affected by the shutoffs which were expected to last through\nat least midday tomorrow.\nPower has been turned off to millions of\ncustomers in California as part of a power\nshutoff plan.\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles. For clarity, only relevant\nexcerpts of the source are shown. Summaries combine information from across the article and prior knowledge.\ndicting masked tokens auto-regressively in a permuted\norder. This objective allows predictions to condition on\nboth left and right context. In contrast, the BART de-\ncoder works left-to-right during pre-training, matching\nthe setting during generation.\nSeveral papers have explored using pre-trained rep-\nresentations to improve machine translation.\nThe\nlargest improvements have come from pre-training on\nboth source and target languages (Song et al., 2019;\nLample & Conneau, 2019), but this requires pre-\ntraining on all languages of interest. Other work has\nshown that encoders can be improved using pre-trained\nrepresentations (Edunov et al., 2019), but gains in de-\ncoders are more limited. We show how BART can be\nused to improve machine translation decoders.\n8\nConclusions\nWe introduced BART, a pre-training approach that\nlearns to map corrupted documents to the original.\nBART achieves similar performance to RoBERTa on\ndiscriminative tasks, while achieving new state-of-the-\nart results on a number of text generation tasks. Fu-\nture work should explore new methods for corrupting\ndocuments for pre-training, perhaps tailoring them to\nspeci\ufb01c end tasks.\n\nReferences\nEneko Agirre, Llu\u2019is M\u2018arquez, and Richard Wicen-\ntowski (eds.).\nProceedings of the Fourth Interna-\ntional Workshop on Semantic Evaluations (SemEval-\n2007). Association for Computational Linguistics,\nPrague, Czech Republic, June 2007.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\nThe PASCAL recognising textual entailment chal-\nlenge.\nIn Machine learning challenges. evaluat-\ning predictive uncertainty, visual object classi\ufb01ca-\ntion, and recognising tectual entailment, pp. 177\u2013\n190. Springer, 2006.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova.\nBERT: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 4171\u2013\n4186, Minneapolis, Minnesota, June 2019. Associa-\ntion for Computational Linguistics. doi: 10.18653/\nv1/N19-1423.\nURL https://www.aclweb.\norg/anthology/N19-1423.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe,\net al.\nThe second conversational in-\ntelligence challenge (convai2).\narXiv preprint\narXiv:1902.00098, 2019.\nWilliam B Dolan and Chris Brockett. Automatically\nconstructing a corpus of sentential paraphrases. In\nProceedings of the International Workshop on Para-\nphrasing, 2005.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. Uni\ufb01ed language model pre-\ntraining for natural language understanding and gen-\neration. arXiv preprint arXiv:1905.03197, 2019.\nSergey Edunov, Alexei Baevski, and Michael Auli.\nPre-trained language model representations for lan-\nguage generation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), 2019.\nAngela Fan, David Grangier, and Michael Auli. Con-\ntrollable abstractive summarization. arXiv preprint\narXiv:1711.05217, 2017.\nAngela Fan, Yacine Jernite, Ethan Perez, David\nGrangier, Jason Weston, and Michael Auli.\nEli5:\nLong form question answering.\narXiv preprint\narXiv:1907.09190, 2019.\nDan Hendrycks and Kevin Gimpel. Gaussian error lin-\near units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\nKarl Moritz Hermann,\nTomas Kocisky,\nEdward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. Teaching machines to\nread and comprehend. In Advances in neural infor-\nmation processing systems, pp. 1693\u20131701, 2015.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. Spanbert: Im-\nproving pre-training by representing and predicting\nspans. arXiv preprint arXiv:1907.10529, 2019.\nGuillaume Lample and Alexis Conneau.\nCross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291, 2019.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut.\nAlbert: A lite bert for self-supervised learn-\ning of language representations.\narXiv preprint\narXiv:1909.11942, 2019.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. The Winograd schema challenge. In AAAI\nSpring Symposium: Logical Formalizations of Com-\nmonsense Reasoning, volume 46, pp. 47, 2011.\nYang Liu and Mirella Lapata.\nText summariza-\ntion with pretrained encoders.\narXiv preprint\narXiv:1908.08345, 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta:\nA robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Ef\ufb01cient estimation of word representations\nin vector space.\narXiv preprint arXiv:1301.3781,\n2013.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\nDon\u2019t give me the details, just the summary! topic-\naware convolutional neural networks for extreme\nsummarization.\narXiv preprint arXiv:1808.08745,\n2018.\nGabriel Pereyra,\nGeorge Tucker,\nJan Chorowski,\n\u0141ukasz Kaiser, and Geoffrey Hinton. Regularizing\nneural networks by penalizing con\ufb01dent output dis-\ntributions. arXiv preprint arXiv:1701.06548, 2017.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representa-\ntions. arXiv preprint arXiv:1802.05365, 2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever.\nImproving language un-\nderstanding by generative pre-training.\nURL\nhttps://s3-us-west-2.\namazonaws.\ncom/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners.\nOpenAI\nBlog, 1(8), 2019.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang.\nSquad: 100,000+ questions for\nmachine comprehension of text.\narXiv preprint\narXiv:1606.05250, 2016.\nAbigail\nSee,\nPeter\nJ\nLiu,\nand\nChristopher\nD\nManning.\nGet to the point:\nSummarization\nwith pointer-generator networks.\narXiv preprint\narXiv:1704.04368, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\nEdinburgh neural machine translation systems for\nWMT 16. In Proceedings of the First Conference\non Machine Translation: Volume 2, Shared Task Pa-\npers, 2016.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts.\nRecursive deep models for se-\nmantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP, pp. 1631\u20131642, 2013.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. Mass: Masked sequence to sequence pre-\ntraining for language generation.\nIn International\nConference on Machine Learning, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin.\nAttention is all you\nneed. In Advances in neural information processing\nsystems, pp. 5998\u20136008, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman.\nGlue:\nA multi-task benchmark and analysis platform for\nnatural language understanding.\narXiv preprint\narXiv:1804.07461, 2018.\nAlex Warstadt, Amanpreet Singh, and Samuel R.\nBowman. Neural network acceptability judgments.\narXiv preprint 1805.12471, 2018.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman.\nA broad-coverage challenge corpus for\nsentence understanding through inference.\narXiv\npreprint arXiv:1704.05426, 2017.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of NAACL-HLT, 2018.\nZhilin Yang,\nZihang Dai,\nYiming Yang,\nJaime\nCarbonell, Ruslan Salakhutdinov, and Quoc V\nLe.\nXlnet:\nGeneralized autoregressive pretrain-\ning for language understanding.\narXiv preprint\narXiv:1906.08237, 2019.\n",
      "start_page": 0,
      "end_page": 0,
      "confidence": 0.5288369178771972
    },
    "methods": {
      "content": "have achieved remarkable\nsuccess in a wide range of NLP tasks (Mikolov et al.,\n2013; Peters et al., 2018; Devlin et al., 2019; Joshi\net al., 2019; Yang et al., 2019; Liu et al., 2019).\nThe most successful approaches have been variants of\nmasked language models, which are denoising autoen-\ncoders that are trained to reconstruct text where a ran-\ndom subset of the words has been masked out. Recent\nwork has shown gains by improving the distribution of\nmasked tokens (Joshi et al., 2019), the order in which\nmasked tokens are predicted (Yang et al., 2019), and the\navailable context for replacing masked tokens (Dong\net al., 2019). However, these methods typically focus\non particular types of end tasks (e.g. span prediction,\ngeneration, etc.), limiting their applicability.\nIn this paper, we present BART, which pre-trains\na model combining Bidirectional and Auto-Regressive\nTransformers. BART is a denoising autoencoder built\nwith a sequence-to-sequence model that is applicable\nto a very wide range of end tasks.\nPretraining has\ntwo stages (1) text is corrupted with an arbitrary nois-\ning function, and (2) a sequence-to-sequence model is\nlearned to reconstruct the original text. BART uses a\nstandard Tranformer-based neural machine translation\narchitecture which, despite its simplicity, can be seen as\ngeneralizing BERT (due to the bidirectional encoder),\nGPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes (see Figure 1).\nA key advantage of this setup is the noising \ufb02exibil-\nity; arbitrary transformations can be applied to the orig-\ninal text, including changing its length. We evaluate\na number of noising approaches, \ufb01nding the best per-\nformance by both randomly shuf\ufb02ing the order of the\noriginal sentences and using a novel in-\ufb01lling scheme,\nwhere arbitrary length spans of text (including zero\nlength) are replaced with a single mask token. This ap-\nproach generalizes the original word masking and next\nsentence prediction objectives in BERT by forcing the\nmodel to reason more about overall sentence length and\nmake longer range transformations to the input.\nBART is particularly effective when \ufb01ne tuned for\ntext generation but also works well for comprehen-\nsion tasks. It matches the performance of RoBERTa\n(Liu et al., 2019) with comparable training resources\non GLUE (Wang et al., 2018) and SQuAD (Rajpurkar\net al., 2016), and achieves new state-of-the-art",
      "start_page": 0,
      "end_page": 0,
      "confidence": 0.5
    },
    "results": {
      "content": "on a range of abstractive di-\nalogue, question answering, and summariza-\ntion tasks, with gains of up to 6 ROUGE.",
      "start_page": 0,
      "end_page": 0,
      "confidence": 0.5
    }
  },
  "summary": {
    "abstract": "BART: Denoising Sequence-to-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goya...",
    "methods": "we present a denoising model combining pretrains and sequence-to-sequence model that is applicable to a very wide range of end tasks built with a sequence-to-sequence model with a very wide range of end tasks built with a sequence-to-sequence model . <n> the model uses a standardformer-based neural - based machine architecture , despite its simplicity as generalizing decoder-based model , which uses the original text machine architecture . <n> key advantage of this setup is that it is pretraining or applied in arbitrary text transformations including changing its length .",
    "results": "on a range of abstractive di-\nalogue, question answering, and summariza-\ntion tasks, with gains of up to 6 ROUGE.",
    "conclusion": "Section not found",
    "full_summary": "\n        Abstract: BART: Denoising Sequence-to-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goya...\n        \n        Methods: we present a denoising model combining pretrains and sequence-to-sequence model that is applicable to a very wide range of end tasks built with a sequence-to-sequence model with a very wide range of end tasks built with a sequence-to-sequence model . <n> the model uses a standardformer-based neural - based machine architecture , despite its simplicity as generalizing decoder-based model , which uses the original text machine architecture . <n> key advantage of this setup is that it is pretraining or applied in arbitrary text transformations including changing its length .\n        \n        Results: on a range of abstractive di-\nalogue, question answering, and summariza-\ntion tasks, with gains of up to 6 ROUGE.\n        \n        Conclusion: Section not found\n        ",
    "key_concepts": [
      "bart",
      "model",
      "2019",
      "et al",
      "al",
      "et",
      "pre",
      "tasks",
      "text",
      "language"
    ]
  },
  "quiz": [
    {
      "question": "What is the main contribution of this work regarding \u0120Base?",
      "answer": "Several trends are clear:\n\nModel\nSQuAD 1.1\nMNLI\nELI5\nXSum\nConvAI2\nCNN/DM\nF1\nAcc\nPPL\nPPL\nPPL\nPPL\nBERT Base (Devlin et al., 2019)\n88.5\n84.3\n-\n-\n-\n-\nMasked Language Model\n90.0\n83.5\n24.77\n7.87\n12.59\n7.06\nMasked Seq2seq\n87.0\n82.1\n23.40\n6.80\n11.43\n6.19\nLanguage Model\n76.7\n80.1\n21.40\n7.00\n11.51\n6.56\nPermuted Language Model\n89.1\n83.7\n24.03\n7.69\n12.23\n6.96\nMultitask Masked Language Model\n89.2\n82.4\n23.73\n7.50\n12.39\n6.74\nBART Base\nw/ Token Masking\n90.4\n84.1\n25.05\n7.08\n11.73\n6.10\nw/ Token Deletion\n90.4\n84.1\n24.61\n6.90\n11.46\n5.87\nw/ Text In\ufb01lling\n90.8\n84.0\n24.26\n6.61\n11.05\n5.83\nw/ Document Rotation\n77.2\n75.3\n53.69\n17.14\n19.87\n10.59\nw/ Sentence Shuf\ufb02ing\n85.4\n81.5\n41.87\n10.93\n16.67\n7.89\nw/ Text In\ufb01lling + Sentence Shuf\ufb02ing\n90.8\n83.8\n24.17\n6.62\n11.12\n5.41\nTable 1: Comparison of pre-training objectives.",
      "section": "abstract",
      "confidence": 0.5288369178771972
    },
    {
      "question": "What is the main contribution of this work regarding \u0120Results?",
      "answer": "CNN/DailyMail\nXSum\nR1\nR2\nRL\nR1\nR2\nRL\nLead-3\n40.42\n17.62\n36.67\n16.30\n1.60\n11.95\nPTGEN (See et al., 2017)\n36.44\n15.66\n33.42\n29.70\n9.21\n23.24\nPTGEN+COV (See et al., 2017)\n39.53\n17.28\n36.38\n28.10\n8.02\n21.72\nUniLM\n43.33\n20.21\n40.51\n-\n-\n-\nBERTSUMABS (Liu & Lapata, 2019)\n41.72\n19.39\n38.76\n38.76\n16.33\n31.15\nBERTSUMEXTABS (Liu & Lapata, 2019)\n42.13\n19.60\n39.18\n38.81\n16.50\n31.27\nBART\n44.16\n21.28\n40.90\n45.14\n22.27\n37.25\nTable 3: Results on two standard summarization datasets.",
      "section": "abstract",
      "confidence": 0.5288369178771972
    },
    {
      "question": "What is the main contribution of this work regarding \u0120Sequence?",
      "answer": "BART: Denoising Sequence-to-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension\nMike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad,\nAbdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer\nFacebook AI\n{mikelewis,yinhanliu,naman}@fb.com\nAbstract\nWe present BART, a denoising autoencoder\nfor pretraining sequence-to-sequence models.",
      "section": "abstract",
      "confidence": 0.5288369178771972
    },
    {
      "question": "What was the outcome regarding on a range of abstractive...?",
      "answer": "on a range of abstractive di-\nalogue, question answering, and summariza-\ntion tasks, with gains of up to 6 ROUGE.",
      "section": "results",
      "confidence": 0.5
    },
    {
      "question": "How was We evaluate a number of... implemented in this study?",
      "answer": "We evaluate\na number of noising approaches, \ufb01nding the best per-\nformance by both randomly shuf\ufb02ing the order of the\noriginal sentences and using a novel in-\ufb01lling scheme,\nwhere arbitrary length spans of text (including zero\nlength) are replaced with a single mask token.",
      "section": "methods",
      "confidence": 0.5
    }
  ]
}